{"text": "# Solving Multiclass Learning Problems via Error-Correcting Output Codes\n\nThomas G. Dietterich\n\nDepartment of Computer Science, 303 Dearborn Hall\n\nOregon State University\n\nCrecallis, OR 97331 USA\n\nGhulum Bakiri\n\nDepartment of Computer Science\n\nUniversity of Bahrain\n\nIsa Town, Bahrain\n\n###### Abstract\n\nMulti-class learning problems involve finding a definition for an unknown function \\(f(\\mathbf{x})\\) whose range is a discrete set containing \\(k>2\\) values (i.e., \\(k\\) \"classes\"). The definition is acquired by studying collections of training examples of the form \\(\\{\\mathbf{x}_{i},f(\\mathbf{x}_{i})\\}\\). Existing approaches to multiclass learning problems include direct application of multi-class algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the \\(k\\) classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.\n\n20195263286 1995 263 286 1995 263 286 1995 263 286 1995 263 286 1995 263 286\n\nSolving Multiclass Learning Problems via Error-Correcting Output Codes\n\n## 1 Introduction\n\nThe task of learning from examples is to find an approximate definition for an unknown function \\(f(\\mathbf{x})\\) given training examples of the form \\(\\{\\mathbf{x}_{i},f(\\mathbf{x}_{i})\\}\\). For cases in which \\(f\\) takes only the values \\(\\{0,1\\}\\)--binary functions--there are many algorithms available. For example, the decision-tree methods, such as C4.5 (Quinlan, 1993) and CART (Breiman, Friedman, Olshen, & Stone, 1984) can construct trees whose leaves are labeled with binary values. Most artificial neural network algorithms, such as the perceptron algorithm (Rosenblatt, 1958) and the error backpropagation (BP) algorithm (Rumelhart, Hinton, & Williams, 1986), are best suited to learning binary functions. Theoretical studies of learning have focused almost entirely on learning binary functions (Valiant, 1984; Natarajan, 1991).\n\nIn many real-world learning tasks, however, the unknown function \\(f\\) often takes values from a discrete set of \"classes\": \\(\\{c_{1},\\ldots,c_{k}\\}\\). For example, in medical diagnosis, the function might map a description of a patient to one of \\(k\\) possible diseases. In digit recognition (e.g.,LeCun, Boser, Denker, Henderson, Howard, Hubbard, & Jackel, 1989), the function maps each hand-printed digit to one of \\(k=10\\) classes. Phoneme recognition systems (e.g., Waibel, Hanazawa, Hinton, Shikano, & Lang, 1989) typically classify a speech segment into one of 50 to 60 phonemes.\n\nDecision-tree algorithms can be easily generalized to handle these \"multiclass\" learning tasks. Each leaf of the decision tree can be labeled with one of the \\(k\\) classes, and internal nodes can be selected to discriminate among these classes. We will call this the _direct multiclass_ approach.\n\nConnectionist algorithms are more difficult to apply to multiclass problems. The standard approach is to learn \\(k\\) individual binary functions \\(f_{1},\\ldots,f_{k}\\), one for each class. To assign a new case, \\(\\mathbf{x}\\), to one of these classes, each of the \\(f_{i}\\) is evaluated on \\(\\mathbf{x}\\), and \\(\\mathbf{x}\\) is assigned the class \\(j\\) of the function \\(f_{j}\\) that returns the highest activation (Nilsson, 1965). We will call this the _one-per-class_ approach, since one binary function is learned for each class.\n\nAn alternative approach explored by some researchers is to employ a _distributed output code_. This approach was pioneered by Sejnowski and Rosenberg (1987) in their widely-known NETtalk system. Each class is assigned a unique binary string of length \\(n\\); we will refer to these strings as \"codewords.\" Then \\(n\\) binary functions are learned, one for each bit position in these binary strings. During training for an example from class \\(i\\), the desired outputs of these \\(n\\) binary functions are specified by the codeword for class \\(i\\). With artificial neural networks, these \\(n\\) functions can be implemented by the \\(n\\) output units of a single network.\n\nNew values of \\(\\mathbf{x}\\) are classified by evaluating each of the \\(n\\) binary functions to generate an \\(n\\)-bit string \\(s\\). This string is then compared to each of the \\(k\\) codewords, and \\(\\mathbf{x}\\) is assigned to the class whose codeword is closest, according to some distance measure, to the generated string \\(s\\).\n\nAs an example, consider Table 1, which shows a six-bit distributed code for a ten-class digit-recognition problem. Notice that each row is distinct, so that each class has a unique codeword. As in most applications of distributed output codes, the bit positions (columns) have been chosen to be meaningful. Table 2 gives the meanings for the six columns. During learning, one binary function will be learned for each column. Notice that each column is also distinct and that each binary function to be learned is a disjunction of the original classes. For example, \\(f_{v1}(\\mathbf{x})=1\\) if \\(f(\\mathbf{x})\\) is 1, 4, or 5.\n\nTo classify a new hand-printed digit, \\(\\mathbf{x}\\), the six functions \\(f_{v1},f_{hl},f_{dl},f_{cc},f_{bl}\\), and \\(f_{or}\\) are evaluated to obtain a six-bit string, such as 110001. Then the distance of this string to each of the ten codewords is computed. The nearest codeword, according to Hamming distance (which counts the number of bits that differ), is 110000, which corresponds to class 4. Hence, this predicts that \\(f(\\mathbf{x})=4\\).\n\nThis process of mapping the output string to the nearest codeword is identical to the decoding step for error-correcting codes (Bose & Ray-Chaudhuri, 1960; Hocquenghem, 1959). This suggests that there might be some advantage to employing error-correcting codes as a distributed representation. Indeed, the idea of employing error-correcting, distributed representations can be traced to early research in machine learning (Duda, Machanik, & Singleton, 1963).\n\n\\begin{table}\n\\begin{tabular}{|c|c|c|c|c|c|c|} \\hline Column position & Abbreviation & Meaning \\\\ \\hline\n1 & vl & contains vertical line \\\\\n2 & hl & contains horizontal line \\\\\n3 & dl & contains diagonal line \\\\\n4 & cc & contains closed curve \\\\\n5 & ol & contains curve open to left \\\\\n6 & or & contains curve open to right \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 2: Meanings of the six columns for the code in Table 1.\n\n\\begin{table}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|} \\hline  & \\multicolumn{10}{|c|}{Co\\(\\,\\)de\\(\\,\\)Word\\(\\,\\)} \\\\ \\cline{2-10} Class & vl & hl & dl & cc & ol & or \\\\ \\hline\n0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n2 & 0 & 1 & 1 & 0 & 1 & 0 \\\\\n3 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n4 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n5 & 1 & 1 & 0 & 0 & 1 & 0 \\\\\n6 & 0 & 0 & 1 & 1 & 0 & 1 \\\\\n7 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n8 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n9 & 0 & 0 & 1 & 1 & 0 & 0 \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 1: A distributed code for the digit recognition task.\n\n\\begin{table}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \\hline  & \\multicolumn{10}{|c|}{Code\\(\\,\\)Word\\(\\,\\)} \\\\ \\cline{2-10} Class & \\(f_{0}\\) & \\(f_{1}\\) & \\(f_{2}\\) & \\(f_{3}\\) & \\(f_{4}\\) & \\(f_{5}\\) & \\(f_{6}\\) & \\(f_{7}\\) & \\(f_{8}\\) & \\(f_{9}\\) & \\(f_{10}\\) & \\(f_{11}\\) & \\(f_{12}\\) & \\(f_{13}\\) & \\(f_{14}\\) \\\\ \\hline\n0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 1 & 0 \\\\\n2 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 1 & 0 & 1 \\\\\n3 & 0 & 0 & 1 & 1 &Table 3 shows a 15-bit error-correcting code for the digit-recognition task. Each class is represented by a code word drawn from an error-correcting code. As with the distributed encoding of Table 1, a separate boolean function is learned for each bit position of the error-correcting code. To classify a new example \\(x\\), each of the learned functions \\(f_{0}(x),\\ldots,f_{14}(x)\\) is evaluated to produce a 15-bit string. This is then mapped to the nearest of the ten codewords. This code can correct up to three errors out of the 15 bits.\n\nThis error-correcting code approach suggests that we view machine learning as a kind of communications problem in which the identity of the correct output class for a new example is being \"transmitted\" over a channel. The channel consists of the input features, the training examples, and the learning algorithm. Because of errors introduced by the finite training sample, poor choice of input features, and flaws in the learning process, the class information is corrupted. By encoding the class in an error-correcting code and \"transmitting\" each bit separately (i.e., via a separate run of the learning algorithm), the system may be able to recover from the errors.\n\nThis perspective further suggests that the one-per-class and \"meaningful\" distributed output approaches will be inferior, because their output representations do not constitute robust error-correcting codes. A measure of the quality of an error-correcting code is the minimum Hamming distance between any pair of code words. If the minimum Hamming distance is \\(d\\), then the code can correct at least \\(\\lfloor\\frac{d-1}{2}\\rfloor\\) single bit errors. This is because each single bit error moves us one unit away from the true codeword (in Hamming distance). If we make only \\(\\lfloor\\frac{d-1}{2}\\rfloor\\) errors, the nearest codeword will still be the correct codeword. (The code of Table 3 has minimum Hamming distance seven and hence it can correct errors in any three bit positions.) The Hamming distance between any two codewords in the one-per-class code is two, so the one-per-class encoding of the \\(k\\) output classes cannot correct any errors.\n\nThe minimum Hamming distance between pairs of codewords in a \"meaningful\" distributed representation tends to be very low. For example, in Table 1, the Hamming distance between the codewords for classes 4 and 5 is only one. In these kinds of codes, new columns are often introduced to discriminate between only two classes. Those two classes will therefore differ only in one bit position, so the Hamming distance between their output representations will be one. This is also true of the distributed representation developed by Sejnowski and Rosenberg (1987) in the NETtalk task.\n\nIn this paper, we compare the performance of the error-correcting code approach to the three existing approaches: the direct multiclass method (using decision trees), the one-per-class method, and (in the NETtalk task only) the meaningful distributed output representation approach. We show that error-correcting codes produce uniformly better generalization performance across a variety of multiclass domains for both the C4.5 decision-tree learning algorithm and the backpropagation neural network learning algorithm. We then report a series of experiments designed to assess the robustness of the error-correcting code approach to various changes in the learning task: length of the code, size of the training set, assignment of codewords to classes, and decision-tree pruning. Finally, we show that the error-correcting code approach can produce reliable class probability estimates.\n\nThe paper concludes with a discussion of the open questions raised by these results. Chief among these questions is the issue of why the errors being made in the different bit positions of the output are somewhat independent of one another. Without this independence, the error-correcting output code method would fail. We address this question--for the case of decision-tree algorithms--in a companion paper (Kong & Dietterich, 1995).\n\n## 2 Methods\n\nThis section describes the data sets and learning algorithms employed in this study. It also discusses the issues involved in the design of error-correcting codes and describes four algorithms for code design. The section concludes with a brief description of the methods applied to make classification decisions and evaluate performance on independent test sets.\n\n### Data Sets\n\nTable 4 summarizes the data sets employed in the study. The glass, vowel, soybean, audiologyS, ISO LET, letter, and NETtalk data sets are available from the Irvine Repository of machine learning databases (Murphy & Aha, 1994).1 The POS (part of speech) data set was provided by C. Cardie (personal communication); an earlier version of the data set was described by Cardie (1993). We did not use the entire NETtalk data set, which consists of a dictionary of 20,003 words and their pronunciations. Instead, to make the experiments feasible, we chose a training set of 1000 words and a disjoint test set of 1000 words at random from the NETtalk dictionary. In this paper, we focus on the percentage of letters pronounced correctly (rather than whole words). To pronounce a letter, both the phoneme and stress of the letter must be determined. Although there are \\(54\\times 6\\) syntactically possible combinations of phonemes and stresses, only 140 of these appear in the training and test sets we selected.\n\n\\begin{table}\n\\begin{tabular}{l c c c c} \\hline  & Number of & Number of & Number of & Number of \\\\ Name & Features & Classes & Training Examples & Test Examples \\\\ \\hline glass & 9 & 6 & 214 & 10-fold xval \\\\ vowel & 10 & 11 & 528 & 462 \\\\ POS & 30 & 12 & 3,060 & 10-fold xval \\\\ soybean & 35 & 19 & 307 & 376 \\\\ audiologyS & 69 & 24 & 200 & 26 \\\\ ISOLET & 617 & 26 & 6,238 & 1,559 \\\\ letter & 16 & 26 & 16,000 & 4,000 \\\\ NETtalk & 203 & 54 phonemes & 1000 words = & 1000 words = \\\\  & & 6 stresses & 7,229 letters & 7,242 letters \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 4: Data sets employed in the study.\n\n### Learning Algorithms\n\nWe employed two general classes of learning methods: algorithms for learning decision trees and algorithms for learning feed-forward networks of sigmoidal units (artificial neural networks). For decision trees, we performed all of our experiments using C4.5, Release 1, which is an older (but substantially identical) version of the program described in Quinlan (1993). We have made several changes to C4.5 to support distributed output representations, but these have not affected the tree-growing part of the algorithm. For pruning, the confidence factor was set to 0.25. C4.5 contains a facility for creating \"soft thresholds\" for continuous features. We found experimentally that this improved the quality of the class probability estimates produced by the algorithm in the \"glass\", \"vowel\", and \"ISO LET\" domains, so the results reported for those domains were computed using soft thresholds.\n\nFor neural networks, we employed two implementations. In most domains, we used the extremely fast backpropagation implementation provided by the CNAPS neurocomputer (Adaptive Solutions, 1992). This performs simple gradient descent with a fixed learning rate. The gradient is updated after presenting each training example; no momentum term was employed. A potential limitation of the CNAPS is that inputs are only represented to eight bits of accuracy, and weights are only represented to 16 bits of accuracy. Weight update arithmetic does not round, but instead performs jamming (i.e., forcing the lowest order bit to 1 when low order bits are lost due to shifting or multiplication). On the speech recognition, letter recognition, and vowel data sets, we employed the opt system distributed by Oregon Graduate Institute (Barnard Cole, 1989). This implements the conjugate gradient algorithm and updates the gradient after each complete pass through the training examples (known as per-epoch updating). No learning rate is required for this approach.\n\nBoth the CNAPS and opt attempt to minimize the squared error between the computed and desired outputs of the network. Many researchers have employed other error measures, particularly cross-entropy (Hinton, 1989) and classification figure-of-merit (CFM, Hampshire II & Waibel, 1990). Many researchers also advocate using a softmax normalizing layer at the outputs of the network (Bridle, 1990). While each of these configurations has good theoretical support, Richard and Lippmann (1991) report that squared error works just as well as these other measures in producing accurate posterior probability estimates. Furthermore, cross-entropy and CFM tend to overfit more easily than squared error (Lippmann, personal communication; Weigend, 1993). We chose to minimize squared error because this is what the CNAPS and opt systems implement.\n\nWith either neural network algorithm, several parameters must be chosen by the user. For the CNAPS, we must select the learning rate, the initial random seed, the number of hidden units, and the stopping criteria. We selected these to optimize performance on a validation set, following the methodology of Lang, Hinton, and Waibel (1990). The training set is subdivided into a subtraining set and a validation set. While training on the subtraining set, we observed generalization performance on the validation set to determine the optimal settings of learning rate and network size and the best point at which to stop training. The training set mean squared error at that stopping point is computed, and training is then performed on the entire training set using the chosen parameters and stopping at the indicated mean squared error. Finally, we measure network performance on the test set.\n\nFor most of the data sets, this procedure worked very well. However, for the letter recognition data set, it was clearly choosing poor stopping points for the full training set. To overcome this problem, we employed a slightly different procedure to determine the stopping epoch. We trained on a series of progressively larger training sets (all of which were subsets of the final training set). Using a validation set, we determined the best stopping epoch on each of these training sets. We then extrapolated from these training sets to predict the best stopping epoch on the full training set.\n\nFor the \"glass\" and \"POS\" data sets, we employed ten-fold cross-validation to assess generalization performance. We chose training parameters based on only one \"fold\" of the ten-fold cross-validation. This creates some test set contamination, since examples in the validation set data of one fold are in the test set data of other folds. However, we found that there was little or no overfitting, so the validation set had little effect on the choice of parameters or stopping points.\n\nThe other data sets all come with designated test sets, which we employed to measure generalization performance.\n\n### Error-Correcting Code Design\n\nWe define an error-correcting code to be a matrix of binary values such as the matrix shown in Table 3. The length of a code is the number of columns in the code. The number of rows in the code is equal to the number of classes in the multiclass learning problem. A \"codeword\" is a row in the code.\n\nA good error-correcting output code for a \\(k\\)-class problem should satisfy two properties:\n\n* **Row separation.** Each codeword should be well-separated in Hamming distance from each of the other codewords.\n* **Column separation.** Each bit-position function \\(f_{i}\\) should be uncorrelated with the functions to be learned for the other bit positions \\(f_{j},j\\neq i\\). This can be achieved by insisting that the Hamming distance between column \\(i\\) and each of the other columns be large and that the Hamming distance between column \\(i\\) and the _complement_ of each of the other columns also be large.\n\nThe power of a code to correct errors is directly related to the row separation, as discussed above. The purpose of the column separation condition is less obvious. If two columns \\(i\\) and \\(j\\) are similar or identical, then when a deterministic learning algorithm such as C4.5 is applied to learn \\(f_{i}\\) and \\(f_{j}\\), it will make similar (correlated) mistakes. Error-correcting codes only succeed if the errors made in the individual bit positions are relatively uncorrelated, so that the number of simultaneous errors in many bit positions is small. If there are many simultaneous errors, the error-correcting code will not be able to correct them (Peterson & Weldon, 1972).\n\nThe errors in columns \\(i\\) and \\(j\\) will also be highly correlated if the bits in those columns are complementary. This is because algorithms such as C4.5 and backpropagation treat a class and its complement symmetrically. C4.5 will construct identical decision trees if the 0-class and 1-class are interchanged. The maximum Hamming distance between two columns is attained when the columns are complements. Hence, the column separation condition attempts to ensure that columns are neither identical nor complementary.\n\nUnless the number of classes is at least five, it is difficult to satisfy both of these properties. For example, when the number of classes is three, there are only \\(2^{3}=8\\) possible columns (see Table 5). Of these, half are complements of the other half. So this leaves us with only four possible columns. One of these will be either all zeroes or all ones, which will make it useless for discriminating among the rows. The result is that we are left with only three possible columns, which is exactly what the one-per-class encoding provides.\n\nIn general, if there are \\(k\\) classes, there will be at most \\(2^{k-1}-1\\) usable columns after removing complements and the all-zeros or all-ones column. For four classes, we get a seven-column code with minimum inter-row Hamming distance 4. For five classes, we get a 15-column code, and so on.\n\nWe have employed four methods for constructing good error-correcting output codes in this paper: (a) an exhaustive technique, (b) a method that selects columns from an exhaustive code, (c) a method based on a randomized hill-climbing algorithm, and (d) BCH codes. The choice of which method to use is based on the number of classes, \\(k\\). Finding a single method suitable for all values of \\(k\\) is an open research problem. We describe each of our four methods in turn.\n\n#### 2.3.1 Exhaustive Codes\n\nWhen \\(3\\leq k\\leq 7\\), we construct a code of length \\(2^{k-1}-1\\) as follows. Row 1 is all ones. Row 2 consists of \\(2^{k-2}\\) zeroes followed by \\(2^{k-2}-1\\) ones. Row 3 consists of \\(2^{k-3}\\) zeroes, followed by \\(2^{k-3}\\) ones, followed by \\(2^{k-3}\\) zeroes, followed by \\(2^{k-3}-1\\) ones. In row \\(i\\), there are alternating runs of \\(2^{k-i}\\) zeroes and ones. Table 6 shows the exhaustive code for a five-class problem. This code has inter-row Hamming distance 8; no columns are identical or complementary.\n\n#### 2.3.2 Column Selection from Exhaustive Codes\n\nWhen \\(8\\leq k\\leq 11\\), we construct an exhaustive code and then select a good subset of its columns. We formulate this as a propositional satisfiability problem and apply the GSAT algorithm (Selman, Levesque, & Mitchell, 1992) to attempt a solution. A solution is required to include exactly \\(L\\) columns (the desired length of the code) while ensuring that the Hamming distance between every two columns is between \\(d\\) and \\(L-d\\), for some chosen value of \\(d\\). Each column is represented by a boolean variable. A pairwise mutual\n\n\\begin{table}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline  & \\multicolumn{8}{|c|}{Co de Word} \\\\ \\cline{2-7} Class & \\(f_{0}\\) & \\(f_{1}\\) & \\(f_{2}\\) & \\(f_{3}\\) & \\(f_{4}\\) & \\(f_{5}\\) & \\(f_{6}\\) & \\(f_{7}\\) \\\\ \\hline \\(c_{0}\\) & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\\\ \\(c_{1}\\) & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 \\\\ \\(c_{2}\\) & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 5: All possible columns for a three-class problem. Note that the last four columns are complements of the first four and that the first column does not discriminate among any of the classes.\n\nexclusion constraint is placed between any two columns that violate the column separation condition. To support these constraints, we extended GSAT to support mutual exclusion and \"m-of-n\" constraints efficiently.\n\n#### 2.3.3 Randomized Hill Climbing\n\nFor \\(k>11\\), we employed a random search algorithm that begins by drawing \\(k\\) random strings of the desired length \\(L\\). Any pair of such random strings will be separated by a Hamming distance that is binomially distributed with mean \\(L/2\\). Hence, such randomly generated codes are generally quite good on average. To improve them, the algorithm repeatedly finds the pair of rows closest together in Hamming distance and the pair of columns that have the \"most extreme\" Hamming distance (i.e., either too close or too far apart). The algorithm then computes the four codeword bits where these rows and columns intersect and changes them to improve the row and column separations as shown in Figure 1. When this hill climbing procedure reaches a local maximum, the algorithm randomly chooses pairs of rows and columns and tries to improve their separations. This combined hill-climbing/random-choice procedure is able to improve the minimum Hamming distance separation quite substantially.\n\n\\begin{table}\n\\begin{tabular}{|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|} \\hline Row & \\multicolumn{10}{|c|}{Column} \\\\ \\cline{2-13}  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 \\\\ \\hline\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n3 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\\\\n4 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\\\\n5 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 6: Exhaustive code for \\(k\\)=5.\n\nFigure 1: Hill-climbing algorithm for improving row and column separation. The two closest rows and columns are indicated by lines. Where these lines intersect, the bits in the code words are changed to improve separations as shown on the right.\n\n#### 2.3.4 BCH Codes\n\nFor \\(k>11\\) we also applied the BCH algorithm to design codes (Bose & Ray-Chaudhuri, 1960; Hocquenghem, 1959). The BCH algorithm employs algebraic methods from Galois field theory to design nearly optimal error-correcting codes. However, there are three practical drawbacks to using this algorithm. First, published tables of the primitive polynomials required by this algorithm only produce codes up to length 64, since this is the largest word size employed in computer memories. Second, the codes do not always exhibit good column separations. Third, the number of rows in these codes is always a power of two. If the number of classes \\(k\\) in our learning problem is not a power of two, we must shorten the code by deleting rows (and possible columns) while maintaining good row and column separations. We have experimented with various heuristic greedy algorithms for code shortening. For most of the codes used in the NETtalk, ISOLET, and Letter Recognition domains, we have used a combination of simple greedy algorithms and manual intervention to design good shortened BCH codes.\n\nIn each of the data sets that we studied, we designed a series of error-correcting codes of increasing lengths. We executed each learning algorithm for each of these codes. We stopped lengthening the codes when performance appeared to be leveling off.\n\n### 2.4 Making Classification Decisions\n\nEach approach to solving multiclass problems--direct multiclass, one-per-class, and error-correcting output coding--assumes a method for classifying new examples. For the C4.5 direct multiclass approach, the C4.5 system computes a class probability estimate for each new example. This estimates the probability that that example belongs to each of the \\(k\\) classes. C4.5 then chooses the class having the highest probability as the class of the example.\n\nFor the one-per-class approach, each decision tree or neural network output unit can be viewed as computing the probability that the new example belongs to its corresponding class. The class whose decision tree or output unit gives the highest probability estimate is chosen as the predicted class. Ties are broken arbitrarily in favor of the class that comes first in the class ordering.\n\nFor the error-correcting output code approach, each decision tree or neural network output unit can be viewed as computing the probability that its corresponding bit in the codeword is one. Call these probability values \\(B=\\langle b_{1},b_{2},\\ldots,b_{n}\\rangle\\), where \\(n\\) is the length of the codewords in the error-correcting code. To classify a new example, we compute the \\(L^{1}\\) distance between this probability vector \\(B\\) and each of the codewords \\(W_{i}\\) (\\(i=1\\ldots k\\)) in the error correcting code. The \\(L^{1}\\) distance between \\(B\\) and \\(W_{i}\\) is defined as\n\n\\[L^{1}(B,W_{i})=\\sum_{j=0}^{L}|b_{j}-W_{i,j}|.\\]\n\nThe class whose codeword has the smallest \\(L^{1}\\) distance to \\(B\\) is assigned as the class of the new example. Ties are broken arbitrarily in favor of the class that comes first in the class ordering.\n\n## 3 Results\n\nWe now present the results of our experiments. We begin with the results for decision trees. Then, we consider neural networks. Finally, we report the results of a series of experiments to assess the robustness of the error-correcting output code method.\n\n### Decision Trees\n\nFigure 2 shows the performance of C4.5 in all eight domains. The horizontal line corresponds to the performance of the standard multiclass decision-tree algorithm. The light bar shows the performance of the one-per-class approach, and the dark bar shows the performance of the ECO.C approach with the longest error-correcting code tested. Performance is displayed as the number of percentage points by which each pair of algorithms differ. An asterisk indicates that the difference is statistically significant at the \\(p<0.05\\) level according to the test for the difference of two proportions (using the normal approximation to the binomial distribution, see Snedecor & Cochran, 1989, p. 124).\n\nFrom this figure, we can see that the one-per-class method performs significantly worse than the multiclass method in four of the eight domains and that its behavior is statistically indistinguishable in the remaining four domains. Much more encouraging is the observation that the error-correcting output code approach is significantly superior to the multiclass approach in six of the eight domains and indistinguishable in the remaining two.\n\nFigure 2: Performance (in percentage points) of the one-per-class and ECO.C methods relative to the direct multiclass method using C4.5. Asterisk indicates difference is significant at the \\(0.05\\) level or better.\n\nIn the NETtalk domain, we can also consider the performance of the meaningful distributed representation developed by Sejnowski and Rosenberg. This representation gave 66.7% correct classification as compared with 68.6% for the one-per-class configuration, 70.0% for the direct-multiclass configuration, and 74.3% for the ECOC configuration. The differences in each of these figures are statistically significant at the 0.05 level or better except that the one-per-class and direct-multiclass configurations are not statistically distinguishable.\n\n### 3.2 Backpropagation\n\nFigure 3 shows the results for backpropagation in five of the most challenging domains. The horizontal line corresponds to the performance of the one-per-class encoding for this method. The bars show the number of percentage points by which the error-correcting output coding representation outperforms the one-per-class representation. In four of the five domains, the ECOC encoding is superior; the differences are statistically significant in the Vowel, NETtalk, and ISOLET domains.2\n\nFootnote 2: The difference for ISOLET is only detectable using a test for paired differences of proportions. See Snedecor & Cochran (1989, p. 122.).\n\nIn the letter recognition domain, we encountered great difficulty in successfully training networks using the CNAPS machine, particularly for the ECOC configuration. Experiments showed that the problem arose from the fact that the CNAPS implementation of backpropagation employs a fixed learning rate. We therefore switched to the much slower opt program, which chooses the learning rate adaptively via conjugate-gradient line searches. This behaved better for both the one-per-class and ECOC configurations.\n\nWe also had some difficulty training ISOLET in the ECOC configuration on large networks (182 units), even with the opt program. Some sets of initial random weights led to local minima and poor performance on the validation set.\n\nIn the NETtalk task, we can again compare the performance of the Sejnowski-Rosenberg distributed encoding to the one-per-class and ECOC encodings. The distributed encoding yielded a performance of 71.5% correct, compared to 72.9% for the one-per-class encoding, and 74.9% for the ECOC encoding. The difference between the distributed encoding and the one-per-class encoding is not statistically significant. From these results and the previous results for C4.5, we can conclude that the distributed encoding has no advantages over the one-per-class and ECOC encoding in this domain.\n\n### 3.3 Robustness\n\nThese results show that the ECOC approach performs as well as, and often better than, the alternative approaches. However, there are several important questions that must be answered before we can recommend the ECOC approach without reservation:\n\n* Do the results hold for small samples? We have found that decision trees learned using error-correcting codes are much larger than those learned using the one-per-class or multiclass approaches. This suggests that with small sample sizes, the ECOC method may not perform as well, since complex trees usually require more data to be learned reliably. On the other hand, the experiments described above covered a wide range of training set sizes, which suggests that the results may not depend on having a large training set.\n* Do the results depend on the particular assignment of codewords to classes? The codewords were assigned to the classes arbitrarily in the experiments reported above, which suggests that the particular assignment may not be important. However, some assignments might still be much better than others.\n* Do the results depend on whether pruning techniques are applied to the decision-tree algorithms? Pruning methods have been shown to improve the performance of multiclass C4.5 in many domains.\n* Can the ECOC approach provide class probability estimates? Both C4.5 and back-propagation can be configured to provide estimates of the probability that a test example belongs to each of the \\(k\\) possible classes. Can the ECOC approach do this as well?\n\n#### 3.3.1 Small sample performance\n\nAs we have noted, we became concerned about the small sample performance of the ECOC method when we noticed that the ECOC method always requires much larger decision trees than the OPC method. Table 7 compares the sizes of the decision trees learned by C4.5 under the multiclass, one-per-class, and ECOC configurations for the letter recognition task and the NETtalk task. For the OPC and ECOC configurations, the tables show the average number of leaves in the trees learned for each bit position of the output representation. For\n\nFigure 3: Performance of the ECOC method relative to the one-per-class using backpropagation. Asterisk indicates difference is significant at the 0.05 level or better.\n\nletter recognition, the trees learned for a 207-bit ECOC are more than six times larger than those learned for the one-per-class representation. For the phoneme classification part of NETtalk, the ECOC trees are 14 times larger than the OPC trees. Another way to compare the sizes of the trees is to consider the total number of leaves in the trees. The tables clearly show that the multiclass approach requires much less memory (many fewer total leaves) than either the OPC or the ECOC approaches.\n\nWith backpropagation, it is more difficult to determine the amount of \"network resources\" that are consumed in training the network. One approach is to compare the number of hidden units that give the best generalization performance. In the ISO LET task, for example, the one-per-class encoding attains peak validation set performance with a 78-hidden-unit network, whereas the 30-bit error-correcting encoding attained peak validation set performance with a 156-hidden-unit network. In the letter recognition task, peak performance for the one-per-class encoding was obtained with a network of 120-hidden units compared to 200 hidden units for a 62-bit error-correcting output code.\n\nFrom the decision tree and neural network sizes, we can see that, in general, the error-correcting output representation requires more complex hypotheses than the one-per-class representation. From learning theory and statistics, we known that complex hypotheses typically require more training data than simple ones. On this basis, one might expect that the performance of the ECOC method would be very poor with small training sets. To test this prediction, we measured performance as a function of training set size in two of the larger domains: NETtalk and letter recognition.\n\nFigure 4 presents learning curves for C4.5 on the NETtalk and letter recognition tasks, which show accuracy for a series of progressively larger training sets. From the figure it is clear that the 61-bit error-correcting code consistently outperforms the other two configurations by a nearly constant margin. Figure 5 shows corresponding results for backpropagation on the NETtalk and letter recognition tasks. On the NETtalk task, the results are the same: sample size has no apparent influence on the benefits of error-correcting output coding. However, for the letter-recognition task, there appears to be an interaction.\n\n\\begin{table}\n\\begin{tabular}{l r r r} \\hline \\hline Letter Recognition & Leaves per bit & Total leaves \\\\ \\hline Multiclass & & & 2353 \\\\ One-per-class & & 242 & 6292 \\\\\n207-bit ECOC & & 1606 & 332383 \\\\ \\hline \\hline NETtalk & \\multicolumn{2}{c}{Leaves per bit} & \\multicolumn{2}{c}{Total leaves} \\\\ \\cline{2-4}  & phoneme & stress & phoneme & stress \\\\ \\hline Multiclass & & & 1425 & 1567 \\\\ One-per-Class & 61 & 600 & 3320 & 3602 \\\\\n159-bit ECOC & 901 & 911 & 114469 & 29140 \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 7: Size of decision trees learned by C4.5 for the letter recognition task and the NETtalk task.\n\nError-correcting output coding works best for small training sets, where there is a statistically significant benefit. With the largest training set--16,000 examples--the one-per-class method very slightly outperforms the ECOC method.\n\nFrom these experiments, we conclude that error-correcting output coding works very well with small samples, despite the increased size of the decision trees and the increased complexity of training neural networks. Indeed, with backpropagation on the letter recognition task, error-correcting output coding worked better for small samples than it did for\n\nFigure 4: Accuracy of C4.5 in the multiclass, one-per-class, and error-correcting output coding configurations for increasing training set sizes in the NETtalk and letter recognition task. Note that the horizontal axis is plotted on a logarithmic scale.\n\nFigure 5: Accuracy of backpropagation in the one-per-class and error-correcting output coding configurations for increasing training set sizes on the NETtalk and letter recognition tasks.\n\nlarge ones. This effect suggests that ECOC works by reducing the variance of the learning algorithm. For small samples, the variance is higher, so ECOC can provide more benefit.\n\n#### 3.3.2 Assignment of Codewords to Classes\n\nIn all of the results reported thus far, the codewords in the error-correcting code have been arbitrarily assigned to the classes of the learning task. We conducted a series of experiments in the NETtalk domain with C4.5 to determine whether randomly reassigning the codewords to the classes had any effect on the success of ECOC. Table 8 shows the results of five random assignments of codewords to classes. There is no statistically significant variation in the performance of the different random assignments. This is consistent with similar experiments reported in Bakiri (1991).\n\n#### 3.3.3 Effect of Tree Pruning\n\nPruning of decision trees is an important technique for preventing overfitting. However, the merit of pruning varies from one domain to another. Figure 6 shows the change in performance due to pruning in each of the eight domains and for each of the three configurations studied in this paper: multiclass, one-per-class, and error-correcting output coding.\n\nFrom the figure, we see that in most cases pruning makes no statistically significant difference in performance (aside from the POS task, where it decreases the performance of all three configurations). Aside from POS, only one of the statistically significant changes involves the ECOC configuration, while two affect the one-per-class configuration, and one affects the multiclass configuration. These data suggest that pruning only occasionally has a major effect on _any_ of these configurations. There is no evidence to suggest that pruning affects one configuration more than another.\n\n#### 3.3.4 Class Probability Estimates\n\nIn many applications, it is important to have a classifier that cannot only classify new cases well but also estimate the probability that a new case belongs to each of the \\(k\\) classes. For example, in medical diagnosis, a simple classifier might classify a patient as \"healthy\" because, given the input features, that is the most likely class. However, if there is a non-zero probability that the patient has a life-threatening disease, the right choice for the physician may still be to prescribe a therapy for that disease.\n\nA more mundane example involves automated reading of handwritten postal codes on envelopes. If the classifier is very confident of its classification (i.e., because the estimated\n\n\\begin{table}\n\\begin{tabular}{|c|c|c|c|c|c|c|} \\hline  & & \\multicolumn{3}{c|}{61-Bit Error-Correcting Code Replications} \\\\ \\cline{3-7} Multiclass & One-per-class & a & b & c & d & e \\\\ \\hline\n70.0 & 68.6 & 73.8 & 73.6 & 73.5 & 73.8 & 73.3 \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 8: Five random assignments of codewords to classes for the NETtalk task. Each column shows the percentage of letters correctly classified by C4.5 decision trees.\n\nFigure 6: Change in percentage points of the performance of C4.5 with and without pruning in three configurations. Horizontal line indicates performance with no pruning. Asterisk indicates that the difference is significant at the 0.05 level or better.\n\nprobabilities are very strong), then it can proceed to route the envelope. However, if it is uncertain, then the envelope should be \"rejected\", and sent to a human being who can attempt to read the postal code and process the envelope (Wilkinson, Geist, Janet, et al., 1992).\n\nOne way to assess the quality of the class probability estimates of a classifier is to compute a \"rejection curve\". When the learning algorithm classifies a new case, we require it to also output a \"confidence\" level. Then we plot a curve showing the percentage of correctly classified test cases whose confidence level exceeds a given value. A rejection curve that increases smoothly demonstrates that the confidence level produced by the algorithm can be transformed into an accurate probability measure.\n\nFor one-per-class neural networks, many researchers have found that the difference in activity between the class with the highest activity and the class with the second-highest activity is a good measure of confidence (e.g., LeCun et al., 1989). If this difference is large, then the chosen class is clearly much better than the others. If the difference is small, then the chosen class is nearly tied with another class. This same measure can be applied to the class probability estimates produced by C4.5.\n\nAn analogous measure of confidence for error-correcting output codes can be computed from the \\(L^{1}\\) distance between the vector \\(B\\) of output probabilities for each bit and the codewords of each of the classes. Specifically, we employ the difference between the \\(L^{1}\\) distance to the second-nearest codeword and the \\(L^{1}\\) distance to the nearest codeword as our confidence measure. If this difference is large, an algorithm can be quite confident of its classification decision. If the difference is small, the algorithm is not confident.\n\nFigure 7 compares the rejection curves for various configurations of C4.5 and backpropagation on the NETtalk task. These curves are constructed by first running all of the test examples through the learned decision trees and computing the predicted class of each example and the confidence value for that prediction. To generate each point along the curve, a value is chosen for a parameter \\(\\theta\\), which defines the minimum required confidence. The classified test examples are then processed to determine the percentage of test examples whose confidence level is less than \\(\\theta\\) (these are \"rejected\") and the percentage of the remaining examples that are correctly classified. The value of \\(\\theta\\) is progressively incremented (starting at 0) until all test examples are rejected.\n\nThe lower left portion of the curve shows the performance of the algorithm when \\(\\theta\\) is small, so only the least confident cases are rejected. The upper right portion of the curve shows the performance when \\(\\theta\\) is large, so only the most confident cases are classified. Good class probability estimates produce a curve that rises smoothly and monotonically. A flat or decreasing region in a rejection curve reveals cases where the confidence estimate of the learning algorithm is unrelated or inversely related to the actual performance of the algorithm.\n\nThe rejection curves often terminate prior to rejecting 100% of the examples. This occurs when the final increment in \\(\\theta\\) causes _all_ examples to be rejected. This gives some idea of the number of examples for which the algorithm was highly confident of its classifications. If the curve terminates early, this shows that there were very few examples that the algorithm could confidently classify.\n\nIn Figure 7, we see that--with the exception of the Multiclass configuration--the rejection curves for all of the various configurations of C4.5 increase fairly smoothly, so all of them are producing acceptable confidence estimates. The two error-correcting configurations have smooth curves that remain above all of the other configurations. This shows that the performance advantage of error-correcting output coding is maintained at all confidence levels--ECOC improves classification decisions on all examples, not just the borderline ones.\n\nSimilar behavior is seen in the rejection curves for backpropagation. Again all configurations of backpropagation give fairly smooth rejection curves. However, note that the 159-bit code actually decreases at high rejection rates. By contrast, the 61-bit code gives a monotonic curve that eventually reaches 100%. We have seen this behavior in several of the cases we have studied: extremely long error-correcting codes are usually the best method at low rejection rates, but at high rejection rates, codes of \"intermediate\" length (typically 60-80 bits) behave better. We have no explanation for this behavior.\n\nFigure 8 compares the rejection curves for various configurations of C4.5 and backpropagation on the ISOLET task. Here we see that the ECOC approach is markedly superior to either the one-per-class or multiclass approaches. This figure illustrates another phenomenon we have frequently observed: the curve for multiclass C4.5 becomes quite flat and terminates very early, and the one-per-class curve eventually surpasses it. This suggests that there may be opportunities to improve the class probability estimates produced by C4.5 on multiclass trees. (Note that we employed \"softened thresholds\" in these experiments.) In the backpropagation rejection curves, the ECOC approach consistently outperforms the one-per-class approach until both are very close to 100% correct. Note that both configurations of backpropagation can confidently classify more than 50% of the test examples with 100% accuracy.\n\nFrom these graphs, it is clear that the error-correcting approach (with codes of intermediate length) can provide confidence estimates that are at least as good as those provided by the standard approaches to multiclass problems.\n\nFigure 7: Rejection curves for various configurations of C4.5 and backpropagation on the NETtalk task. The \u201cDistributed\u201d curve plots the behavior of the Sejnowski-Rosenberg distributed representation.\n\n## 4 Conclusions\n\nIn this paper, we experimentally compared four approaches to multiclass learning problems: multiclass decision trees, the one-per-class (OPC) approach, the meaningful distributed output approach, and the error-correcting output coding (ECOC) approach. The results clearly show that the ECOC approach is superior to the other three approaches. The improvements provided by the ECOC approach can be quite substantial: improvements on the order of ten percentage points were observed in several domains. Statistically significant improvements were observed in six of eight domains with decision trees and three of five domains with backpropagation.\n\nThe improvements were also robust:\n\n* ECOC improves both decision trees and neural networks;\n* ECOC provides improvements even with very small sample sizes; and\n* The improvements do not depend on the particular assignment of codewords to classes.\n\nThe error-correcting approach can also provide estimates of the confidence of classification decisions that are at least as accurate as those provided by existing methods.\n\nThere are some additional costs to employing error-correcting output codes. Decision trees learned using ECOC are generally much larger and more complex than trees constructed using the one-per-class or multiclass approaches. Neural networks learned using ECOC often require more hidden units and longer and more careful training to obtain the improved performance (see Section 3.2). These factors may argue against using error-correcting output coding in some domains. For example, in domains where it is important for humans to understand and interpret the induced decision trees, ECOC methods are not appropriate, because they produce such complex trees. In domains where training must be rapid and completely autonomous, ECOC methods with backpropagation cannot be recommended, because of the potential for encountering difficulties during training.\n\nFigure 8: Rejection curves for various configurations of C4.5 and backpropagation on the ISOLET task.\n\nFinally, we found that error-correcting codes of intermediate length tend to give better confidence estimates than very long error-correcting codes, even though the very long codes give the best generalization performance.\n\nThere are many open problems that require further research. First and foremost, it is important to obtain a deeper understanding of why the ECOC method works. If we assume that each of the learned hypotheses makes classification errors independently, then coding theory provides the explanation: individual errors can be corrected because the codewords are \"far apart\" in the output space. However, because each of the hypotheses is learned using the same algorithm on the same training data, we would expect that the errors made by individual hypotheses would be highly correlated, and such errors cannot be corrected by an error-correcting code. So the key open problem is to understand why the classification errors at different bit positions are fairly independent. How does the error-correcting output code result in this independence?\n\nA closely related open problem concerns the relationship between the ECOC approach and various \"ensemble\", \"committee\", and \"boosting\" methods (Perone & Cooper, 1993; Schapire, 1990; Freund, 1992). These methods construct multiple hypotheses which then \"vote\" to determine the classification of an example. An error-correcting code can also be viewed as a very compact form of voting in which a certain number of incorrect votes can be corrected. An interesting difference between standard ensemble methods and the ECOC approach is that in the ensemble methods, each hypothesis is attempting to predict the same function, whereas in the ECOC approach, each hypothesis predicts a different function. This may reduce the correlations between the hypotheses and make them more effective \"voters.\" Much more work is needed to explore this relationship.\n\nAnother open question concerns the relationship between the ECOC approach and the flexible discriminant analysis technique of Hastie, Tibshirani, and Buja (In Press). Their method first employs the one-per-class approach (e.g., with neural networks) and then applies a kind of discriminant analysis to the outputs. This discriminant analysis maps the outputs into a \\(k-1\\) dimensional space such that each class has a defined \"center point\". New cases are classified by mapping them into this space and then finding the nearest \"center point\" and its class. These center points are similar to our codewords but in a continuous space of dimension \\(k-1\\). It may be that the ECOC method is a kind of randomized, higher-dimensional variant of this approach.\n\nFinally, the ECOC approach shows promise of scaling neural networks to very large classification problems (with hundreds or thousands of classes) much better than the one-per-class method. This is because a good error-correcting code can have a length \\(n\\) that is much less than the total number of classes, whereas the one-per-class approach requires that there be one output unit for each class. Networks with thousands of output units would be expensive and difficult to train. Future studies should test the scaling ability of these different approaches to such large classification tasks.\n\n## Acknowledgements\n\nThe authors thank the anonymous reviewers for their valuable suggestions which improved the presentation of the paper. The authors also thank Prasad Tadepalli for proof-reading the final manuscript. The authors gratefully acknowledge the support of the National Science Foundation under grants numbered IRI-8667316, CDA-9216172, and IRI-9204129. Bakiri also thanks Bahrain University for its support of his doctoral research.\n\n## References\n\n* Adaptive Solutions (1992) Adaptive Solutions (1992). CNAPS back-propagation guide. Tech. rep. 801-20030-04, A daptive Solutions, Inc., Beaverton, OR.\n* Bakiri (1991) Bakiri, G. (1991). Converting English text to speech: A machine learning approach. Tech. rep. 91-30-2, Department of Computer Science, Oregon State University, Corvallis, OR.\n* Barnard Cole (1989) Barnard, E., & Cole, R. A. (1989). A neural-net training program based on conjugate-gradient optimization. Tech. rep. CSE 89-014, Oregon Graduate Institute, Beaverton, OR.\n* Bose & Ray-Chaudhuri (1960) Bose, R. C., & Ray-Chaudhuri, D. K. (1960). On a class of error-correcting binary group codes. _Information and Control_, \\(3\\), 68-79.\n* Breiman et al. (1984) Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). _Classification and Regression Trees_. Wadsworth International Group.\n* Bridle (1990) Bridle, J. S. (1990). Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. In Touretzky, D. S. (Ed.), _Neural Information Processing Systems_, Vol. 2, pp. 211-217 San Francisco, CA. Morgan Kaufmann.\n* Cardie (1993) Cardie, C. (1993). Using decision trees to improve case-based learning. In _Proceedings of the Tenth International Conference on Machine Learning_, pp. 17-24 San Francisco, CA. Morgan Kaufmann.\n* Duda et al. (1963) Duda, R. O., Machanik, J. W., & Singleton, R. C. (1963). Function modeling experiments. Tech. rep. 3605, Stanford Research Institute.\n* Freund (1992) Freund, Y. (1992). An improved boosting algorithm and its implications on learning complexity. In _Proc. 5th Annu. Workshop on Comput. Learning Theory_, pp. 391-398. ACM Press, New York, NY.\n* Hampshire II & Waibel (1990) Hampshire II, J. B., & Waibel, A. H. (1990). A novel objective function for improved phoneme recognition using time-delay neural networks. _IEEE Transactions on Neural Networks_, _1_(2), 216-228.\n* Hastie et al. (2011) Hastie, T., Tibshirani, R., & Buja, A. (In Press). Flexible discriminant analysis by optimal scoring. _Journal of the American Statistical Association_.\n* Hinton (1989) Hinton, G. (1989). Connectionist learning procedures. _Artificial Intelligence_, _40_, 185-234.\n* Hocquenghem (1959) Hocquenghem, A. (1959). Codes corecteurs d'erreurs. _Chiffres_, \\(2\\), 147-156.\n* Hellinger (1959)* Kong & Dietterich (1995) Kong, E. B., & Dietterich, T. G. (1995). Why error-correcting output coding works with decision trees. Tech. rep., Department of Computer Science, Oregon State University, Corvallis, OR.\n* Lang et al. (1990) Lang, K. J., Hinton, G. E., & Waibel, A. (1990). A time-delay neural network architecture for isolated word recognition. _Neural Networks_, _3_(1), 23-43.\n* LeCun et al. (1989) LeCun, Y., Boser, B., Denker, J. S., Henderson, B., Howard, R. E., Hubbard, W., & Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition. _Neural Computation_, _1_(4), 541-551.\n* Murphy & Aha (1994) Murphy, P., & Aha, D. (1994). UCI repository of machine learning databases [machine-readable data repository]. Tech. rep., University of California, Irvine.\n* Natarajan (1991) Natarajan, B. K. (1991). _Machine Learning: A Theoretical Approach_. Morgan Kaufmann, San Mateo, CA.\n* Nilsson (1965) Nilsson, N. J. (1965). _Learning Machines_. McGraw-Hill, New York.\n* Perrone & Cooper (1993) Perrone, M. P., & Cooper, L. N. (1993). When networks disagree: Ensemble methods for hybrid neural networks. In Mammone, R. J. (Ed.), _Neural networks for speech and image processing_. Chapman and Hall.\n* Peterson & Weldon (1972) Peterson, W. W., & Weldon, Jr., E. J. (1972). _Error-Correcting Codes_. MIT Press, Cambridge, MA.\n* Quinlan (1993) Quinlan, J. R. (1993). _C4.5: Programs for Empirical Learning_. Morgan Kaufmann, San Francisco, CA.\n* Richard & Lippmann (1991) Richard, M. D., & Lippmann, R. P. (1991). Neural network classifiers estimate bayesian _a posteriori_ probabilities. _Neural Computation_, _3_(4), 461-483.\n* Rosenblatt (1958) Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. _Psychological Review_, _65_(6), 386-408.\n* Explorations in the Mieroslructure of Cognition_, chap. 8, pp. 318-362. MIT Press.\n* Schapire (1990) Schapire, R. E. (1990). The strength of weak learnability. _Machine Learning_, _5_(2), 197-227.\n* Sejnowski & Rosenberg (1987) Sejnowski, T. J., & Rosenberg, C. R. (1987). Parallel networks that learn to pronounce english text. _Journal of Complex Syslems_, _1_(1), 145-168.\n* Selman et al. (1992) Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satisfiability problems. In _Proceedings of AAAI-92_, pp. 440-446. AAAI/MIT Press.\n* Snedeor & Cochran (1989) Snedeor, G. W., & Cochran, W. G. (1989). _Statistical Methods_. Iowa State University Press, Ames, IA. Eighth Edition.\n* Valiant (1984) Valiant, L. G. (1984). A theory of the learnable. _Commun. ACM_, _27_(11), 1134-1142.\n* Snedeor & Cochran (1989)* Waibel et al. (1989) Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. (1989). Phoneme recognition using time-delay networks. _IEEE Transactions on Acoustics, Speech, and Signal Processing_, _37_(3), 328-339.\n* Weigend (1993) Weigend, A. (1993). Measuring the effective number of dimensions during backpropagation training. In _Proceedings of the 1993 Connectionist Models Summer School_, pp. 335-342. Morgan Kaufmann, San Francisco, CA.\n* Wilkinson et al. (1992) Wilkinson, R. A., Geist, J., Janet, S., et al. (1992). The first census optical character recognition systems conference. Tech. rep. NISTIR 4912, National Institute of Standards and Technology."}
{"text": "# Reinforcement Learning: A Survey\n\nLeslie Pack Kaelbling\n\nMichael L. Littman\n\nComputer Science Department, Box 1910, Brown University\n\nProvidence, RI 02912-1910 USA\n\nAndrew W. Moore\n\nAWM@cs.cmu.edu\n\nSmith Hall 221, Carnegie Mellon University, 5000 Forbes Avenue\n\nPittsburgh, PA 15213 USA\n\n###### Abstract\n\nThis paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word \"reinforcement.\" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.\n\n2000 4 1996 237 285 1\n\n## 1 Introduction\n\nReinforcement learning dates back to the early days of cybernetics and work in statistics, psychology, neuroscience, and computer science. In the last five to ten years, it has attracted rapidly increasing interest in the machine learning and artificial intelligence communities. Its promise is beguiling--a way of programming agents by reward and punishment without needing to specify _how_ the task is to be achieved. But there are formidable computational obstacles to fulfilling the promise.\n\nThis paper surveys the historical basis of reinforcement learning and some of the current work from a computer science perspective. We give a high-level overview of the field and a taste of some specific approaches. It is, of course, impossible to mention all of the important work in the field; this should not be taken to be an exhaustive account.\n\nReinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment. The work described here has a strong family resemblance to eponymous work in psychology, but differs considerably in the details and in the use of the word \"reinforcement.\" It is appropriately thought of as a class of problems, rather than as a set of techniques.\n\nThere are two main strategies for solving reinforcement-learning problems. The first is to search in the space of behaviors in order to find one that performs well in the environment. This approach has been taken by work in genetic algorithms and genetic programming,as well as some more novel search techniques (Schmidhuber, 1996). The second is to use statistical techniques and dynamic programming methods to estimate the utility of taking actions in states of the world. This paper is devoted almost entirely to the second set of techniques because they take advantage of the special structure of reinforcement-learning problems that is not available in optimization problems in general. It is not yet clear which set of approaches is best in which circumstances.\n\nThe rest of this section is devoted to establishing notation and describing the basic reinforcement-learning model. Section 2 explains the trade-off between exploration and exploitation and presents some solutions to the most basic case of reinforcement-learning problems, in which we want to maximize the immediate reward. Section 3 considers the more general problem in which rewards can be delayed in time from the actions that were crucial to gaining them. Section 4 considers some classic model-free algorithms for reinforcement learning from delayed reward: adaptive heuristic critic, \\(TD(\\lambda)\\) and Q-learning. Section 5 demonstrates a continuum of algorithms that are sensitive to the amount of computation an agent can perform between actual steps of action in the environment. Generalization--the cornerstone of mainstream machine learning research--has the potential of considerably aiding reinforcement learning, as described in Section 6. Section 7 considers the problems that arise when the agent does not have complete perceptual access to the state of the environment. Section 8 catalogs some of reinforcement learning's successful applications. Finally, Section 9 concludes with some speculations about important open problems and the future of reinforcement learning.\n\n### Reinforcement-Learning Model\n\nIn the standard reinforcement-learning model, an agent is connected to its environment via perception and action, as depicted in Figure 1. On each step of interaction the agent receives as input, \\(i\\), some indication of the current state, \\(s\\), of the environment; the agent then chooses an action, \\(a\\), to generate as output. The action changes the state of the environment, and the value of this state transition is communicated to the agent through a scalar _reinforcement signal_, \\(r\\). The agent's behavior, \\(B\\), should choose actions that tend to increase the long-runs sum of values of the reinforcement signal. It can learn to do this over time by systematic trial and error, guided by a wide variety of algorithms that are the subject of later sections of this paper.\n\nFigure 1: The standard reinforcement-learning model.\n\n[MISSING_PAGE_EMPTY:626]\n\nSome aspects of reinforcement learning are closely related to search and planning issues in artificial intelligence. AI search algorithms generate a satisfactory trajectory through a graph of states. Planning operates in a similar manner, but typically within a construct with more complexity than a graph, in which states are represented by compositions of logical expressions instead of atomic symbols. These AI algorithms are less general than the reinforcement-learning methods, in that they require a predefined model of state transitions, and with a few exceptions assume determinism. On the other hand, reinforcement learning, at least in the kind of discrete cases for which theory has been developed, assumes that the entire statespace can be enumerated and stored in memory--an assumption to which conventional search algorithms are not tied.\n\n### Models of Optimal Behavior\n\nBefore we can start thinking about algorithms for learning to behave optimally, we have to decide what our model of optimality will be. In particular, we have to specify how the agent should take the future into account in the decisions it makes about how to behave now. There are three models that have been the subject of the majority of work in this area.\n\nThe _finite-horizon_ model is the easiest to think about; at a given moment in time, the agent should optimize its expected reward for the next \\(h\\) steps:\n\n\\[E(\\sum_{t=0}^{h}r_{t})\\enspace;\\]\n\nit need not worry about what will happen after that. In this and subsequent expressions, \\(r_{t}\\) represents the scalar reward received \\(t\\) steps into the future. This model can be used in two ways. In the first, the agent will have a non-stationary policy; that is, one that changes over time. On its first step it will take what is termed a _h-step optimal action_. This is defined to be the best action available given that it has \\(h\\) steps remaining in which to act and gain reinforcement. On the next step it will take a \\((h-1)\\)-step optimal action, and so on, until it finally takes a \\(1\\)-step optimal action and terminates. In the second, the agent does _receding-horizon control_, in which it always takes the \\(h\\)-step optimal action. The agent always acts according to the same policy, but the value of \\(h\\) limits how far ahead it looks in choosing its actions. The finite-horizon model is not always appropriate. In many cases we may not know the precise length of the agent's life in advance.\n\nThe infinite-horizon discounted model takes the long-run reward of the agent into account, but rewards that are received in the future are geometrically discounted according to discount factor \\(\\gamma\\), (where \\(0\\leq\\gamma<1\\)):\n\n\\[E(\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t})\\enspace.\\]\n\nWe can interpret \\(\\gamma\\) in several ways. It can be seen as an interest rate, a probability of living another step, or as a mathematical trick to bound the infinite sum. The model is conceptually similar to receding-horizon control, but the discounted model is more mathematically tractable than the finite-horizon model. This is a dominant reason for the wide attention this model has received.\n\nAnother optimality criterion is the _average-reward model_, in which the agent is supposed to take actions that optimize its long-run average reward:\n\n\\[\\lim_{h\\to\\infty}E\\left(\\frac{1}{h}\\sum_{t=0}^{h}r_{t}\\right)\\enspace.\\]\n\nSuch a policy is referred to as a _gain optimal_ policy; it can be seen as the limiting case of the infinite-horizon discounted model as the discount factor approaches 1 (Bertsekas, 1995). One problem with this criterion is that there is no way to distinguish between two policies, one of which gains a large amount of reward in the initial phases and the other of which does not. Reward gained on any initial prefix of the agent's life is overshadowed by the long-run average performance. It is possible to generalize this model so that it takes into account both the long run average and the amount of initial reward than can be gained. In the generalized, _bias optimal_ model, a policy is preferred if it maximizes the long-run average and ties are broken by the initial extra reward.\n\nFigure 2 contrasts these models of optimality by providing an environment in which changing the model of optimality changes the optimal policy. In this example, circles represent the states of the environment and arrows are state transitions. There is only a single action choice from every state except the start state, which is in the upper left and marked with an incoming arrow. All rewards are zero except where marked. Under a finite-horizon model with \\(h=5\\), the three actions yield rewards of \\(+6.0\\), \\(+0.0\\), and \\(+0.0\\), so the first action should be chosen; under an infinite-horizon discounted model with \\(\\gamma=0.9\\), the three choices yield \\(+16.2\\), \\(+59.0\\), and \\(+58.5\\) so the second action should be chosen; and under the average reward model, the third action should be chosen since it leads to an average reward of \\(+11\\). If we change \\(h\\) to \\(1000\\) and \\(\\gamma\\) to \\(0.2\\), then the second action is optimal for the finite-horizon model and the first for the infinite-horizon discounted model; however, the average reward model will always prefer the best long-term average. Since the choice of optimality model and parameters matters so much, it is important to choose it carefully in any application.\n\nThe finite-horizon model is appropriate when the agent's lifetime is known; one important aspect of this model is that as the length of the remaining lifetime decreases, the agent's policy may change. A system with a hard deadline would be appropriately modeled this way. The relative usefulness of infinite-horizon discounted and bias-optimal models is still under debate. Bias-optimality has the advantage of not requiring a discount parameter; however, algorithms for finding bias-optimal policies are not yet as well-understood as those for finding optimal infinite-horizon discounted policies.\n\n### Measuring Learning Performance\n\nThe criteria given in the previous section can be used to assess the policies learned by a given algorithm. We would also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use.\n\n* **Eventual convergence to optimal.** Many algorithms come with a provable guarantee of asymptotic convergence to optimal behavior (Watkins & Dayan, 1992). This is reassuring, but useless in practical terms. An agent that quickly reaches a plateau at 99% of optimality may, in many applications, be preferable to an agent that has a guarantee of eventual optimality but a sluggish early learning rate.\n* **Speed of convergence to optimality.** Optimality is usually an asymptotic result, and so convergence speed is an ill-defined measure. More practical is the _speed of convergence to near-optimality_. This measure begs the definition of how near to optimality is sufficient. A related measure is _level of performance after a given time_, which similarly requires that someone define the given time. It should be noted that here we have another difference between reinforcement learning and conventional supervised learning. In the latter, expected future predictive accuracy or statistical efficiency are the prime concerns. For example, in the well-known PAC framework (Valiant, 1984), there is a learning period during which mistakes do not count, then a performance period during which they do. The framework provides bounds on the necessary length of the learning period in order to have a probabilistic guarantee on the subsequent performance. That is usually an inappropriate view for an agent with a long existence in a complex environment. In spite of the mismatch between embedded reinforcement learning and the train/test perspective, Fiechter (1994) provides a PAC analysis for Q-learning (described in Section 4.2) that sheds some light on the connection between the two views. Measures related to speed of learning have an additional weakness. An algorithm that merely tries to achieve optimality as fast as possible may incur unnecessarily large penalties during the learning period. A less aggressive strategy taking longer to achieve optimality, but gaining greater total reinforcement during its learning might be preferable.\n* **Regret.** A more appropriate measure, then, is the expected decrease in reward gained due to executing the learning algorithm instead of behaving optimally from the very beginning. This measure is known as _regret_ (Berry & Fristedt, 1985). It penalizes mistakes wherever they occur during the run. Unfortunately, results concerning the regret of algorithms are quite hard to obtain.\n\nFigure 2: Comparing models of optimality. All unlabeled arrows produce a reward of zero.\n\n### Reinforcement Learning and Adaptive Control\n\nAdaptive control (Burghes & Graham, 1980; Stengel, 1986) is also concerned with algorithms for improving a sequence of decisions from experience. Adaptive control is a much more mature discipline that concerns itself with dynamic systems in which states and actions are vectors and system dynamics are smooth: linear or locally linearizable around a desired trajectory. A very common formulation of cost functions in adaptive control are quadratic penalties on deviation from desired state and action vectors. Most importantly, although the dynamic model of the system is not known in advance, and must be estimated from data, the _structure_ of the dynamic model is fixed, leaving model estimation as a parameter estimation problem. These assumptions permit deep, elegant and powerful mathematical analysis, which in turn lead to robust, practical, and widely deployed adaptive control algorithms.\n\n## 2 Exploitation versus Exploration: The Single-State Case\n\nOne major difference between reinforcement learning and supervised learning is that a reinforcement-learner must explicitly explore its environment. In order to highlight the problems of exploration, we treat a very simple case in this section. The fundamental issues and approaches described here will, in many cases, transfer to the more complex instances of reinforcement learning discussed later in the paper.\n\nThe simplest possible reinforcement-learning problem is known as the \\(k\\)-armed bandit problem, which has been the subject of a great deal of study in the statistics and applied mathematics literature (Berry & Fristedt, 1985). The agent is in a room with a collection of \\(k\\) gambling machines (each called a \"one-armed bandit\" in colloquial English). The agent is permitted a fixed number of pulls, \\(h\\). Any arm may be pulled on each turn. The machines do not require a deposit to play; the only cost is in wasting a pull playing a suboptimal machine. When arm \\(i\\) is pulled, machine \\(i\\) pays off 1 or 0, according to some underlying probability parameter \\(p_{i}\\), where payoffs are independent events and the \\(p_{i}\\)s are unknown. What should the agent's strategy be?\n\nThis problem illustrates the fundamental tradeoff between exploitation and exploration. The agent might believe that a particular arm has a fairly high payoff probability; should it choose that arm all the time, or should it choose another one that it has less information about, but seems to be worse? Answers to these questions depend on how long the agent is expected to play the game; the longer the game lasts, the worse the consequences of prematurely converging on a sub-optimal arm, and the more the agent should explore.\n\nThere is a wide variety of solutions to this problem. We will consider a representative selection of them, but for a deeper discussion and a number of important theoretical results, see the book by Berry and Fristedt (1985). We use the term \"action\" to indicate the agent's choice of arm to pull. This eases the transition into delayed reinforcement models in Section 3. It is very important to note that bandit problems fit our definition of a reinforcement-learning environment with a single state with only self transitions.\n\nSection 2.1 discusses three solutions to the basic one-state bandit problem that have formal correctness results. Although they can be extended to problems with real-valued rewards, they do not apply directly to the general multi-state delayed-reinforcement case.\n\n[MISSING_PAGE_FAIL:8]\n\nBecause of the guarantee of optimal exploration and the simplicity of the technique (given the table of index values), this approach holds a great deal of promise for use in more complex applications. This method proved useful in an application to robotic manipulation with immediate reward (Salgancoff & Ungar, 1995). Unfortunately, no one has yet been able to find an analog of index values for delayed reinforcement problems.\n\n#### 2.1.3 Learning Automata\n\nA branch of the theory of adaptive control is devoted to _learning automata_, surveyed by Narendra and Thathachar (1989), which were originally described explicitly as finite state automata. The _Tsetlin automaton_ shown in Figure 3 provides an example that solves a 2-armed bandit arbitrarily near optimally as \\(N\\) approaches infinity.\n\nIt is inconvenient to describe algorithms as finite-state automata, so a move was made to describe the internal state of the agent as a probability distribution according to which actions would be chosen. The probabilities of taking different actions would be adjusted according to their previous successes and failures.\n\nAn example, which stands among a set of algorithms independently developed in the mathematical psychology literature (Hilgard & Bower, 1975), is the _linear reward-inaction_ algorithm. Let \\(p_{i}\\) be the agent's probability of taking action \\(i\\).\n\n* When action \\(a_{i}\\) succeeds, \\[\\begin{array}{rcl}p_{i}&:=&p_{i}+\\alpha(1-p_{i})\\\\ p_{j}&:=&p_{j}-\\alpha p_{j}\\;\\;\\mbox{for}\\;j\\neq i\\end{array}\\]\n* When action \\(a_{i}\\) fails, \\(p_{j}\\) remains unchanged (for all \\(j\\)).\n\nThis algorithm converges with probability 1 to a vector containing a single 1 and the rest 0's (choosing a particular action with probability 1). Unfortunately, it does not always converge to the correct action; but the probability that it converges to the wrong one can be made arbitrarily small by making \\(\\alpha\\) small (Narendra & Thathachar, 1974). There is no literature on the regret of this algorithm.\n\nFigure 3: A Tsetlin automaton with \\(2N\\) states. The top row shows the state transitions that are made when the previous action resulted in a reward of 1; the bottom row shows transitions after a reward of 0. In states in the left half of the figure, action 0 is taken; in those on the right, action 1 is taken.\n\n### Ad-Hoc Techniques\n\nIn reinforcement-learning practice, some simple, _ad hoc_ strategies have been popular. They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. Thrun (1992) has surveyed a variety of these techniques.\n\n#### Greedy Strategies\n\nThe first strategy that comes to mind is to always choose the action with the highest estimated payoff. The flaw is that early unlucky sampling might indicate that the best action's reward is less than the reward obtained from a suboptimal action. The suboptimal action will always be picked, leaving the true optimal action starved of data and its superiority never discovered. An agent must explore to ameliorate this outcome.\n\nA useful heuristic is _optimism in the face of uncertainty_ in which actions are selected greedily, but strongly optimistic prior beliefs are put on their payoffs so that strong negative evidence is needed to eliminate an action from consideration. This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrarily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the _exploration bonus_ in Dyna (Sutton, 1990), _curiosity-driven exploration_ (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993).\n\n#### Randomized Strategies\n\nAnother simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability \\(p\\), choose an action at random. Some versions of this strategy start with a large value of \\(p\\) to encourage initial exploration, which is slowly decreased.\n\nAn objection to the simple strategy is that when it experiments with a non-greedy action it is no more likely to try a promising alternative than a clearly hopeless alternative. A slightly more sophisticated strategy is _Boltzmann exploration_. In this case, the expected reward for taking action \\(a\\), \\(ER(a)\\) is used to choose an action probabilistically according to the distribution\n\n\\[P(a)=\\frac{e^{ER(a)/T}}{\\sum_{a^{\\prime}\\in A}e^{ER(a^{\\prime})/T}}\\enspace.\\]\n\nThe _temperature_ parameter \\(T\\) can be decreased over time to decrease exploration. This method works well if the best action is well separated from the others, but suffers somewhat when the values of the actions are close. It may also converge unnecessarily slowly unless the temperature schedule is manually tuned with great care.\n\n#### Interval-based Techniques\n\nExploration is often more efficient when it is based on second-order information about the certainty or variance of the estimated values of actions. Kaelbling's _interval estimation_ algorithm (1993b) stores statistics for each action _a_: \\(w\\) is the number of successes and \\(n\\) is the number of trials. An action is chosen by computing the upper bound of a \\(100\\cdot(1-\\alpha)\\)confidence interval on the success probability of each action and choosing the action with the highest upper bound. Smaller values of the \\(\\alpha\\) parameter encourage greater exploration. When payoffs are boolean, the normal approximation to the binomial distribution can be used to construct the confidence interval (though the binomial should be used for small \\(n\\)). Other payoff distributions can be handled using their associated statistics or with nonparametric methods. The method works very well in empirical trials. It is also related to a certain class of statistical techniques known as _experiment design_ methods (Box & Draper, 1987), which are used for comparing multiple treatments (for example, fertilizers or drugs) to determine which treatment (if any) is best in as small a set of experiments as possible.\n\n### More General Problems\n\nWhen there are multiple states, but reinforcement is still immediate, then any of the above solutions can be replicated, once for each state. However, when generalization is required, these solutions must be integrated with generalization methods (see section 6); this is straightforward for the simple ad-hoc methods, but it is not understood how to maintain theoretical guarantees.\n\nMany of these techniques focus on converging to some regime in which exploratory actions are taken rarely or never; this is appropriate when the environment is stationary. However, when the environment is non-stationary, exploration must continue to take place, in order to notice changes in the world. Again, the more ad-hoc techniques can be modified to deal with this in a plausible manner (keep temperature parameters from going to 0; decay the statistics in interval estimation), but none of the theoretically guaranteed methods can be applied.\n\n## 3 Delayed Reward\n\nIn the general case of the reinforcement learning problem, the agent's actions determine not only its immediate reward, but also (at least probabilistically) the next state of the environment. Such environments can be thought of as networks of bandit problems, but the agent must take into account the next state as well as the immediate reward when it decides which action to take. The model of long-run optimality the agent is using determines exactly how it should take the value of the future into account. The agent will have to be able to learn from delayed reinforcement: it may take a long sequence of actions, receiving insignificant reinforcement, then finally arrive at a state with high reinforcement. The agent must be able to learn which of its actions are desirable based on reward that can take place arbitrarily far in the future.\n\n### Markov Decision Processes\n\nProblems with delayed reinforcement are well modeled as _Markov decision processes_ (MDPs). An MDP consists of\n\n* a set of states \\(\\mathcal{S}\\),\n* a set of actions \\(\\mathcal{A}\\),* a reward function \\(R:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\Re\\), and\n* a state transition function \\(T:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\Pi(\\mathcal{S})\\), where a member of \\(\\Pi(\\mathcal{S})\\) is a probability distribution over the set \\(\\mathcal{S}\\) (i.e. it maps states to probabilities). We write \\(T(s,a,s^{\\prime})\\) for the probability of making a transition from state \\(s\\) to state \\(s^{\\prime}\\) using action \\(a\\).\n\nThe state transition function probabilistically specifies the next state of the environment as a function of its current state and the agent's action. The reward function specifies expected instantaneous reward as a function of the current state and action. The model is _Markov_ if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models (Bellman, 1957; Bertsekas, 1987; Howard, 1960; Puterman, 1994).\n\nAlthough general MDPs may have infinite (even uncountable) state and action spaces, we will only discuss methods for solving finite-state and finite-action problems. In section 6, we discuss methods for solving problems with continuous input and output spaces.\n\n### Finding a Policy Given a Model\n\nBefore we consider algorithms for learning to behave in MDP environments, we will explore techniques for determining the optimal policy given a correct model. These dynamic programming techniques will serve as the foundation and inspiration for the learning algorithms to follow. We restrict our attention mainly to finding optimal policies for the infinite-horizon discounted model, but most of these algorithms have analogs for the finite-horizon and average-case models as well. We rely on the result that, for the infinite-horizon discounted model, there exists an optimal deterministic stationary policy (Bellman, 1957).\n\nWe will speak of the optimal _value_ of a state-it is the expected infinite discounted sum of reward that the agent will gain if it starts in that state and executes the optimal policy. Using \\(\\pi\\) as a complete decision policy, it is written\n\n\\[V^{*}(s)=\\max_{\\pi}E\\left(\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\right)\\enspace.\\]\n\nThis optimal value function is unique and can be defined as the solution to the simultaneous equations\n\n\\[V^{*}(s)=\\max_{a}\\left(R(s,a)+\\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T(s,a,s^{ \\prime})V^{*}(s^{\\prime})\\right),\\forall s\\in\\mathcal{S}\\enspace, \\tag{1}\\]\n\nwhich assert that the value of a state \\(s\\) is the expected instantaneous reward plus the expected discounted value of the next state, using the best available action. Given the optimal value function, we can specify the optimal policy as\n\n\\[\\pi^{*}(s)=\\arg\\max_{a}\\left(R(s,a)+\\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T(s,a,s^{\\prime})V^{*}(s^{\\prime})\\right)\\enspace.\\]\n\n#### 3.2.1 Value Iteration\n\nOne way, then, to find an optimal policy is to find the optimal value function. It can be determined by a simple iterative algorithm called _value iteration_ that can be shown to converge to the correct \\(V^{*}\\) values (Bellman, 1957; Bertsekas, 1987).\n\ninitialize \\(V(s)\\) arbitrarily  loop until policy good enough  loop for \\(s\\in\\mathcal{S}\\)  loop for \\(a\\in\\mathcal{A}\\) \\(Q\\left(s,\\,a\\right):=R\\left(s,a\\right)+\\gamma\\,\\sum_{s^{\\prime}\\in\\mathcal{S}} \\,T\\left(s,\\,a,\\,s^{\\prime}\\right)V(s^{\\prime})\\) \\(V(s):=\\max_{a}Q\\left(s,\\,a\\right)\\)  end loop  end loop ```\n\nIt is not obvious when to stop the value iteration algorithm. One important result bounds the performance of the current greedy policy as a function of the _Bellman residual_ of the current value function (Williams & Baird, 1993b). It says that if the maximum difference between two successive value functions is less than \\(\\epsilon\\), then the value of the greedy policy, (the policy obtained by choosing, in every state, the action that maximizes the estimated discounted reward, using the current estimate of the value function) differs from the value function of the optimal policy by no more than \\(2\\epsilon/(1-\\gamma)\\) at any state. This provides an effective stopping criterion for the algorithm. Puterman (1994) discusses another stopping criterion, based on the _span semi-norm_, which may result in earlier termination. Another important result is that the greedy policy is guaranteed to be optimal in some finite number of steps even though the value function may not have converged (Bertsekas, 1987). And in practice, the greedy policy is often optimal long before the value function has converged.\n\nValue iteration is very flexible. The assignments to \\(V\\) need not be done in strict order as shown above, but instead can occur asynchronously in parallel provided that the value of every state gets updated infinitely often on an infinite run. These issues are treated extensively by Bertsekas (1989), who also proves convergence results.\n\nUpdates based on Equation 1 are known as _full backups_ since they make use of information from all possible successor states. It can be shown that updates of the form\n\n\\[Q\\left(s,\\,a\\right):=Q\\left(s,\\,a\\right)+\\alpha\\left(r+\\gamma\\,\\max_{a^{\\prime} }Q\\left(s^{\\prime},\\,a^{\\prime}\\right)-Q\\left(s,\\,a\\right)\\right)\\]\n\ncan also be used as long as each pairing of \\(a\\) and \\(s\\) is updated infinitely often, \\(s^{\\prime}\\) is sampled from the distribution \\(T\\left(s,\\,a,\\,s^{\\prime}\\right)\\), \\(r\\) is sampled with mean \\(R\\left(s,\\,a\\right)\\) and bounded variance, and the learning rate \\(\\alpha\\) is decreased slowly. This type of _sample backup_ (Singh, 1993) is critical to the operation of the model-free methods discussed in the next section.\n\nThe computational complexity of the value-iteration algorithm with full backups, per iteration, is quadratic in the number of states and linear in the number of actions. Commonly, the transition probabilities \\(T\\left(s,\\,a,\\,s^{\\prime}\\right)\\) are sparse. If there are on average a constant number of next states with non-zero probability then the cost per iteration is linear in the number of states and linear in the number of actions. The number of iterations required to reach the optimal value function is polynomial in the number of states and the magnitude of the largest reward if the discount factor is held constant. However, in the worst case the number of iterations grows polynomially in \\(1/(1-\\gamma)\\), so the convergence rate slows considerably as the discount factor approaches 1 (Littman, Dean, & Kaelbling, 1995b).\n\n#### 3.2.2 Policy Iteration\n\nThe _policy iteration_ algorithm manipulates the policy directly, rather than finding it indirectly via the optimal value function. It operates as follows:\n\nchoose an arbitrary policy \\(\\pi^{\\prime}\\) loop \\(\\pi:=\\pi^{\\prime}\\)  compute the value function of policy \\(\\pi\\):  solve the linear equations \\(V_{\\pi}(s)=R(s,\\pi(s))+\\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T\\,(s,\\pi(s),s^{ \\prime})V_{\\pi}(s^{\\prime})\\)  improve the policy at each state: \\(\\pi^{\\prime}(s):=\\arg\\max_{a}\\,(R(s,a)+\\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T \\,(s,a,s^{\\prime})V_{\\pi}(s^{\\prime}))\\) until \\(\\pi=\\pi^{\\prime}\\)\n\nThe value function of a policy is just the expected infinite discounted reward that will be gained, at each state, by executing that policy. It can be determined by solving a set of linear equations. Once we know the value of each state under the current policy, we consider whether the value could be improved by changing the first action taken. If it can, we change the policy to take the new action whenever it is in that situation. This step is guaranteed to strictly improve the performance of the policy. When no improvements are possible, then the policy is guaranteed to be optimal.\n\nSince there are at most \\(|\\mathcal{A}|^{|\\mathcal{S}|}\\) distinct policies, and the sequence of policies improves at each step, this algorithm terminates in at most an exponential number of iterations (Puterman, 1994). However, it is an important open question how many iterations policy iteration takes in the worst case. It is known that the running time is pseudopolynomial and that for any fixed discount factor, there is a polynomial bound in the total size of the MDP (Littman et al., 1995b).\n\n#### 3.2.3 Enhancement to Value Iteration and Policy Iteration\n\nIn practice, value iteration is much faster per iteration, but policy iteration takes fewer iterations. Arguments have been put forth to the effect that each approach is better for large problems. Puterman's _modified policy iteration_ algorithm (Puterman & Shin, 1978) provides a method for trading iteration time for iteration improvement in a smoother way. The basic idea is that the expensive part of policy iteration is solving for the exact value of \\(V_{\\pi}\\). Instead of finding an exact value for \\(V_{\\pi}\\), we can perform a few steps of a modified value-iteration step where the policy is held fixed over successive iterations. This can be shown to produce an approximation to \\(V_{\\pi}\\) that converges linearly in \\(\\gamma\\). In practice, this can result in substantial speedups.\n\nSeveral standard numerical-analysis techniques that speed the convergence of dynamic programming can be used to accelerate value and policy iteration. _Multigrid methods_ can be used to quickly seed a good initial approximation to a high resolution value function by initially performing value iteration at a coarser resolution (Rude, 1993). _State aggregation_ works by collapsing groups of states to a single meta-state solving the abstracted problem (Bertsekas & Castanon, 1989).\n\n#### Computational Complexity\n\nValue iteration works by producing successive approximations of the optimal value function. Each iteration can be performed in \\(O\\left(|A|\\middle|S\\middle|^{2}\\right)\\) steps, or faster if there is sparsity in the transition function. However, the number of iterations required can grow exponentially in the discount factor (Condon, 1992); as the discount factor approaches 1, the decisions must be based on results that happen farther and farther into the future. In practice, policy iteration converges in fewer iterations than value iteration, although the per-iteration costs of \\(O\\left(|A|\\middle|S\\middle|^{2}+|S|^{3}\\right)\\) can be prohibitive. There is no known tight worst-case bound available for policy iteration (Littman et al., 1995b). Modified policy iteration (Puterman & Shin, 1978) seeks a trade-off between cheap and effective iterations and is preferred by some practitioners (Rust, 1996).\n\nLinear programming (Schrijver, 1986) is an extremely general problem, and MDPs can be solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux, 1963; Hoffman & Karp, 1966). An advantage of this approach is that commercial-quality linear-programming packages are available, although the time and space requirements can still be quite high. From a theoretic perspective, linear programming is the only known algorithm that can solve MDPs in polynomial time, although the theoretically efficient algorithms have not been shown to be efficient in practice.\n\n## 4 Learning an Optimal Policy: Model-free Methods\n\nIn the previous section we reviewed methods for obtaining an optimal policy for an MDP assuming that we already had a model. The model consists of knowledge of the state transition probability function \\(T(s,a,s^{\\prime})\\) and the reinforcement function \\(R(s,a)\\). Reinforcement learning is primarily concerned with how to obtain the optimal policy when such a model is not known in advance. The agent must interact with its environment directly to obtain information which, by means of an appropriate algorithm, can be processed to produce an optimal policy.\n\nAt this point, there are two ways to proceed.\n\n* **Model-free:** Learn a controller without learning a model.\n* **Model-based:** Learn a model, and use it to derive a controller.\n\nWhich approach is better? This is a matter of some debate in the reinforcement-learning community. A number of algorithms have been proposed on both sides. This question also appears in other fields, such as adaptive control, where the dichotomy is between _direct_ and _indirect_ adaptive control.\n\nThis section examines model-free learning, and Section 5 examines model-based methods.\n\nThe biggest problem facing a reinforcement-learning agent is _temporal credit assignment_. How do we know whether the action just taken is a good one, when it might have far-reaching effects? One strategy is to wait until the \"end\" and reward the actions taken if the result was good and punish them if the result was bad. In ongoing tasks, it is difficult to know what the \"end\" is, and this might require a great deal of memory. Instead, we will use insights from value iteration to adjust the estimated value of a state based onthe immediate reward and the estimated value of the next state. This class of algorithms is known as _temporal difference methods_ (Sutton, 1988). We will consider two different temporal-difference learning strategies for the discounted infinite-horizon model.\n\n### Adaptive Heuristic Critic and TD(\\(\\lambda\\))\n\nThe _adaptive heuristic critic_ algorithm is an adaptive version of policy iteration (Barto, Sutton, & Anderson, 1983) in which the value-function computation is no longer implemented by solving a set of linear equations, but is instead computed by an algorithm called \\(TD(0)\\). A block diagram for this approach is given in Figure 4. It consists of two components: a critic (labeled AHC), and a reinforcement-learning component (labeled RL). The reinforcement-learning component can be an instance of any of the \\(k\\)-armed bandit algorithms, modified to deal with multiple states and non-stationary rewards. But instead of acting to maximize instantaneous reward, it will be acting to maximize the heuristic value, \\(v\\), that is computed by the critic. The critic uses the real external reinforcement signal to learn to map states to their expected discounted values given that the policy being executed is the one currently instantiated in the RL component.\n\nWe can see the analogy with modified policy iteration if we imagine these components working in alternation. The policy \\(\\pi\\) implemented by RL is fixed and the critic learns the value function \\(V_{\\pi}\\) for that policy. Now we fix the critic and let the RL component learn a new policy \\(\\pi^{\\prime}\\) that maximizes the new value function, and so on. In most implementations, however, both components operate simultaneously. Only the alternating implementation can be guaranteed to converge to the optimal policy, under appropriate conditions. Williams and Baird explored the convergence properties of a class of AHC-related algorithms they call \"incremental variants of policy iteration\" (Williams & Baird, 1993a).\n\nIt remains to explain how the critic can learn the value of a policy. We define \\(\\langle s,a,r,s^{\\prime}\\rangle\\) to be an _experience tuple_ summarizing a single transition in the environment. Here \\(s\\) is the agent's state before the transition, \\(a\\) is its choice of action, \\(r\\) the instantaneous reward it receives, and \\(s^{\\prime}\\) its resulting state. The value of a policy is learned using Sutton's \\(TD(0)\\) algorithm (Sutton, 1988) which uses the update rule\n\n\\[V(s):=V(s)+\\alpha(r+\\gamma V(s^{\\prime})-V(s))\\enspace.\\]\n\nWhenever a state \\(s\\) is visited, its estimated value is updated to be closer to \\(r+\\gamma V(s^{\\prime})\\), since \\(r\\) is the instantaneous reward received and \\(V(s^{\\prime})\\) is the estimated value of the actually occurring next state. This is analogous to the sample-backup rule from value iteration--the only difference is that the sample is drawn from the real world rather than by simulating a known model. The key idea is that \\(r+\\gamma V(s^{\\prime})\\) is a sample of the value of \\(V(s)\\), and it is\n\nFigure 4: Architecture for the adaptive heuristic critic.\n\n[MISSING_PAGE_FAIL:17]\n\nbecause an action can be chosen just by taking the one with the maximum \\(Q\\) value for the current state.\n\nThe Q-learning rule is\n\n\\[Q\\left(s,a\\right):=Q\\left(s,a\\right)+\\alpha\\left(r+\\gamma\\max_{a^{\\prime}}Q \\left(s^{\\prime},a^{\\prime}\\right)-Q\\left(s,a\\right)\\right)\\enspace,\\]\n\nwhere \\(\\left\\langle s,a,r,s^{\\prime}\\right\\rangle\\) is an experience tuple as described earlier. If each action is executed in each state an infinite number of times on an infinite run and \\(\\alpha\\) is decayed appropriately, the \\(Q\\) values will converge with probability 1 to \\(Q^{*}\\)(Watkins, 1989; Tsitsiklis, 1994; Jaakkola, Jordan, & Singh, 1994). Q-learning can also be extended to update states that occurred more than one step previously, as in \\(TD(\\lambda)\\)(Peng & Williams, 1994).\n\nWhen the \\(Q\\) values are nearly converged to their optimal values, it is appropriate for the agent to act greedily, taking, in each situation, the action with the highest \\(Q\\) value. During learning, however, there is a difficult exploitation versus exploration trade-off to be made. There are no good, formally justified approaches to this problem in the general case; standard practice is to adopt one of the _ad hoc_ methods discussed in section 2.2.\n\nAHC architectures seem to be more difficult to work with than Q-learning on a practical level. It can be hard to get the relative learning rates right in AHC so that the two components converge together. In addition, Q-learning is _exploration insensitive_: that is, that the Q values will converge to the optimal values, independent of how the agent behaves while the data is being collected (as long as all state-action pairs are tried often enough). This means that, although the exploration-exploitation issue must be addressed in Q-learning, the details of the exploration strategy will not affect the convergence of the learning algorithm. For these reasons, Q-learning is the most popular and seems to be the most effective model-free algorithm for learning from delayed reinforcement. It does not, however, address any of the issues involved in generalizing over large state and/or action spaces. In addition, it may converge quite slowly to a good policy.\n\n### Model-free Learning With Average Reward\n\nAs described, Q-learning can be applied to discounted infinite-horizon MDPs. It can also be applied to undiscounted problems as long as the optimal policy is guaranteed to reach a reward-free absorbing state and the state is periodically reset.\n\nSchwartz (1993) examined the problem of adapting Q-learning to an average-reward framework. Although his R-learning algorithm seems to exhibit convergence problems for some MDPs, several researches have found the average-reward criterion closer to the true problem they wish to solve than a discounted criterion and therefore prefer R-learning to Q-learning (Mahadevan, 1994).\n\nWith that in mind, researchers have studied the problem of learning optimal average-reward policies. Mahadevan (1996) surveyed model-based average-reward algorithms from a reinforcement-learning perspective and found several difficulties with existing algorithms. In particular, he showed that existing reinforcement-learning algorithms for average reward (and some dynamic programming algorithms) do not always produce bias-optimal policies. Jaakkola, Jordan and Singh (1995) described an average-reward learning algorithm with guaranteed convergence properties. It uses a Monte-Carlo component to estimate the expected future reward for each state as the agent moves through the environment. Inaddition, Bertsekas presents a Q-learning-like algorithm for average-case reward in his new textbook (1995). Although this recent work provides a much needed theoretical foundation to this area of reinforcement learning, many important problems remain unsolved.\n\n## 5 Computing Optimal Policies by Learning Models\n\nThe previous section showed how it is possible to learn an optimal policy without knowing the models \\(T(s,a,s^{\\prime})\\) or \\(R(s,a)\\) and without even learning those models en route. Although many of these methods are guaranteed to find optimal policies eventually and use very little computation time per experience, they make extremely inefficient use of the data they gather and therefore often require a great deal of experience to achieve good performance. In this section we still begin by assuming that we don't know the models in advance, but we examine algorithms that do operate by learning these models. These algorithms are especially important in applications in which computation is considered to be cheap and real-world experience costly.\n\n### Certainty Equivalent Methods\n\nWe begin with the most conceptually straightforward method: first, learn the \\(T\\) and \\(R\\) functions by exploring the environment and keeping statistics about the results of each action; next, compute an optimal policy using one of the methods of Section 3. This method is known as _certainty equivalence_ (Kumar & Varaiya, 1986).\n\nThere are some serious objections to this method:\n\n* It makes an arbitrary division between the learning phase and the acting phase.\n* How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely inefficient method of gathering data, requiring exponentially more data (Whitehead, 1991) than a system that interleaves experience gathering with policy-building more tightly (Koenig & Simmons, 1993). See Figure 5 for an example.\n* The possibility of changes in the environment is also problematic. Breaking up an agent's life into a pure learning and a pure acting phase has a considerable risk that the optimal controller based on early life becomes, without detection, a suboptimal controller if the environment changes.\n\nA variation on this idea is _certainty equivalence_, in which the model is learned continually through the agent's lifetime and, at each step, the current model is used to compute an optimal policy and value function. This method makes very effective use of available data, but still ignores the question of exploration and is extremely computationally demanding, even for fairly small state spaces. Fortunately, there are a number of other model-based algorithms that are more practical.\n\n### Dyna\n\nSutton's Dyna architecture (1990, 1991) exploits a middle ground, yielding strategies that are both more effective than model-free learning and more computationally efficient than the certainty-equivalence approach. It simultaneously uses experience to build a model (\\(\\hat{T}\\) and \\(\\hat{R}\\)), uses experience to adjust the policy, and uses the model to adjust the policy.\n\nDyna operates in a loop of interaction with the environment. Given an experience tuple \\(\\left\\langle s,a,s^{\\prime},r\\right\\rangle\\), it behaves as follows:\n\n* Update the model, incrementing statistics for the transition from \\(s\\) to \\(s^{\\prime}\\) on action \\(a\\) and for receiving reward \\(r\\) for taking action \\(a\\) in state \\(s\\). The updated models are \\(\\hat{T}\\) and \\(\\hat{R}\\).\n* Update the policy at state \\(s\\) based on the newly updated model using the rule \\[Q\\left(s,a\\right):=\\hat{R}\\left(s,a\\right)+\\gamma\\sum_{s^{\\prime}}\\hat{T} \\left(s,a,s^{\\prime}\\right)\\max_{a^{\\prime}}Q\\left(s^{\\prime},a^{\\prime}\\right)\\enspace,\\] which is a version of the value-iteration update for \\(Q\\) values.\n* Perform \\(k\\) additional updates: choose \\(k\\) state-action pairs at random and update them according to the same rule as before: \\[Q\\left(s_{k},a_{k}\\right):=\\hat{R}\\left(s_{k},a_{k}\\right)+\\gamma\\sum_{s^{ \\prime}}\\hat{T}\\left(s_{k},a_{k},s^{\\prime}\\right)\\max_{a^{\\prime}}Q\\left(s^{ \\prime},a^{\\prime}\\right)\\enspace.\\]\n* Choose an action \\(a^{\\prime}\\) to perform in state \\(s^{\\prime}\\), based on the \\(Q\\) values but perhaps modified by an exploration strategy.\n\nThe Dyna algorithm requires about \\(k\\) times the computation of Q-learning per instance, but this is typically vastly less than for the naive model-based method. A reasonable value of \\(k\\) can be determined based on the relative speeds of computation and of taking action.\n\nFigure 6 shows a grid world in which in each cell the agent has four actions (N, S, E, W) and transitions are made deterministically to an adjacent cell, unless there is a block, in which case no movement occurs. As we will see in Table 1, Dyna requires an order of magnitude fewer steps of experience than does Q-learning to arrive at an optimal policy. Dyna requires about six times more computational effort, however.\n\nFigure 5: In this environment, due to Whitehead (1991), random exploration would take take \\(O\\left(2^{n}\\right)\\) steps to reach the goal even once, whereas a more intelligent exploration strategy (e.g. \u201cassume any untried action leads directly to goal\u201d) would require only \\(O\\left(n^{2}\\right)\\) steps.\n\nFigure 6: A 3277-state grid world. This was formulated as a shortest-path reinforcement-learning problem, which yields the same result as if a reward of 1 is given at the goal, a reward of zero elsewhere and a discount factor is used.\n\n\\begin{table}\n\\begin{tabular}{l r r} \\hline \\hline  & **Steps before** & **Backups before** \\\\  & **convergence** & **convergence** \\\\ \\hline Q-learning & 531,000 & 531,000 \\\\ \\hline Dyna & 62,000 & 3,055,000 \\\\ \\hline prioritized sweeping & 28,000 & 1,010,000 \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 1: The performance of three algorithms described in the text. All methods used the exploration heuristic of \u201coptimism in the face of uncertainty\u201d: any state not previously visited was assumed by default to be a goal state. Q-learning used its optimal learning rate parameter for a deterministic maze: \\(\\alpha=1\\). Dyna and prioritized sweeping were permitted to take \\(k=200\\) backups per transition. For prioritized sweeping, the priority queue often emptied before all backups were used.\n\n### Prioritized Sweeping / Queue-Dyna\n\nAlthough Dyna is a great improvement on previous methods, it suffers from being relatively undirected. It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the \"interesting\" parts of the state space. These problems are addressed by prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna (Peng & Williams, 1993), which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail.\n\nThe algorithm is similar to Dyna, except that updates are no longer chosen at random and values are now associated with states (as in value iteration) instead of state-action pairs (as in Q-learning). To make appropriate choices, we must store additional information in the model. Each state remembers its _predecessors_: the states that have a non-zero transition probability to it under some action. In addition, each state has a _priority_, initially set to zero.\n\nInstead of updating \\(k\\) random state-action pairs, prioritized sweeping updates \\(k\\) states with the highest priority. For each high-priority state \\(s\\), it works as follows:\n\n* Remember the current value of the state: \\(V_{old}=V(s)\\).\n* Update the state's value \\[V(s):=\\max_{a}\\left(\\hat{R}(s,a)+\\gamma\\sum_{s^{\\prime}}\\hat{T}\\left(s,a,s^{ \\prime}\\right)V(s^{\\prime})\\right)\\enspace.\\]\n* Set the state's priority back to \\(0\\).\n* Compute the value change \\(\\Delta=|V_{old}-V(s)|\\).\n* Use \\(\\Delta\\) to modify the priorities of the predecessors of \\(s\\).\n\nIf we have updated the \\(V\\) value for state \\(s^{\\prime}\\) and it has changed by amount \\(\\Delta\\), then the immediate predecessors of \\(s^{\\prime}\\) are informed of this event. Any state \\(s\\) for which there exists an action \\(a\\) such that \\(\\hat{T}\\left(s,a,s^{\\prime}\\right)\\neq 0\\) has its priority promoted to \\(\\Delta\\cdot\\hat{T}\\left(s,a,s^{\\prime}\\right)\\), unless its priority already exceeded that value.\n\nThe global behavior of this algorithm is that when a real-world transition is \"surprising\" (the agent happens upon a goal state, for instance), then lots of computation is directed to propagate this new information back to relevant predecessor states. When the real-world transition is \"boring\" (the actual result is very similar to the predicted result), then computation continues in the most deserving part of the space.\n\nRunning prioritized sweeping on the problem in Figure 6, we see a large improvement over Dyna. The optimal policy is reached in about half the number of steps of experience and one-third the computation as Dyna required (and therefore about 20 times fewer steps and twice the computational effort of Q-learning).\n\n### Other Model-Based Methods\n\nMethods proposed for solving MDPs given a model can be used in the context of model-based methods as well.\n\nRTDP (real-time dynamic programming) (Barto, Bradtke, & Singh, 1995) is another model-based method that uses Q-learning to concentrate computational effort on the areas of the state-space that the agent is most likely to occupy. It is specific to problems in which the agent is trying to achieve a particular goal state and the reward everywhere else is 0. By taking into account the start state, it can find a short path from the start to the goal, without necessarily visiting the rest of the state space.\n\nThe Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman, 1994) exploits a similar intuition. It starts by making an approximate version of the MDP which is much smaller than the original one. The approximate MDP contains a set of states, called the _envelope_, that includes the agent's current state and the goal state, if there is one. States that are not in the envelope are summarized by a single \"out\" state. The planning process is an alternation between finding an optimal policy on the approximate MDP and adding useful states to the envelope. Action may take place in parallel with planning, in which case irrelevant states are also pruned out of the envelope.\n\n## 6 Generalization\n\nAll of the previous discussion has tacitly assumed that it is possible to enumerate the state and actions spaces and store tables of values over them. Except in very small environments, this means impractical memory requirements. It also makes inefficient use of experience. In a large, smooth state space we generally expect similar states to have similar values and similar optimal actions. Surely, therefore, there should be some more compact representation than a table. Most problems will have continuous or large discrete state spaces; some will have large or continuous action spaces. The problem of learning in large spaces is addressed through _generalization techniques_, which allow compact storage of learned information and transfer of knowledge between \"similar\" states and actions.\n\nThe large literature of generalization techniques from inductive concept learning can be applied to reinforcement learning. However, techniques often need to be tailored to specific details of the problem. In the following sections, we explore the application of standard function-approximation techniques, adaptive resolution models, and hierarchical methods to the problem of reinforcement learning.\n\nThe reinforcement-learning architectures and algorithms discussed above have included the storage of a variety of mappings, including \\(\\mathcal{S}\\rightarrow\\mathcal{A}\\) (policies), \\(\\mathcal{S}\\rightarrow\\Re\\) (value functions), \\(\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\Re\\) (\\(Q\\) functions and rewards), \\(\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathcal{S}\\) (deterministic transitions), and \\(\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow[0,1]\\) (transition probabilities). Some of these mappings, such as transitions and immediate rewards, can be learned using straightforward supervised learning, and can be handled using any of the wide variety of function-approximation techniques for supervised learning that support noisy training examples. Popular techniques include various neural-network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991). CMA C (Albus, 1981), and local memory-based methods (Moore, Atkeson, & Schaal, 1995), such as generalizations of nearest neighbor methods. Other mappings, especially the policymapping, typically need specialized algorithms because training sets of input-output pairs are not available.\n\n### Generalization over Input\n\nA reinforcement-learning agent's current state plays a central role in its selection of reward-maximizing actions. Viewing the agent as a state-free black box, a description of the current state is its input. Depending on the agent architecture, its output is either an action selection, or an evaluation of the current state that can be used to select an action. The problem of deciding how the different aspects of an input affect the value of the output is sometimes called the \"structural credit-assignment\" problem. This section examines approaches to generating actions or evaluations as a function of a description of the agent's current state.\n\nThe first group of techniques covered here is specialized to the case when reward is not delayed; the second group is more generally applicable.\n\n#### 6.1.1 Immediate Reward\n\nWhen the agent's actions do not influence state transitions, the resulting problem becomes one of choosing actions to maximize immediate reward as a function of the agent's current state. These problems bear a resemblance to the bandit problems discussed in Section 2 except that the agent should condition its action selection on the current state. For this reason, this class of problems has been described as _associative_ reinforcement learning.\n\nThe algorithms in this section address the problem of learning from immediate boolean reinforcement where the state is vector valued and the action is a boolean vector. Such algorithms can and have been used in the context of a delayed reinforcement, for instance, as the RL component in the AHC architecture described in Section 4.1. They can also be generalized to real-valued reward through _reward comparison_ methods (Sutton, 1984).\n\n**CRBP** The complementary reinforcement backpropagation algorithm (Ackley & Littman, 1990) (crbp) consists of a feed-forward network mapping an encoding of the state to an encoding of the action. The action is determined probabilistically from the activation of the output units: if output unit \\(i\\) has activation \\(y_{i}\\), then bit \\(i\\) of the action vector has value 1 with probability \\(y_{i}\\), and 0 otherwise. Any neural-network supervised training procedure can be used to adapt the network as follows. If the result of generating action \\(a\\) is \\(r=1\\), then the network is trained with input-output pair \\(\\langle s,a\\rangle\\). If the result is \\(r=0\\), then the network is trained with input-output pair \\(\\langle s,\\widetilde{a}\\rangle\\), where \\(\\widetilde{a}=(1-a_{1},\\ldots,1-a_{n})\\).\n\nThe idea behind this training rule is that whenever an action fails to generate reward, crbp will try to generate an action that is different from the current choice. Although it seems like the algorithm might oscillate between an action and its complement, that does not happen. One step of training a network will only change the action slightly and since the output probabilities will tend to move toward 0.5, this makes action selection more random and increases search. The hope is that the random distribution will generate an action that works better, and then that action will be reinforced.\n\n**ARC** The associative reinforcement comparison (arc) algorithm (Sutton, 1984) is an instance of the ahc architecture for the case of boolean actions, consisting of two feed forward networks. One learns the value of situations, the other learns a policy. These can be simple linear networks or can have hidden units.\n\nIn the simplest case, the entire system learns only to optimize immediate reward. First, let us consider the behavior of the network that learns the policy, a mapping from a vector describing \\(s\\) to a 0 or 1. If the output unit has activation \\(y_{i}\\), then \\(a\\), the action generated, will be 1 if \\(y+\\nu>0\\), where \\(\\nu\\) is normal noise, and 0 otherwise.\n\nThe adjustment for the output unit is, in the simplest case,\n\n\\[e=r(a-1/2)\\enspace,\\]\n\nwhere the first factor is the reward received for taking the most recent action and the second encodes which action was taken. The actions are encoded as 0 and 1, so \\(a-1/2\\) always has the same magnitude; if the reward and the action have the same sign, then action 1 will be made more likely, otherwise action 0 will be.\n\nAs described, the network will tend to seek actions that given positive reward. To extend this approach to maximize reward, we can compare the reward to some baseline, \\(b\\). This changes the adjustment to\n\n\\[e=(r-b)(a-1/2)\\enspace,\\]\n\nwhere \\(b\\) is the output of the second network. The second network is trained in a standard supervised mode to estimate \\(r\\) as a function of the input state \\(s\\).\n\nVariations of this approach have been used in a variety of applications (Anderson, 1986; Barto et al., 1983; Lin, 1993b; Sutton, 1984).\n\n#### Reinforce Algorithms\n\nWilliams (1987, 1992) studied the problem of choosing actions to maximize immediate reward. He identified a broad class of update rules that perform gradient descent on the expected reward and showed how to integrate these rules with backpropagation. This class, called reinforce algorithms, includes linear reward-inaction (Section 2.1.3) as a special case.\n\nThe generic reinforce update for a parameter \\(w_{ij}\\) can be written\n\n\\[\\Delta w_{ij}=\\alpha_{ij}(r-b_{ij})\\frac{\\partial}{\\partial w_{ij}}\\ln(g_{j})\\]\n\nwhere \\(\\alpha_{ij}\\) is a non-negative factor, \\(r\\) the current reinforcement, \\(b_{ij}\\) a reinforcement baseline, and \\(g_{i}\\) is the probability density function used to randomly generate actions based on unit activations. Both \\(\\alpha_{ij}\\) and \\(b_{ij}\\) can take on different values for each \\(w_{ij}\\), however, when \\(\\alpha_{ij}\\) is constant throughout the system, the expected update is exactly in the direction of the expected reward gradient. Otherwise, the update is in the same half space as the gradient but not necessarily in the direction of steepest increase.\n\nWilliams points out that the choice of baseline, \\(b_{ij}\\), can have a profound effect on the convergence speed of the algorithm.\n\n#### Logic-Based Methods\n\nAnother strategy for generalization in reinforcement learning is to reduce the learning problem to an associative problem of learning boolean functions. A boolean function has a vector of boolean inputs and a single boolean output. Taking inspiration from mainstream machine learning work, Kaelbling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of \\(k\\)-DNF to drive the generalization process (Kaelbling, 1994b); the other searches the space of syntactic descriptions of functions using a simple generate-and-test method (Kaelbling, 1994a).\n\nThe restriction to a single boolean output makes these techniques difficult to apply. In very benign learning situations, it is possible to extend this approach to use a collection of learners to independently learn the individual bits that make up a complex output. In general, however, that approach suffers from the problem of very unreliable reinforcement: if a single learner generates an inappropriate output bit, all of the learners receive a low reinforcement value. The cascade method (Kaelbling, 1993b) allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational effort.\n\n#### 6.1.2 Delayed Reward\n\nAnother method to allow reinforcement-learning techniques to be applied in large state spaces is modeled on value iteration and Q-learning. Here, a function approximator is used to represent the value function by mapping a state description to a value.\n\nMany researchers have experimented with this approach: Boyan and Moore (1995) used local memory-based methods in conjunction with value iteration; Lin (1991) used backpropagation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992, 1995) used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich (1995) used backpropagation and \\(TD(\\lambda)\\) to learn good strategies for job-shop scheduling.\n\nAlthough there have been some positive examples, in general there are unfortunate interactions between function approximation and the learning rules. In discrete environments there is a guarantee that any operation that updates the value function (according to the Bellman equations) can only reduce the error between the current value function and the optimal value function. This guarantee no longer holds when generalization is used. These issues are discussed by Boyan and Moore (1995), who give some simple examples of value function errors growing arbitrarily large when generalization is used with value iteration. Their solution to this, applicable only to certain classes of problems, discourages such divergence by only permitting updates whose estimated values can be shown to be near-optimal via a battery of Monte-Carlo experiments.\n\nThrun and Schwartz (1993) theorize that function approximation of value functions is also dangerous because the errors in value functions due to generalization can become compounded by the \"max\" operator in the definition of the value function.\n\nSeveral recent results (Gordon, 1995; Tsitsiklis & Van Roy, 1996) show how the appropriate choice of function approximator can guarantee convergence, though not necessarily to the optimal values. Baird's _residual gradient_ technique (Baird, 1995) provides guaranteed convergence to locally optimal solutions.\n\nPerhaps the gloominess of these counter-examples is misplaced. Boyan and Moore (1995) report that their counter-examples _can_ be made to work with problem-specific hand-tuning despite the unreliability of untuned algorithms that provably converge in discrete domains. Sutton (1996) shows how modified versions of Boyan and Moore's examples can converge successfully. An open question is whether general principles, ideally supported by theory, can help us understand when value function approximation will succeed. In Sutton's comparative experiments with Boyan and Moore's counter-examples, he changes four aspects of the experiments:\n\n1. Small changes to the task specifications.\n2. A very different kind of function approximator (CMAC (Albus, 1975)) that has weak generalization.\n3. A different learning algorithm: SARSA (Rummery & Niranjan, 1994) instead of value iteration.\n4. A different training regime. Boyan and Moore sampled states uniformly in state space, whereas Sutton's method sampled along empirical trajectories.\n\nThere are intuitive reasons to believe that the fourth factor is particularly important, but more careful research is needed.\n\nAdaptive Resolution ModelsIn many cases, what we would like to do is partition the environment into regions of states that can be considered the same for the purposes of learning and generating actions. Without detailed prior knowledge of the environment, it is very difficult to know what granularity or placement of partitions is appropriate. This problem is overcome in methods that use adaptive resolution; during the course of learning, a partition is constructed that is appropriate to the environment.\n\nDecision TreesIn environments that are characterized by a set of boolean or discrete-valued variables, it is possible to learn compact decision trees for representing \\(Q\\) values. The _G-learning_ algorithm (Chapman & Kaelbling, 1991), works as follows. It starts by assuming that no partitioning is necessary and tries to learn \\(Q\\) values for the entire environment as if it were one state. In parallel with this process, it gathers statistics based on individual input bits; it asks the question whether there is some bit \\(b\\) in the state description such that the \\(Q\\) values for states in which \\(b=1\\) are significantly different from \\(Q\\) values for states in which \\(b=0\\). If such a bit is found, it is used to split the decision tree. Then, the process is repeated in each of the leaves. This method was able to learn very small representations of the \\(Q\\) function in the presence of an overwhelming number of irrelevant, noisy state attributes. It outperformed Q-learning with backpropagation in a simple video-game environment and was used by McCallum (1995) (in conjunction with other techniques for dealing with partial observability) to learn behaviors in a complex driving-simulator. It cannot, however, acquire partitions in which attributes are only significant in combination (such as those needed to solve parity problems).\n\nVariable Resolution Dynamic ProgrammingThe VRDP algorithm (Moore, 1991) enables conventional dynamic programming to be performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimensionality. A _kd_-tree (similar to a decision tree) is used to partition state space into coarse regions. The coarse regions are refined into detailed regions, but only in parts of the state space which are predicted to be important. This notion of importance is obtained by running \"mental trajectories\" through state space. This algorithm proved effective on a number of problems for which full high-resolution arrays would have been impractical. It has the disadvantage of requiring a guess at an initially valid trajectory through state-space.\n\nPartiGame AlgorithmMoore's PartiGame algorithm (Moore, 1994) is another solution to the problem of learning to achieve goal configurations in deterministic high-dimensional continuous spaces by learning an adaptive-resolution model. It also divides the environment into cells; but in each cell, the actions available consist of aiming at the neighboring cells (this aiming is accomplished by a local controller, which must be provided as part of the problem statement). The graph of cell transitions is solved for shortest paths in an online incremental manner, but a minimax criterion is used to detect when a group of cells is too coarse to prevent movement between obstacles or to avoid limit cycles. The offending cells are split to higher resolution. Eventually, the environment is divided up just enough to choose appropriate actions for achieving the goal, but no unnecessary distinctions are made. An important feature is that, as well as reducing memory and computational requirements, it also structures exploration of state space in a multi-resolution manner. Given a failure, the agent will initially try something very different to rectify the failure, and only resort to small local changes when all the qualitatively different strategies have been exhausted.\n\nFigure 7a shows a two-dimensional continuous maze. Figure 7b shows the performance of a robot using the PartiGame algorithm during the very first trial. Figure 7c shows the second trial, started from a slightly different position.\n\nThis is a very fast algorithm, learning policies in spaces of up to nine dimensions in less than a minute. The restriction of the current implementation to deterministic environments limits its applicability, however. McCallum (1995) suggests some related tree-structured methods.\n\nFigure 7: **(a) A two-dimensional maze problem. The point robot must find a path from start to goal without crossing any of the barrier lines. (b) The path taken by PartiGame during the entire first trial. It begins with intense exploration to find a route out of the almost entirely enclosed start region. Having eventually reached a sufficiently high resolution, it discovers the gap and proceeds greedily towards the goal, only to be temporarily blocked by the goal\u2019s barrier region. (c) The second trial.**\n\n### Generalization over Actions\n\nThe networks described in Section 6.1.1 generalize over state descriptions presented as inputs. They also produce outputs in a discrete, factored representation and thus could be seen as generalizing over actions as well.\n\nIn cases such as this when actions are described combinatorially, it is important to generalize over actions to avoid keeping separate statistics for the huge number of actions that can be chosen. In continuous action spaces, the need for generalization is even more pronounced.\n\nWhen estimating \\(Q\\) values using a neural network, it is possible to use either a distinct network for each action, or a network with a distinct output for each action. When the action space is continuous, neither approach is possible. An alternative strategy is to use a single network with both the state and action as input and \\(Q\\) value as the output. Training such a network is not conceptually difficult, but using the network to find the optimal action can be a challenge. One method is to do a local gradient-ascent search on the action in order to find one with high value (Baird & Klopf, 1993).\n\nGullapalli (1990, 1992) has developed a \"neural\" reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience. When the chosen actions are not performing well, the variance is high, resulting in exploration of the range of choices. When an action performs well, the mean is moved in that direction and the variance decreased, resulting in a tendency to generate more action values near the successful one. This method was successfully employed to learn to control a robot arm with many continuous degrees of freedom.\n\n### Hierarchical Methods\n\nAnother strategy for dealing with large state spaces is to treat them as a hierarchy of learning problems. In many cases, hierarchical solutions introduce slight sub-optimality in performance, but potentially gain a good deal of efficiency in execution time, learning time, and space.\n\nHierarchical learners are commonly structured as _gated behaviors_, as shown in Figure 8. There is a collection of _behaviors_ that map environment states into low-level actions and a _gating function_ that decides, based on the state of the environment, which behavior's actions should be switched through and actually executed. Maes and Brooks (1990) used a version of this architecture in which the individual behaviors were fixed _a priori_ and the gating function was learned from reinforcement. Mahadevan and Connell (1991b) used the dual approach: they fixed the gating function, and supplied reinforcement functions for the individual behaviors, which were learned. Lin (1993a) and Dorigo and Colombetti (1995, 1994) both used this approach, first training the behaviors and then training the gating function. Many of the other hierarchical learning methods can be cast in this framework.\n\n#### 6.3.1 Feudal Q-learning\n\nFeudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learning modules. In the simplest case, there is a high-level master and a low-level slave. The master receives reinforcement from the external environment. Its actions consist of commands thatit can give to the low-level learner. When the master generates a particular command to the slave, it must reward the slave for taking actions that satisfy the command, even if they do not result in external reinforcement. The master, then, learns a mapping from states to commands. The slave learns a mapping from commands and states to external actions. The set of \"commands\" and their associated reinforcement functions are established in advance of the learning.\n\nThis is really an instance of the general \"gated behaviors\" approach, in which the slave can execute any of the behaviors depending on its command. The reinforcement functions for the individual behaviors (commands) are given, but learning takes place simultaneously at both the high and low levels.\n\n#### 6.3.2 Compositional Q-learning\n\nSingh's compositional Q-learning (1992b, 1992a) (C-QL) consists of a hierarchy based on the temporal sequencing of subgoals. The _elemental tasks_ are behaviors that achieve some recognizable condition. The high-level goal of the system is to achieve some set of conditions in sequential order. The achievement of the conditions provides reinforcement for the elemental tasks, which are trained first to achieve individual subgoals. Then, the gating function learns to switch the elemental tasks in order to achieve the appropriate high-level sequential goal. This method was used by Tham and Prager (1994) to learn to control a simulated multi-link robot arm.\n\n#### 6.3.3 Hierarchical Distance to Goal\n\nEspecially if we consider reinforcement learning modules to be part of larger agent architectures, it is important to consider problems in which goals are dynamically input to the learner. Kaelbling's HDG algorithm (1993a) uses a hierarchical approach to solving problems when goals of achievement (the agent should get to a particular state as quickly as possible) are given to an agent dynamically.\n\nThe HDG algorithm works by analogy with navigation in a harbor. The environment is partitioned (_a priori_, but more recent work (Ashar, 1994) addresses the case of learning the partition) into a set of regions whose centers are known as \"landmarks.\" If the agent is\n\nFigure 8: A structure of gated behaviors.\n\ncurrently in the same region as the goal, then it uses low-level actions to move to the goal. If not, then high-level information is used to determine the next landmark on the shortest path from the agent's closest landmark to the goal's closest landmark. Then, the agent uses low-level information to aim toward that next landmark. If errors in action cause deviations in the path, there is no problem; the best aiming point is recomputed on every step.\n\n## 7 Partially Observable Environments\n\nIn many real-world environments, it will not be possible for the agent to have perfect and complete perception of the state of the environment. Unfortunately, complete observability is necessary for learning methods based on MDPs. In this section, we consider the case in which the agent makes _observations_ of the state of the environment, but these observations may be noisy and provide incomplete information. In the case of a robot, for instance, it might observe whether it is in a corridor, an open room, a T-junction, etc., and those observations might be error-prone. This problem is also referred to as the problem of \"incomplete perception,\" \"perceptual aliasing,\" or \"hidden state.\"\n\nIn this section, we will consider extensions to the basic MDP framework for solving partially observable problems. The resulting formal model is called a _partially observable Markov decision process_ or POMDP.\n\n### State-Free Deterministic Policies\n\nThe most naive strategy for dealing with partial observability is to ignore it. That is, to treat the observations as if they were the states of the environment and try to learn to behave. Figure 9 shows a simple environment in which the agent is attempting to get to the printer from an office. If it moves from the office, there is a good chance that the agent will end up in one of two places that look like \"hall\", but that require different actions for getting to the printer. If we consider these states to be the same, then the agent cannot possibly behave optimally. But how well can it do?\n\nThe resulting problem is not Markovian, and Q-learning cannot be guaranteed to converge. Small breaches of the Markov requirement are well handled by Q-learning, but it is possible to construct simple environments that cause Q-learning to oscillate (Chrisman &\n\nFigure 9: An example of a partially observable environment.\n\nLittman, 1993). It is possible to use a model-based approach, however; act according to some policy and gather statistics about the transitions between observations, then solve for the optimal policy based on those observations. Unfortunately, when the environment is not Markovian, the transition probabilities depend on the policy being executed, so this new policy will induce a new set of transition probabilities. This approach may yield plausible results in some cases, but again, there are no guarantees.\n\nIt is reasonable, though, to ask what the optimal policy (mapping from observations to actions, in this case) is. It is NP-hard (Littman, 1994b) to find this mapping, and even the best mapping can have very poor performance. In the case of our agent trying to get to the printer, for instance, any deterministic state-free policy takes an infinite number of steps to reach the goal on average.\n\n### State-Free Stochastic Policies\n\nSome improvement can be gained by considering stochastic policies; these are mappings from observations to probability distributions over actions. If there is randomness in the agent's actions, it will not get stuck in the hall forever. Jaakkola, Singh, and Jordan (1995) have developed an algorithm for finding locally-optimal stochastic policies, but finding a globally optimal policy is still NP hard.\n\nIn our example, it turns out that the optimal stochastic policy is for the agent, when in a state that looks like a hall, to go east with probability \\(2-\\sqrt{2}\\approx 0.6\\) and west with probability \\(\\sqrt{2}-1\\approx 0.4\\). This policy can be found by solving a simple (in this case) quadratic program. The fact that such a simple example can produce irrational numbers gives some indication that it is a difficult problem to solve exactly.\n\n### Policies with Internal State\n\nThe only way to behave truly effectively in a wide-range of environments is to use memory of previous actions and observations to disambiguate the current state. There are a variety of approaches to learning policies with internal state.\n\nRecurrent Q-learningOne intuitively simple approach is to use a recurrent neural network to learn \\(Q\\) values. The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain \"history features\" to predict value. This approach has been used by a number of researchers (Meeden, McGraw, & Blank, 1993; Lin & Mitchell, 1992; Schmidhuber, 1991b). It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems.\n\nClassifier SystemsClassifier systems (Holland, 1975; Goldberg, 1989) were explicitly developed to solve problems with delayed reward, including those requiring short-term memory. The internal mechanism typically used to pass reward back through chains of decisions, called the _bucket brigade algorithm_, bears a close resemblance to Q-learning. In spite of some early successes, the original design does not appear to handle partially observed environments robustly.\n\nRecently, this approach has been reexamined using insights from the reinforcement-learning literature, with some success. Dorigo did a comparative study of Q-learning and classifier systems (Dorigo & Bersini, 1994). Cliff and Ross (1994) start with Wilson's zerothlevel classifier system (Wilson, 1995) and add one and two-bit memory registers. They find that, although their system can learn to use short-term memory registers effectively, the approach is unlikely to scale to more complex environments.\n\nDorigo and Colombetti applied classifier systems to a moderately complex problem of learning robot behavior from immediate reinforcement (Doigo, 1995; Doigo & Colombetti, 1994).\n\nFinite-history-window ApproachOne way to restore the Markov property is to allow decisions to be based on the history of recent observations and perhaps actions. Lin and Mitchell (1992) used a fixed-width finite history window to learn a pole balancing task. McCallum (1995) describes the \"utile suffix memory\" which learns a variable-width window that serves simultaneously as a model of the environment and a finite-memory policy. This system has had excellent results in a very complex driving-simulation domain (McCallum, 1995). Ring (1994) has a neural-network approach that uses a variable history window, adding history when necessary to disambiguate situations.\n\nPOMDP ApproachAnother strategy consists of using hidden Markov model (HMM) techniques to learn a model of the environment, including the hidden state, then to use that model to construct a _perfect memory_ controller (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Monahan, 1982).\n\nChrisman (1992) showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum (1993), also gave heuristic _state-splitting rules_ to attempt to learn the smallest possible model for a given environment. The resulting model can then be used to integrate information from the agent's observations in order to make decisions.\n\nFigure 10 illustrates the basic structure for a perfect-memory controller. The component on the left is the _state estimator_, which computes the agent's _belief state_, \\(b\\) as a function of the old belief state, the last action \\(a\\), and the current observation \\(i\\). In this context, a belief state is a probability distribution over states of the environment, indicating the likelihood, given the agent's past experience, that the environment is actually in each of those states. The state estimator can be constructed straightforwardly using the estimated world model and Bayes' rule.\n\nNow we are left with the problem of finding a policy mapping belief states into action. This problem can be formulated as an MDP, but it is difficult to solve using the techniques described earlier, because the input space is continuous. Chrisman's approach (1992) does not take into account future uncertainty, but yields a policy after a small amount of computation. A standard approach from the operations-research literature is to solve for the\n\nFigure 10: Structure of a POMDP agent.\n\noptimal policy (or a close approximation thereof) based on its representation as a piecewise-linear and convex function over the belief space. This method is computationally intractable, but may serve as inspiration for methods that make further approximations (Cassandra et al., 1994; Littman, Cassandra, & Kaelbling, 1995a).\n\n## 8 Reinforcement Learning Applications\n\nOne reason that reinforcement learning is popular is that is serves as a theoretical tool for studying the principles of agents learning to act. But it is unsurprising that it has also been used by a number of researchers as a practical computational tool for constructing autonomous systems that improve themselves with experience. These applications have ranged from robotics, to industrial manufacturing, to combinatorial search problems such as computer game playing.\n\nPractical applications provide a test of the efficacy and usefulness of learning algorithms. They are also an inspiration for deciding which components of the reinforcement learning framework are of practical importance. For example, a researcher with a real robotic task can provide a data point to questions such as:\n\n* How important is optimal exploration? Can we break the learning period into exploration phases and exploitation phases?\n* What is the most useful model of long-term reward: Finite horizon? Discounted? Infinite horizon?\n* How much computation is available between agent decisions and how should it be used?\n* What prior knowledge can we build into the system, and which algorithms are capable of using that knowledge?\n\nLet us examine a set of practical applications of reinforcement learning, while bearing these questions in mind.\n\n### Game Playing\n\nGame playing has dominated the Artificial Intelligence world as a problem domain ever since the field was born. Two-player games do not fit into the established reinforcement-learning framework since the optimality criterion for games is not one of maximizing reward in the face of a fixed environment, but one of maximizing reward against an optimal adversary (minimax). Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games (Littman, 1994a) and many researchers have used reinforcement learning in these environments. One application, spectacularly far ahead of its time, was Samuel's checkers playing system (Samuel, 1959). This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal differences and Q-learning.\n\nMore recently, Tesauro (1992, 1994, 1995) applied the temporal difference algorithm to backgammon. Backgammon has approximately \\(10^{20}\\) states, making table-based reinforcement learning impossible. Instead, Tesauro used a backpropagation-based three-layerneural network as a function approximator for the value function\n\n\\[\\mathit{Board\\ Position}\\to\\mathit{Probability\\ of\\ victory\\ for\\ current\\ player}.\\]\n\nTwo versions of the learning algorithm were used. The first, which we will call Basic TD-Gammon, used very little predefined knowledge of the game, and the representation of a board position was virtually a raw encoding, sufficiently powerful only to permit the neural network to distinguish between conceptually different positions. The second, TD-Gammon, was provided with the same raw state information supplemented by a number of hand-crafted features of backgammon board positions. Providing hand-crafted features in this manner is a good example of how inductive biases from human knowledge of the task can be supplied to a learning algorithm.\n\nThe training of both learning algorithms required several months of computer time, and was achieved by constant self-play. No exploration strategy was used--the system always greedily chose the move with the largest expected probability of victory. This naive exploration strategy proved entirely adequate for this environment, which is perhaps surprising given the considerable work in the reinforcement-learning literature which has produced numerous counter-examples to show that greedy exploration can lead to poor learning performance. Backgammon, however, has two important properties. Firstly, whatever policy is followed, every game is guaranteed to end in finite time, meaning that useful reward information is obtained fairly frequently. Secondly, the state transitions are sufficiently stochastic that independent of the policy, all states will occasionally be visited--a wrong initial value function has little danger of starving us from visiting a critical part of state space from which important information could be obtained.\n\nThe results (Table 2) of TD-Gammon are impressive. It has competed at the very top level of international human play. Basic TD-Gammon played respectably, but not at a professional standard.\n\n\\begin{table}\n\\begin{tabular}{l c c l} \\hline  & **Training** & **Hidden** & **Results** \\\\  & **Games** & **Units** & \\\\ \\hline Basic & & & Poor \\\\ TD 1.0 & 300,000 & 80 & Lost by 13 points in 51 \\\\  & & & games \\\\ TD 2.0 & 800,000 & 40 & Lost by 7 points in 38 \\\\  & & & games \\\\ TD 2.1 & 1,500,000 & 80 & Lost by 1 point in 40 \\\\  & & & games \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 2: TD-Gammon\u2019s performance in games against the top human professional players. A backgammon tournament involves playing a series of games for points until one player reaches a set target. TD-Gammon won none of these tournaments but came sufficiently close that it is now considered one of the best few players in the world.\n\nAlthough experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) and Chess (Thrun, 1995). It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains.\n\n### Robotics and Control\n\nIn recent years there have been many robotics and control applications that have used reinforcement learning. Here we will concentrate on the following four examples, although many other interesting ongoing robotics investigations are underway.\n\n1. Schaal and Atkeson (1994) constructed a two-armed robot, shown in Figure 11, that learns to juggle a device known as a devil-stick. This is a complex non-linear control task involving a six-dimensional state space and less than 200 msecs per control decision. After about 40 initial attempts the robot learns to keep juggling for hundreds of hits. A typical human learning the task requires an order of magnitude more practice to achieve proficiency at mere tens of hits. The juggling robot learned a world model from experience, which was generalized to unvisited states by a function approximation scheme known as locally weighted regression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992). Between each trial, a form of dynamic programming specific to linear control policies and locally linear transitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design (Sage & White, 1977).\n\nFigure 11: Schaal and Atkeson\u2019s devil-sticking robot. The tapered stick is hit alternately by each of the two hand sticks. The task is to keep the devil stick from falling for as many hits as possible. The robot has three motors indicated by torque vectors \\(\\tau_{1},\\tau_{2},\\tau_{3}\\).\n\n2. Mahadevan and Connell (1991a) discuss a task in which a mobile robot pushes large boxes for extended periods of time. Box-pushing is a well-known difficult robotics problem, characterized by immense uncertainty in the results of actions. Q-learning was used in conjunction with some novel clustering techniques designed to enable a higher-dimensional input than a tabular approach would have permitted. The robot learned to perform competitively with the performance of a human-programmed solution. Another aspect of this work, mentioned in Section 6.3, was a pre-programmed breakdown of the monolithic task description into a set of lower level tasks to be learned.\n3. Mataric (1994) describes a robotics experiment with, from the viewpoint of theoretical reinforcement learning, an unthinkably high dimensional state space, containing many dozens of degrees of freedom. Four mobile robots traveled within an enclosure collecting small disks and transporting them to a destination region. There were three enhancements to the basic Q-learning algorithm. Firstly, pre-programmed signals called _progress estimators_ were used to break the monolithic task into subtasks. This was achieved in a robust manner in which the robots were not forced to use the estimators, but had the freedom to profit from the inductive bias they provided. Secondly, control was decentralized. Each robot learned its own policy independently without explicit communication with the others. Thirdly, state space was brutally quantized into a small number of discrete states according to values of a small number of pre-programmed boolean features of the underlying sensors. The performance of the Q-learned policies were almost as good as a simple hand-crafted controller for the job.\n4. Q-learning has been used in an elevator dispatching task (Crites & Barto, 1996). The problem, which has been implemented in simulation only at this stage, involved four elevators servicing ten floors. The objective was to minimize the average squared wait time for passengers, discounted into future time. The problem can be posed as a discrete Markov system, but there are \\(10^{22}\\) states even in the most simplified version of the problem. Cites and Barto used neural networks for function approximation and provided an excellent comparison study of their Q-learning approach against the most popular and the most sophisticated elevator dispatching algorithms. The squared wait time of their controller was approximately \\(7\\%\\) less than the best alternative algorithm (\"Empty the System\" heuristic with a receding horizon controller) and less than half the squared wait time of the controller most frequently used in real elevator systems.\n5. The final example concerns an application of reinforcement learning by one of the authors of this survey to a packaging task from a food processing industry. The problem involves filling containers with variable numbers of non-identical products. The product characteristics also vary with time, but can be sensed. Depending on the task, various constraints are placed on the container-filling procedure. Here are three examples: * The mean weight of all containers produced by a shift must not be below the manufacturer's declared weight \\(W\\). * The mean weight of all containers produced by a shift must not be below the manufacturer's declared weight \\(W\\).\n\n* The number of containers below the declared weight must be less than \\(P\\%\\).\n* No containers may be produced below weight \\(W^{\\prime}\\).\n\nSuch tasks are controlled by machinery which operates according to various _setpoints_. Conventional practice is that setpoints are chosen by human operators, but this choice is not easy as it is dependent on the current product characteristics and the current task constraints. The dependency is often difficult to model and highly non-linear. The task was posed as a finite-horizon Markov decision task in which the state of the system is a function of the product characteristics, the amount of time remaining in the production shift and the mean wastage and percent below declared in the shift so far. The system was discretized into 200,000 discrete states and local weighted regression was used to learn and generalize a transition model. Prioritized sweeping was used to maintain an optimal value function as each new piece of transition information was obtained. In simulated experiments the savings were considerable, typically with wastage reduced by a factor of ten. Since then the system has been deployed successfully in several factories within the United States.\n\nSome interesting aspects of practical reinforcement learning come to light from these examples. The most striking is that in all cases, to make a real system work it proved necessary to supplement the fundamental algorithm with extra pre-programmed knowledge. Supplying extra knowledge comes at a price: more human effort and insight is required and the system is subsequently less autonomous. But it is also clear that for tasks such as these, a knowledge-free approach would not have achieved worthwhile performance within the finite lifetime of the robots.\n\nWhat forms did this pre-programmed knowledge take? It included an assumption of linearity for the juggling robot's policy, a manual breaking up of the task into subtasks for the two mobile-robot examples, while the box-pusher also used a clustering technique for the \\(Q\\) values which assumed locally consistent \\(Q\\) values. The four disk-collecting robots additionally used a manually discretized state space. The packaging example had far fewer dimensions and so required correspondingly weaker assumptions, but there, too, the assumption of local piecewise continuity in the transition model enabled massive reductions in the amount of learning data required.\n\nThe exploration strategies are interesting too. The juggler used careful statistical analysis to judge where to profitably experiment. However, both mobile robot applications were able to learn well with greedy exploration--always exploiting without deliberate exploration. The packaging task used optimism in the face of uncertainty. None of these strategies mirrors theoretically optimal (but computationally intractable) exploration, and yet all proved adequate.\n\nFinally, it is also worth considering the computational regimes of these experiments. They were all very different, which indicates that the differing computational demands of various reinforcement learning algorithms do indeed have an array of differing applications. The juggler needed to make very fast decisions with low latency between each hit, but had long periods (30 seconds and more) between each trial to consolidate the experiences collected on the previous trial and to perform the more aggressive computation necessary to produce a new reactive controller on the next trial. The box-pushing robot was meant to operate autonomously for hours and so had to make decisions with a uniform length control cycle. The cycle was sufficiently long for quite substantial computations beyond simple Q-learning backups. The four disk-collecting robots were particularly interesting. Each robot had a short life of less than 20 minutes (due to battery constraints) meaning that substantial number crunching was impractical, and any significant combinatorial search would have used a significant fraction of the robot's learning lifetime. The packaging task had easy constraints. One decision was needed every few minutes. This provided opportunities for fully computing the optimal value function for the 200,000-state system between every control cycle, in addition to performing massive cross-validation-based optimization of the transition model being learned.\n\nA great deal of further work is currently in progress on practical implementations of reinforcement learning. The insights and task constraints that they produce will have an important effect on shaping the kind of algorithms that are developed in future.\n\n## 9 Conclusions\n\nThere are a variety of reinforcement-learning techniques that work effectively on a variety of small problems. But very few of these techniques scale well to larger problems. This is not because researchers have done a bad job of inventing learning techniques, but because it is very difficult to solve arbitrary problems in the general case. In order to solve highly complex problems, we must give up _tabula rasa_learning techniques and begin to incorporate bias that will give leverage to the learning process.\n\nThe necessary bias can come in a variety of forms, including the following:\n\n**shaping:**: The technique of shaping is used in training animals (Hilgard & Bower, 1975); a teacher presents very simple problems to solve first, then gradually exposes the learner to more complex problems. Shaping has been used in supervised-learning systems, and can be used to train hierarchical reinforcement-learning systems from the bottom up (Lin, 1991), and to alleviate problems of delayed reinforcement by decreasing the delay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995).\n**local reinforcement signals:**: Whenever possible, agents should be given reinforcement signals that are local. In applications in which it is possible to compute a gradient, rewarding the agent for taking steps up the gradient, rather than just for achieving the final goal, can speed learning significantly (Mataric, 1994).\n**imitation:**: An agent can learn by \"watching\" another agent perform the task (Lin, 1991). For real robots, this requires perceptual abilities that are not yet available. But another strategy is to have a human supply appropriate motor commands to a robot through a joystick or steering wheel (Pomerleau, 1993).\n**problem decomposition:**: Decomposing a huge learning problem into a collection of smaller ones, and providing useful reinforcement signals for the subproblems is a very powerful technique for biasing learning. Most interesting examples of robotic reinforcement learning employ this technique to some extent (Connell & Mahadevan, 1993).\n**reflexes:**: One thing that keeps agents that know nothing from learning anything is that they have a hard time even finding the interesting parts of the space; they wander around at random never getting near the goal, or they are always \"killed\" immediately. These problems can be ameliorated by programming a set of \"reflexes\" that cause the agent to act initially in some way that is reasonable (Mataic, 1994; Singh, Barto, Grupen, & Connolly, 1994). These reflexes can eventually be overridden by more detailed and accurate learned knowledge, but they at least keep the agent alive and pointed in the right direction while it is trying to learn. Recent work by Millan (1996) explores the use of reflexes to make robot learning safer and more efficient.\n\nWith appropriate biases, supplied by human programmers or teachers, complex reinforcement learning problems will eventually be solvable. There is still much work to be done and many interesting questions remaining for learning techniques and especially regarding methods for approximating, decomposing, and incorporating bias into problems.\n\n## Acknowledgements\n\nThanks to Marco Dorigo and three anonymous reviewers for comments that have helped to improve this paper. Also thanks to our many colleagues in the reinforcement-learning community who have done this work and explained it to us.\n\nLeslie Pack Kaelbling was supported in part by NSF grants IRI-9453383 and IRI-9312395. Michael Littman was supported in part by Bellcore. Andrew Moore was supported in part by an NSF Research Initiation Award and by 3M Corporation.\n\n## References\n\n* Ackley & Littman (1990) Ackley, D. H., & Littman, M. L. (1990). Generalization and scaling in reinforcement learning. In Touretzky, D. S. (Ed.), _Advances in Neural Information Processing Systems 2_, pp. 550-557 San Mateo, CA. Morgan Kaufmann.\n* Albus (1975) Albus, J. S. (1975). A new approach to manipulator control: Cerebellar model articulation controller (c mac). _Journal of Dynamic Systems_, _Measurement and Control_, _97_, 220-227.\n* Albus (1981) Albus, J. S. (1981). _Brains, Behavior, and Robotics_. BYTE Books, Subsidiary of McGraw-Hill, Peterborough, New Hampshire.\n* Anderson (1986) Anderson, C. W. (1986). _Learning and Problem Solving with Multilayer Connectionist Systems_. Ph.D. thesis, University of Massachusetts, Amherst, MA.\n* Ashar (1994) Ashar, R. R. (1994). Hierarchical learning in stochastic domains. Master's thesis, Brown University, Providence, Rhode Island.\n* Baird (1995) Baird, L. (1995). Residual algorithms: Reinforcement learning with function approximation. In Prieditis, A., & Russell, S. (Eds.), _Proceedings of the Twelfth International Conference on Machine Learning_, pp. 30-37 San Francisco, CA. Morgan Kaufmann.\n* Baird & Klopf (1993) Baird, L. C., & Klopf, A. H. (1993). Reinforcement learning with high-dimensional, continuous actions. Tech. rep. WL-TR-93-1147, Wright-Patterson Air Force Base Ohio: Wright Laboratory.\n* Baird & Klopf (1994)* Barto et al. (1995) Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic programming. _Artificial Intelligence_, _72_(1), 81-138.\n* Barto et al. (1983) Barto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neurolike adaptive elements that can solve difficult learning control problems. _IEEE Transactions on Systems_, _Man, and Cybernetics_, _SMC-13_(5), 834-846.\n* Bellman (1957) Bellman, R. (1957). _Dynamic Programming_. Princeton University Press, Princeton, NJ.\n* Berenji (1991) Berenji, H. R. (1991). Artificial neural networks and approximate reasoning for intelligent control in space. In _American Control Conference_, pp. 1075-1080.\n* Berry & Fristedt (1985) Berry, D. A., & Fristedt, B. (1985). _Bandit Problems: Sequential Allocation of Experiments_. Chapman and Hall, London, UK.\n* Bertsekas (1987) Bertsekas, D. P. (1987). _Dynamic Programming: Deterministic and Stochastic Models_. Prentice-Hall, Englewood Cliffs, NJ.\n* Bertsekas (1995) Bertsekas, D. P. (1995). _Dynamic Programming and Optimal Control_. Athena Scientific, Belmont, Massachusetts. Volumes 1 and 2.\n* Bertsekas & Castanon (1989) Bertsekas, D. P., & Castanon, D. A. (1989). Adaptive aggregation for infinite horizon dynamic programming. _IEEE Transactions on Automatic Control_, _34_(6), 589-598.\n* Bertsekas & Tsitsiklis (1989) Bertsekas, D. P., & Tsitsiklis, J. N. (1989). _Parallel and Distributed Computation: Numerical Methods_. Prentice-Hall, Englewood Cliffs, NJ.\n* Box & Draper (1987) Box, G. E. P., & Draper, N. R. (1987). _Empirical Model-Building and Response Surfaces_. Wiley.\n* Boyan & Moore (1995) Boyan, J. A., & Moore, A. W. (1995). Generalization in reinforcement learning: Safely approximating the value function. In Tesauro, G., Touretzky, D. S., & Leen, T. K. (Eds.), _Advances in Neural Information Processing Systems 7_ Cambridge, MA. The MIT Press.\n* Burghes & Graham (1980) Burghes, D., & Graham, A. (1980). _Introduction to Control Theory including Optimal Control_. Ellis Horwood.\n* Cassandra et al. (1994) Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally in partially observable stochastic domains. In _Proceedings of the Twelfth National Conference on Artificial Intelligence_ Seattle, WA.\n* Chapman & Kaelbling (1991) Chapman, D., & Kaelbling, L. P. (1991). Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. In _Proceedings of the International Joint Conference on Artificial Intelligence_ Sydney, Australia.\n* Chrisman (1992) Chrisman, L. (1992). Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. In _Proceedings of the Tenth National Conference on Artificial Intelligence_, pp. 183-188 San Jose, CA. AAAI Press.\n* Chrisman (1993)* Chisman & Littman (1993) Chisman, L., & Littman, M. (1993). Hidden state and short-term memory.. Presentation at Reinforcement Learning Workshop, Machine Learning Conference.\n* Cichosz & Mulawka (1995) Cichosz, P., & Mulawka, J. J. (1995). Fast and efficient reinforcement learning with truncated temporal differences. In Prieditis, A., & Russell, S. (Eds.), _Proceedings of the Twelfth International Conference on Machine Learning_, pp. 99-107 San Francisco, CA. Morgan Kaufmann.\n* Cleveland & Delvin (1988) Cleveland, W. S., & Delvin, S. J. (1988). Locally weighted regression: An approach to regression analysis by local fitting. _Journal of the American Statistical Association_, _83_(403), 596-610.\n* Cliff & Ross (1994) Cliff, D., & Ross, S. (1994). Adding temporary memory to ZCS. _Adaptive Behavior_, _3_(2), 101-150.\n* Condon (1992) Condon, A. (1992). The complexity of stochastic games. _Information and Computation_, _96_(2), 203-224.\n* Connell & Mahadevan (1993) Connell, J., & Mahadevan, S. (1993). Rapid task learning for real robots. In _Robot Learning_. Kluwer Academic Publishers.\n* Cites & Barto (1996) Cites, R. H., & Barto, A. G. (1996). Improving elevator performance using reinforcement learning. In Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), _Neural Information Processing Systems 8_.\n* Dayan (1992) Dayan, P. (1992). The convergence of TD(\\(\\lambda\\)) for general \\(\\lambda\\). _Machine Learning_, _8_(3), 341-362.\n* Dayan & Hinton (1993) Dayan, P., & Hinton, G. E. (1993). Feudal reinforcement learning. In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), _Advances in Neural Information Processing Systems 5_ San Mateo, CA. Morgan Kaufmann.\n* Dayan & Sejnowski (1994) Dayan, P., & Sejnowski, T. J. (1994). TD(\\(\\lambda\\)) converges with probability 1. _Machine Learning_, _14_(3).\n* Dean et al. (1993) Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning with deadlines in stochastic domains. In _Proceedings of the Eleventh National Conference on Artificial Intelligence_ Washington, DC.\n* D'Epenoux (1963) D'Epenoux, F. (1963). A probabilistic production and inventory problem. _Management Science_, _10_, 98-108.\n* Derman (1970) Derman, C. (1970). _Finite State Markovian Decision Processes_. Academic Press, New York.\n* Dorigo & Besini (1994) Dorigo, M., & Besini, H. (1994). A comparison of q-learning and classifier systems. In _From Animals to Animats: Proceedings of the Third International Conference on the Simulation of Adaptive Behavior_ Brighton, UK.\n* Dorigo & Colombetti (1994) Dorigo, M., & Colombetti, M. (1994). Robot shaping: Developing autonomous agents through learning. _Artificial Intelligence_, _71_(2), 321-370.\n* Dorigo & Colombetti (1994)* [DoigoDoigo1995] Doigo, M. (1995). Alecsys and the Auton0Mouse: Learning to control a real robot by distributed classifier systems. _Machine Learning_, _19_.\n* [FiechterFiechter1994] Fiechter, C.-N. (1994). Efficient reinforcement learning. In _Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory_, pp. 88-97. Association of Computing Machinery.\n* [GittinsGittins1989] Gittins, J. C. (1989). _Multi-armed Bandit Allocation Indices_. Wiley-Interscience series in systems and optimization. Wiley, Chichester, NY.\n* [GoldbergGoldberg1989] Goldberg, D. (1989). _Genetic algorithms in search, optimization, and machine learning_. Addison-Wesley, MA.\n* [GordonGordon1995] Gordon, G. J. (1995). Stable function approximation in dynamic programming. In Prieditis, A., & Russell, S. (Eds.), _Proceedings of the Twelfth International Conference on Machine Learning_, pp. 261-268 San Francisco, CA. Morgan Kaufmann.\n* [GullapalliGullapalli1990] Gullapalli, V. (1990). A stochastic reinforcement learning algorithm for learning real-valued functions. _Neural Networks_, \\(3\\), 671-692.\n* [GullapalliGullapalli1992] Gullapalli, V. (1992). _Reinforcement learning and its application to control_. Ph.D. thesis, University of Massachusetts, Amherst, MA.\n* [Hilgard and BowerHilgard and Bower1975] Hilgard, E. R., & Bower, G. H. (1975). _Theories of Learning_ (fourth edition). Prentice-Hall, Englewood Cliffs, NJ.\n* [Hoffman and KarpHoffman and Karp1966] Hoffman, A. J., & Karp, R. M. (1966). On nonterminating stochastic games. _Management Science_, _12_, 359-370.\n* [HollandHolland1975] Holland, J. H. (1975). _Adaptation in Natural and Artificial Systems_. University of Michigan Press, Ann Arbor, MI.\n* [HowardHoward1960] Howard, R. A. (1960). _Dynamic Programming and Markov Processes_. The MIT Press, Cambridge, MA.\n* [Jaakkola, Jordan, Singh, Singh, 1994] Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994). On the convergence of stochastic iterative dynamic programming algorithms. _Neural Computation_, _6_(6).\n* [Jaakkola, Singh, Jordan, 1995] Jaakkola, T., Singh, S. P., & Jordan, M. I. (1995). Monte-carlo reinforcement learning in non-Markovian decision problems. In Tesauro, G., Touretzky, D. S., & Leen, T. K. (Eds.), _Advances in Neural Information Processing Systems 7_ Cambridge, MA. The MIT Press.\n* [KaelblingKaelbling1993a] Kaelbling, L. P. (1993a). Hierarchical learning in stochastic domains: Preliminary results. In _Proceedings of the Tenth International Conference on Machine Learning_ Amherst, MA. Morgan Kaufmann.\n* [KaelblingKaelbling1993b] Kaelbling, L. P. (1993b). _Learning in Embedded Systems_. The MIT Press, Cambridge, MA.\n* [KaelblingKaelbling1994a] Kaelbling, L. P. (1994a). Associative reinforcement learning: A generate and test algorithm. _Machine Learning_, _15_(3).\n* [KaelblingKaelbling1995]* Kaelbling (1994) Kaelbling, L. P. (1994b). Associative reinforcement learning: Functions in \\(k\\)-DNF. _Machine Learning_, _15_(3).\n* Kirman (1994) Kirman, J. (1994). _Predicting Real-Time Planner Performance by Domain Characterization_. Ph.D. thesis, Department of Computer Science, Brown University.\n* Koenig & Simmons (1993) Koenig, S., & Simmons, R. G. (1993). Complexity analysis of real-time reinforcement learning. In _Proceedings of the Eleventh National Conference on Artificial Intelligence_, pp. 99-105 Menlo Park, California. AAAI Press/MIT Press.\n* Kumar & Varaiya (1986) Kumar, P. R., & Varaiya, P. P. (1986). _Stochastic Systems: Estimation, Identification, and Adaptive Control_. Prentice Hall, Englewood Cliffs, New Jersey.\n* Lee (1991) Lee, C. C. (1991). A self learning rule-based controller employing approximate reasoning and neural net concepts. _International Journal of Intelligent Systems_, _6_(1), 71-93.\n* Lin (1991) Lin, L.-J. (1991). Programming robots using reinforcement learning and teaching. In _Proceedings of the Ninth National Conference on Artificial Intelligence_.\n* Lin (1993a) Lin, L.-J. (1993a). Hierarchical learning of robot skills by reinforcement. In _Proceedings of the International Conference on Neural Networks_.\n* Lin (1993b) Lin, L.-J. (1993b). _Reinforcement Learning for Robots Using Neural Networks_. Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA.\n* Lin & Mitchell (1992) Lin, L.-J., & Mitchell, T. M. (1992). Memory approaches to reinforcement learning in non-Markovian domains. Tech. rep. CMU-CS-92-138, Carnegie Mellon University, School of Computer Science.\n* Littman (1994a) Littman, M. L. (1994a). Markov games as a framework for multi-agent reinforcement learning. In _Proceedings of the Eleventh International Conference on Machine Learning_, pp. 157-163 San Francisco, CA. Morgan Kaufmann.\n* Littman (1994b) Littman, M. L. (1994b). Memoryless policies: Theoretical limitations and practical results. In Cliff, D., Husbands, P., Meyer, J.-A., & Wilson, S. W. (Eds.), _From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior_ Cambridge, MA. The MIT Press.\n* Littman et al. (1995a) Littman, M. L., Cassandra, A., & Kaelbling, L. P. (1995a). Learning policies for partially observable environments: Scaling up. In Prieditis, A., & Russell, S. (Eds.), _Proceedings of the Twelfth International Conference on Machine Learning_, pp. 362-370 San Francisco, CA. Morgan Kaufmann.\n* Littman et al. (1995b) Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995b). On the complexity of solving Markov decision problems. In _Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence (UAI-95)_ Montreal, Quebec, Canada.\n* Lovejoy (1991) Lovejoy, W. S. (1991). A survey of algorithmic methods for partially observable Markov decision processes. _Annals of Operations Research_, _28_, 47-66.\n* Lovejoy (1992)* Maes & Brooks (1990) Maes, P., & Brooks, R. A. (1990). Learning to coordinate behaviors. In _Proceedings Eighth National Conference on Artificial Intelligence_, pp. 796-802. Morgan Kaufmann.\n* Mahadevan (1994) Mahadevan, S. (1994). To discount or not to discount in reinforcement learning: A case study comparing R learning and Q learning. In _Proceedings of the Eleventh International Conference on Machine Learning_, pp. 164-172 San Francisco, CA. Morgan Kaufmann.\n* Mahadevan (1996) Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and empirical results. _Machine Learning_, _22_ (1).\n* Mahadevan & Connell (1991a) Mahadevan, S., & Connell, J. (1991a). Automatic programming of behavior-based robots using reinforcement learning. In _Proceedings of the Ninth National Conference on Artificial Intelligence_ Anaheim, CA.\n* Mahadevan & Connell (1991b) Mahadevan, S., & Connell, J. (1991b). Scaling reinforcement learning to robotics by exploiting the subsumption architecture. In _Proceedings of the Eighth International Workshop on Machine Learning_, pp. 328-332.\n* Mataric (1994) Mataric, M. J. (1994). Reward functions for accelerated learning. In Cohen, W. W., & Hirsh, H. (Eds.), _Proceedings of the Eleventh International Conference on Machine Learning_. Morgan Kaufmann.\n* McCallum (1995) McCallum, A. K. (1995). _Reinforcement Learning with Selective Perception and Hidden State_. Ph.D. thesis, Department of Computer Science, University of Rochester.\n* McCallum (1993) McCallum, R. A. (1993). Overcoming incomplete perception with utile distinction memory. In _Proceedings of the Tenth International Conference on Machine Learning_, pp. 190-196 Amherst, Massachusetts. Morgan Kaufmann.\n* McCallum (1995) McCallum, R. A. (1995). Instance-based utile distinctions for reinforcement learning with hidden state. In _Proceedings of the Twelfth International Conference Machine Learning_, pp. 387-395 San Francisco, CA. Morgan Kaufmann.\n* Meeden et al. (1993) Meeden, L., McGraw, G., & Blank, D. (1993). Emergent control and planning in an autonomous vehicle. In Touretsky, D. (Ed.), _Proceedings of the Fifteenth Annual Meeting of the Cognitive Science Society_, pp. 735-740. Lawrence Erlbaum Associates, Hillsdale, NJ.\n* Millan (1996) Millan, J. d. R. (1996). Rapid, safe, and incremental learning of navigation strategies. _IEEE Transactions on Systems, Man, and Cybernetics_, _26_(3).\n* Monahan (1982) Monahan, G. E. (1982). A survey of partially observable Markov decision processes: Theory, models, and algorithms. _Management Science_, _28_, 1-16.\n* Moore (1991) Moore, A. W. (1991). Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued spaces. In _Proc. Eighth International Machine Learning Workshop_.\n* Moore (1991)Moore, A. W. (1994). The partial-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), _Advances in Neural Information Processing Systems 6_, pp. 711-718 San Mateo, CA. Morgan Kaufmann.\n* Moore & Atkeson (1992) Moore, A. W., & Atkeson, C. G. (1992). An investigation of memory-based function approximators for learning control. Tech. rep., MIT Artifical Intelligence Laboratory, Cambridge, MA.\n* Moore & Atkeson (1993) Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less data and less real time. _Machine Learning_, _13_.\n* Moore et al. (1995) Moore, A. W., Atkeson, C. G., & Schaal, S. (1995). Memory-based learning for control. Tech. rep. CMU-RI-TR-95-18, CMU Robotics Institute.\n* Narendra & Thathachar (1989) Narendra, K., & Thathachar, M. A. L. (1989). _Learning Automata: An Introduction_. Prentice-Hall, Englewood Cliffs, NJ.\n* Narendra & Thathachar (1974) Narendra, K. S., & Thathachar, M. A. L. (1974). Learning automata--a survey. _IEEE Transactions on Systems, Man, and Cybernetics_, \\(4\\) (4), 323-334.\n* Peng & Williams (1993) Peng, J., & Williams, R. J. (1993). Efficient learning and planning within the Dyna framework. _Adaptive Behavior_, _1_(4), 437-454.\n* Peng & Williams (1994) Peng, J., & Williams, R. J. (1994). Incremental multi-step Q-learning. In _Proceedings of the Eleventh International Conference on Machine Learning_, pp. 226-232 San Francisco, CA. Morgan Kaufmann.\n* Pomerleau (1993) Pomerleau, D. A. (1993). _Neural network perception for mobile robot guidance._Kluwer Academic Publishing.\n* Puterman (1994) Puterman, M. L. (1994). _Markov Decision Processes--Discrete Stochastic Dynamic Programming._John Wiley & Sons, Inc., New York, NY.\n* Puterman & Shin (1978) Puterman, M. L., & Shin, M. C. (1978). Modified policy iteration algorithms for discounted Markov decision processes. _Management Science_, _24_, 1127-1137.\n* Ring (1994) Ring, M. B. (1994). _Continual Learning in Reinforcement Environments_. Ph.D. thesis, University of Texas at Austin, Austin, Texas.\n* Rude (1993) Rude, U. (1993). _Mathematical and computational techniques for multilevel adaptive methods_. Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania.\n* Rumelhart & McClelland (1986) Rumelhart, D. E., & McClelland, J. L. (Eds.). (1986). _Parallel Distributed Processing: Explorations in the microstructures of cognition_. _Volume 1: Foundations_. The MIT Press, Cambridge, MA.\n* Rummery & Niranjan (1994) Rummery, G. A., & Niranjan, M. (1994). On-line Q-learning using connectionist systems. Tech. rep. CUED/F-INFENG/TR166, Cambridge University.\n\n* Rust (1996) Rust, J. (1996). Numerical dynamic programming in economics. In _Handbook of Computational Economics_. Elsevier, North Holland.\n* Sage & White (1977) Sage, A. P., & White, C. C. (1977). _Optimum Systems Control_. Prentice Hall.\n* Salganicoff & Ungar (1995) Salganicoff, M., & Ungar, L. H. (1995). Active exploration and learning in real-valued spaces using multi-armed bandit allocation indices. In Prieditis, A., & Russell, S. (Eds.), _Proceedings of the Twelfth International Conference on Machine Learning_, pp. 480-487 San Francisco, CA. Morgan Kaufmann.\n* Samuel (1959) Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. _IBM Journal of Research and Development_, \\(3\\), 211-229. Reprinted in E. A. Feigenbaum and J. Feldman, editors, _Computers and Thought_, McGraw-Hill, New York 1963.\n* Schaal & Atkeson (1994) Schaal, S., & Atkeson, C. (1994). Robot juggling: An implementation of memory-based learning. _Control Systems Magazine_, _14_.\n* Schmidhuber (1996) Schmidhuber, J. (1996). A general method for multi-agent learning and incremental self-improvement in unrestricted environments. In Yao, X. (Ed.), _Evolutionary Computation: Theory and Applications_. Scientific Publ. Co., Singapore.\n* Schmidhuber (1991a) Schmidhuber, J. H. (1991a). Curious model-building control systems. In _Proc. International Joint Conference on Neural Networks, Singapore_, Vol. 2, pp. 1458-1463. IEEE.\n* Schmidhuber (1991b) Schmidhuber, J. H. (1991b). Reinforcement learning in Markovian and non-Markovian environments. In Lippman, D. S., Moody, J. E., & Touretzky, D. S. (Eds.), _Advances in Neural Information Processing Systems 3_, pp. 500-506 San Mateo, CA. Morgan Kaufmann.\n* Schraudolph et al. (1994) Schraudolph, N. N., Dayan, P., & Sejnowski, T. J. (1994). Temporal difference learning of position evaluation in the game of Go. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), _Advances in Neural Information Processing Systems 6_, pp. 817-824 San Mateo, CA. Morgan Kaufmann.\n* Schrijver (1986) Schrijver, A. (1986). _Theory of Linear and Integer Programming_. Wiley-Interscience, New York, NY.\n* Schwartz (1993) Schwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In _Proceedings of the Tenth International Conference on Machine Learning_, pp. 298-305 Amherst, Massachusetts. Morgan Kaufmann.\n* Singh et al. (1994) Singh, S. P., Barto, A. G., Grupen, R., & Connolly, C. (1994). Robust reinforcement learning in motion planning. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), _Advances in Neural Information Processing Systems 6_, pp. 655-662 San Mateo, CA. Morgan Kaufmann.\n* Singh & Sutton (1996) Singh, S. P., & Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces. _Machine Learning_, _22_(1).\n* Singh et al. (1997)* Singh (1992a) Singh, S. P. (1992a). Reinforcement learning with a hierarchy of abstract models. In _Proceedings of the Tenth National Conference on Artificial Intelligence_, pp. 202-207 San Jose, CA. AAAI Press.\n* Singh (1992b) Singh, S. P. (1992b). Transfer of learning by composing solutions of elemental sequential tasks. _Machine Learning_, _8_(3), 323-340.\n* Singh (1993) Singh, S. P. (1993). _Learning to Solve Markovian Decision Processes_. Ph.D. thesis, Department of Computer Science, University of Massachusetts. Also, CMPSCI Technical Report 93-77.\n* Stengel (1986) Stengel, R. F. (1986). _Stochastic Optimal Control_. John Wiley and Sons.\n* Sutton (1996) Sutton, R. S. (1996). Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding. In Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), _Neural Information Processing Systems 8_.\n* Sutton (1984) Sutton, R. S. (1984). _Temporal Credit Assignment in Reinforcement Learning_. Ph.D. thesis, University of Massachusetts, Amherst, MA.\n* Sutton (1988) Sutton, R. S. (1988). Learning to predict by the method of temporal differences. _Machine Learning_, _3_(1), 9-44.\n* Sutton (1990) Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In _Proceedings of the Seventh International Conference on Machine Learning_ Austin, TX. Morgan Kaufmann.\n* Sutton (1991) Sutton, R. S. (1991). Planning by incremental dynamic programming. In _Proceedings of the Eighth International Workshop on Machine Learning_, pp. 353-357. Morgan Kaufmann.\n* Tesauro (1992) Tesauro, G. (1992). Practical issues in temporal difference learning. _Machine Learning_, \\(8\\), 257-277.\n* Tesauro (1994) Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level play. _Neural Computation_, _6_(2), 215-219.\n* Tesauro (1995) Tesauro, G. (1995). Temporal difference learning and TD-Gammon. _Communications of the ACM_, _38_(3), 58-67.\n* Tham & Prager (1994) Tham, C.-K., & Prager, R. W. (1994). A modular q-learning architecture for manipulator task decomposition. In _Proceedings of the Eleventh International Conference on Machine Learning_ San Francisco, CA. Morgan Kaufmann.\n* Thrun (1995) Thrun, S. (1995). Learning to play the game of chess. In Tesauro, G., Touretzky, D. S., & Leen, T. K. (Eds.), _Advances in Neural Information Processing Systems 7_ Cambridge, MA. The MIT Press.\n\n* Thrun & Schwartz (1993) Thrun, S., & Schwartz, A. (1993). Issues in using function approximation for reinforcement learning. In Mozer, M., Smolensky, P., Touretzky, D., Elman, J., & Weigend, A. (Eds.), _Proceedings of the 1993 Connectionist Models Summer School_ Hillsdale, NJ. Lawrence Erlbaum.\n* Thrun (1992) Thrun, S. B. (1992). The role of exploration in learning control. In White, D. A., & Sofge, D. A. (Eds.), _Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches_. Van Nostrand Reinhold, New York, NY.\n* Tsitsiklis (1994) Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. _Machine Learning_, _16_(3).\n* Tsitsiklis & Van Roy (1996) Tsitsiklis, J. N., & Van Roy, B. (1996). Feature-based methods for large scale dynamic programming. _Machine Learning_, _22_(1).\n* Valiant (1984) Valiant, L. G. (1984). A theory of the learnable. _Communications of the ACM_, _27_(11), 1134-1142.\n* Watkins (1989) Watkins, C. J. C. H. (1989). _Learning from Delayed Rewards_. Ph.D. thesis, King's College, Cambridge, UK.\n* Watkins & Dayan (1992) Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. _Machine Learning_, _8_(3), 279-292.\n* Whitehead (1991) Whitehead, S. D. (1991). Complexity and cooperation in Q-learning. In _Proceedings of the Eighth International Workshop on Machine Learning_ Evanston, IL. Morgan Kaufmann.\n* Williams (1987) Williams, R. J. (1987). A class of gradient-estimating algorithms for reinforcement learning in neural networks. In _Proceedings of the IEEE First International Conference on Neural Networks_ San Diego, CA.\n* Williams (1992) Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine Learning_, _8_(3), 229-256.\n* Williams & Baird (1993a) Williams, R. J., & Baird, III, L. C. (1993a). Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems. Tech. rep. NU-CCS-93-11, Northeastern University, College of Computer Science, Boston, MA.\n* Williams & Baird (1993b) Williams, R. J., & Baird, III, L. C. (1993b). Tight performance bounds on greedy policies based on imperfect value functions. Tech. rep. NU-CCS-93-14, Northeastern University, College of Computer Science, Boston, MA.\n* Wilson (1995) Wilson, S. (1995). Classifier fitness based on accuracy. _Evolutionary Computation_, _3_(2), 147-173.\n* Zhang & Dietterich (1995) Zhang, W., & Dietterich, T. G. (1995). A reinforcement learning approach to job-shop scheduling. In _Proceedings of the International Joint Conference on Artificial Intelligence_."}