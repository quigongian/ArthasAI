
{"text": "# A Kernel Method for the Two-Sample ProblemArthur Gretton arthur@tuebingen.mpg.de MPI for Biological CyberneticsSpemannstrasse 3872076, Tubingen, GermanyKarsten M Borgwardt1 kmb51@cam.ac.uk University of CambridgeDepartment of EngineeringTrumpington Street, CB2 1PZ Cambridge, United KingdomMalte J. Rasch malte@igi.tu-graz.ac.at Graz University of TechnologyInffeldgasse 16b/1 8010 Graz, AustriaBernhard Scholkopf bernhard.scheolkopf@tuebingen.mpg.de MPI for Biological CyberneticsSpemannstrasse 3872076, Tubingen, GermanyAlexander Smola alex.smola@gmail.com National ICT AustraliaCanberra, ACT 0200, AustraliaFootnote 1: This work was carried out while K.M.B. was with the Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen.###### AbstractWe propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.K Kernel methods, two sample test, uniform convergence bounds, schema matching, asymptotic analysis, hypothesis testing.## 1 IntroductionWe address the problem of comparing samples from two probability distributions, by proposing statistical tests of the hypothesis that these distributions are different (this is called the two-sample or homogeneity problem). Such tests have application in a variety of areas. In bioinformatics, it is of interest to compare microarray data from identical tissue types as measured by different laboratories, to detect whether the data may be analysed jointly, or whether differences in experimental procedure have caused systematic differences in the data distributions. Equally of interest are comparisons between microarray data from different tissue types, either to determine whether two subtypes of cancer may be treated as statistically indistinguishable from a diagnosis perspective, or to detect differences in healthy and cancerous tissue. In database attribute matching, it is desirable to merge databases containing multiple fields, where it is not known in advance which fields correspond: the fields are matched by maximising the similarity in the distributions of their entries.We test whether distributions \\(p\\) and \\(q\\) are different on the basis of samples drawn from each of them, by finding a well behaved (e.g. smooth) function which is large on the points drawn from \\(p\\), and small (as negative as possible) on the points from \\(q\\). We use as our test statistic the difference between the mean function values on the two samples; when this is large, the samples are likely from different distributions. We call this statistic the Maximum Mean Discrepancy (MMD).Clearly the quality of the MMD as a statistic depends on the class \\(\\mathcal{F}\\) of smooth functions that define it. On one hand, \\(\\mathcal{F}\\) must be \"rich enough\" so that the population MMD vanishes if and only if \\(p=q\\). On the other hand, for the test to be consistent, \\(\\mathcal{F}\\) needs to be \"restrictive\" enough for the empirical estimate of MMD to converge quickly to its expectation as the sample size increases. We shall use the unit balls in universal reproducing kernel Hilbert spaces (Steinwart, 2001) as our function classes, since these will be shown to satisfy both of the foregoing properties (we also review classical metrics on distributions, namely the Kolmogorov-Smirnov and Earth-Mover's distances, which are based on different function classes). On a more practical note, the MMD has a reasonable computational cost, when compared with other two-sample tests: given \\(m\\) points sampled from \\(p\\) and \\(n\\) from \\(q\\), the cost is \\(O(m+n)^{2}\\) time. We also propose a less statistically efficient algorithm with a computational cost of \\(O(m+n)\\), which can yield superior performance at a given computational cost by looking at a larger volume of data.We define three non-parametric statistical tests based on the MMD. The first two, which use distribution-independent uniform convergence bounds, provide finite sample guarantees of test performance, at the expense of being conservative in detecting differences between \\(p\\) and \\(q\\). The third test is based on the asymptotic distribution of the MMD, and is in practice more sensitive to differences in distribution at small sample sizes. The present work synthesizes and expands on results of Gretton et al. (2007a,b), Smola et al. (2007), and Song et al. (2008)1 who in turn build on the earlier work of Borgwardt et al. (2006). Note that the latter addresses only the third kind of test, and that the approach of Gretton et al. (2007a,b) employs a more accurate approximation to the asymptotic distribution of the test statistic.We begin our presentation in Section 2 with a formal definition of the MMD, and a proof that the population MMD is zero if and only if \\(p=q\\) when \\(\\mathcal{F}\\) is the unit ball of a universal RKHS. We also review alternative function classes for which the MMD defines a metric on probability distributions. In Section 3, we give an overview of hypothesis testing as it applies to the two-sample problem, and review other approaches to this problem. We present our first two hypothesis tests in Section 4, based on two different bounds on the deviation between the population and empirical MMD. We take a different approach in Section 5, where we use the asymptotic distribution of the empirical MMD estimate as the basis for a third test. When large volumes of data are available, the cost of computing the MMD (quadratic in the sample size) may be excessive: we therefore propose in Section 6 a modified version of the MMD statistic that has a linear cost in the number of samples, and an associated asymptotic test. In Section 7, we provide an overview of methods related to the MMD in the statistics and machine learning literature. Finally, in Section 8, we demonstrate the performance of MMD-based two-sample tests on problems from neuroscience, bioinformatics, and attribute matching using the Hungarian marriage method. Our approach performs well on high dimensional data with low sample size; in addition, we are able to successfully distinguish distributions on graph data, for which ours is the first proposed test.## 2 The Maximum Mean DiscrepancyIn this section, we present the maximum mean discrepancy (MMD), and describe conditions under which it is a metric on the space of probability distributions. The MMD is defined in terms of particular function spaces that witness the difference in distributions: we therefore begin in Section 2.1 by introducing the MMD for some arbitrary function space. In Section 2.2, we compute both the population MMD and two empirical estimates when the associated function space is a reproducing kernel Hilbert space, and we derive the RKHS function that witnesses the MMD for a given pair of distributions in Section 2.3. Finally, we describe the MMD for more general function classes in Section 2.4.### Definition of the Maximum Mean DiscrepancyOur goal is to formulate a statistical test that answers the following question:**Problem 1**: _Let \\(p\\) and \\(q\\) be Borel probability measures defined on a domain \\(\\mathcal{X}\\). Given observations \\(X:=\\{x_{1},\\ldots,x_{m}\\}\\) and \\(Y:=\\{y_{1},\\ldots,y_{n}\\}\\), drawn independently and identically distributed (i.i.d.) from \\(p\\) and \\(q\\), respectively, can we decide whether \\(p\eq q\\)?_To start with, we wish to determine a criterion that, in the population setting, takes on a unique and distinctive value only when \\(p=q\\). It will be defined based on Lemma 9.3.2 of Dudley (2002).**Lemma 1**: _Let \\((\\mathcal{X},d)\\) be a metric space, and let \\(p,q\\) be two Borel probability measures defined on \\(\\mathcal{X}\\). Then \\(p=q\\) if and only if \\(\\mathbf{E}_{x\\sim p}(f(x))=\\mathbf{E}_{y\\sim q}(f(y))\\) for all \\(f\\in C(\\mathcal{X})\\), where \\(C(\\mathcal{X})\\) is the space of bounded continuous functions on \\(\\mathcal{X}\\)._Although \\(C(\\mathcal{X})\\) in principle allows us to identify \\(p=q\\) uniquely, it is not practical to work with such a rich function class in the finite sample setting. We thus define a more general class of statistic, for as yet unspecified function classes \\(\\mathcal{F}\\), to measure the disparity between \\(p\\) and \\(q\\) (Fortet and Mourier, 1953; Muller, 1997).**Definition 2**: _Let \\(\\mathcal{F}\\) be a class of functions \\(f:\\mathcal{X}\\rightarrow\\mathbb{R}\\) and let \\(p,q,X,Y\\) be defined as above. We define the maximum mean discrepancy (MMD) as_\\[\\mathrm{MMD}\\left[\\mathcal{F},p,q\\right]:=\\sup_{f\\in\\mathcal{F}}\\left(\\mathbf{E }_{x\\sim p}[f(x)]-\\mathbf{E}_{y\\sim q}[f(y)]\\right). \\tag{1}\\]_Muller (1997) calls this an integral probability metric. A biased empirical estimate of the MMD is_\\[\\mathrm{MMD}_{b}\\left[\\mathcal{F},X,Y\\right]:=\\sup_{f\\in\\mathcal{F}}\\left( \\frac{1}{m}\\sum_{i=1}^{m}f(x_{i})-\\frac{1}{n}\\sum_{i=1}^{n}f(y_{i})\\right). \\tag{2}\\]The empirical MMD defined above has an upward bias (we will define an unbiased statistic in the following section). We must now identify a function class that is rich enough to uniquely identify whether \\(p=q\\), yet restrictive enough to provide useful finite sample estimates (the latter property will be established in subsequent sections).### The MMD in Reproducing Kernel Hilbert SpacesIf \\(\\mathcal{F}\\) is the unit ball in a reproducing kernel Hilbert space \\(\\mathcal{H}\\), the empirical MMD can be computed very efficiently. This will be the main approach we pursue in the present study. Other possible function classes \\(\\mathcal{F}\\) are discussed at the end of this section. We will refer to \\(\\mathcal{H}\\) as universal whenever \\(\\mathcal{H}\\), defined on a compact metric space \\(\\mathcal{X}\\) and with associated kernel \\(k:\\mathcal{X}^{2}\\rightarrow\\mathbb{R}\\), is dense in \\(C(\\mathcal{X})\\) with respect to the \\(L_{\\infty}\\) norm. It is shown in Steinwart (2001) that Gaussian and Laplace kernels are universal. We have the following result:**Theorem 3**: _Let \\(\\mathcal{F}\\) be a unit ball in a universal RKHS \\(\\mathcal{H}\\), defined on the compact metric space \\(\\mathcal{X}\\), with associated kernel \\(k(\\cdot,\\cdot)\\). Then \\(\\mathrm{MMD}\\left[\\mathcal{F},p,q\\right]=0\\) if and only if \\(p=q\\)._**Proof** It is clear that \\(\\mathrm{MMD}\\left[\\mathcal{F},p,q\\right]\\) is zero if \\(p=q\\). We prove the converse by showing that \\(\\mathrm{MMD}\\left[C(\\mathcal{X}),p,q\\right]=D\\) for some \\(D>0\\) implies \\(\\mathrm{MMD}\\left[\\mathcal{F},p,q\\right]>0\\): this is equivalent to \\(\\mathrm{MMD}\\left[\\mathcal{F},p,q\\right]=0\\) implying \\(\\mathrm{MMD}\\left[C(\\mathcal{X}),p,q\\right]=0\\) (where this last result implies \\(p=q\\) by Lemma 1, noting that compactness of the metric space \\(\\mathcal{X}\\) implies its separability). Let \\(\\mathcal{H}\\) be the universal RKHS of which \\(\\mathcal{F}\\) is the unit ball. If \\(\\mathrm{MMD}\\left[C(\\mathcal{X}),p,q\\right]=D\\), then there exists some \\(\\tilde{f}\\in C(\\mathcal{X})\\) for which \\(\\mathbf{E}_{p}\\left[\\tilde{f}\\right]-\\mathbf{E}_{q}\\left[\\tilde{f}\\right]\\geq D/2\\). We know that \\(\\mathcal{H}\\) is dense in \\(C(\\mathcal{X})\\) with respect to the \\(L_{\\infty}\\) norm: this means that for \\(\\epsilon=D/8\\), we can find some \\(f^{*}\\in\\mathcal{H}\\) satisfying \\(\\left\\|f^{*}-\\tilde{f}\\right\\|_{\\infty}<\\epsilon\\). Thus, we obtain \\(\\left|\\mathbf{E}_{p}\\left[f^{*}\\right]-\\mathbf{E}_{p}\\left[\\tilde{f}\\right] \\right|<\\epsilon\\) and consequently\\[\\left|\\mathbf{E}_{p}\\left[f^{*}\\right]-\\mathbf{E}_{q}\\left[f^{*}\\right]\\right| >\\left|\\mathbf{E}_{p}\\left[\\tilde{f}\\right]-\\mathbf{E}_{q}\\left[\\tilde{f} \\right]\\right|-2\\epsilon>\\tfrac{D}{2}-2\\tfrac{D}{8}=\\tfrac{D}{4}>0.\\]Finally, using \\(\\left\\|f^{*}\\right\\|_{\\mathcal{H}}<\\infty\\), we have\\[\\left[\\mathbf{E}_{p}\\left[f^{*}\\right]-\\mathbf{E}_{q}\\left[f^{*}\\right]\\right] /\\left\\|f^{*}\\right\\|_{\\mathcal{H}}\\geq D/(4\\left\\|f^{*}\\right\\|_{\\mathcal{H }})>0,\\]and hence \\(\\mathrm{MMD}\\left[\\mathcal{F},p,q\\right]>0\\).We now review some properties of \\(\\mathcal{H}\\) that will allow us to express the MMD in a more easily computable form (Scholkopf and Smola, 2002). Since \\(\\mathcal{H}\\) is an RKHS, the operator of evaluation \\(\\delta_{x}\\) mapping \\(f\\in\\mathcal{H}\\) to \\(f(x)\\in\\mathbb{R}\\) is continuous. Thus, by the Riesz representation theorem, there is a feature mapping \\(\\phi(x)\\) from \\(\\mathcal{X}\\) to \\(\\mathbb{R}\\) such that \\(f(x)=\\left\\langle f,\\phi(x)\\right\\rangle_{\\mathcal{H}}\\). Moreover, \\(\\left\\langle\\phi(x),\\phi(y)\\right\\rangle_{\\mathcal{H}}=k(x,y)\\), where \\(k(x,y)\\) is a positive definite kernel function. The following lemma is due to Borgwardt et al. (2006).**Lemma 4**: _Denote the expectation of \\(\\phi(x)\\) by \\(\\mu_{p}:=\\mathbf{E}_{p}\\left[\\phi(x)\\right]\\) (assuming its existence).2 Then_Footnote 2: A sufficient condition for this is \\(\\|\\mu_{p}\\|_{\\mathcal{H}}^{2}<\\infty\\), which is rearranged as \\(\\mathbf{E}_{p}[k(x,x^{\\prime})]<\\infty\\), where \\(x\\) and \\(x^{\\prime}\\) are independent random variables drawn according to \\(p\\). In other words, \\(k\\) is a trace class operator with respect to the measure \\(p\\).\\[\\mathrm{MMD}[\\mathcal{F},p,q]=\\sup_{\\|f\\|_{\\mathcal{H}}\\leq 1}\\left\\langle\\mu [p]-\\mu[q],f\\right\\rangle=\\|\\mu[p]-\\mu[q]\\|_{\\mathcal{H}}. \\tag{3}\\]**Proof**\\[\\mathrm{MMD}^{2}[\\mathcal{F},p,q] = \\left[\\sup_{\\|f\\|_{\\mathcal{H}}\\leq 1}\\left(\\mathbf{E}_{p} \\left[f(x)\\right]-\\mathbf{E}_{q}\\left[f(y)\\right]\\right)\\right]^{2}\\] \\[= \\left[\\sup_{\\|f\\|_{\\mathcal{H}}\\leq 1}\\left(\\mathbf{E}_{p} \\left[\\left\\langle\\phi(x),f\\right\\rangle_{\\mathcal{H}}\\right]-\\mathbf{E}_{q} \\left[\\left\\langle\\phi(y),f\\right\\rangle_{\\mathcal{H}}\\right]\\right)^{2}\\right.\\] \\[= \\left[\\sup_{\\|f\\|_{\\mathcal{H}}\\leq 1}\\left\\langle\\mu_{p}-\\mu_{q},f \\right\\rangle_{\\mathcal{H}}\\right]^{2}=\\|\\mu_{p}-\\mu_{q}\\|_{\\mathcal{H}}^{2}\\]Given we are in an RKHS, the norm \\(\\|\\mu_{p}-\\mu_{q}\\|_{\\mathcal{H}}^{2}\\) may easily be computed in terms of kernel functions. This leads to a first empirical estimate of the MMD, which is unbiased.**Lemma 5**: _Given \\(x\\) and \\(x^{\\prime}\\) independent random variables with distribution \\(p\\), and \\(y\\) and \\(y^{\\prime}\\) independent random variables with distribution \\(q\\), the population \\(\\mathrm{MMD}^{2}\\) is_\\[\\mathrm{MMD}^{2}\\left[\\mathcal{F},p,q\\right]=\\mathbf{E}_{x,x^{\\prime}\\sim p} \\left[k(x,x^{\\prime})\\right]-2\\mathbf{E}_{x\\sim p,y\\sim q}\\left[k(x,y)\\right]+ \\mathbf{E}_{y,y^{\\prime}\\sim q}\\left[k(y,y^{\\prime})\\right]. \\tag{4}\\]_Let \\(Z:=(z_{1},\\ldots,z_{m})\\) be \\(m\\) i.i.d. random variables, where \\(z_{i}:=(x_{i},y_{i})\\) (i.e. we assume \\(m=n\\)). An unbiased empirical estimate of \\(\\mathrm{MMD}^{2}\\) is_\\[\\mathrm{MMD}^{2}_{u}\\left[\\mathcal{F},X,Y\\right]=\\frac{1}{(m)(m-1)}\\sum_{i\eq j }^{m}h(z_{i},z_{j}), \\tag{5}\\]_which is a one-sample U-statistic with \\(h(z_{i},z_{j}):=k(x_{i},x_{j})+k(y_{i},y_{j})-k(x_{i},y_{j})-k(x_{j},y_{i})\\) (we define \\(h(z_{i},z_{j})\\) to be symmetric in its arguments due to requirements that will arise in Section 5)._**Proof** Starting from the expression for \\(\\mathrm{MMD}^{2}[\\mathcal{F},p,q]\\) in Lemma 4,\\[\\mathrm{MMD}^{2}[\\mathcal{F},p,q] = \\|\\mu_{p}-\\mu_{q}\\|_{\\mathcal{H}}^{2}\\] \\[= \\left\\langle\\mu_{p},\\mu_{p}\\right\\rangle_{\\mathcal{H}}+\\left\\langle \\mu_{q},\\mu_{q}\\right\\rangle_{\\mathcal{H}}-2\\left\\langle\\mu_{p},\\mu_{q}\\right \\rangle_{\\mathcal{H}}\\] \\[= \\mathbf{E}_{p}\\left\\langle\\phi(x),\\phi(x^{\\prime})\\right\\rangle _{\\mathcal{H}}+\\mathbf{E}_{q}\\left\\langle\\phi(y),\\phi(y^{\\prime})\\right\\rangle _{\\mathcal{H}}-2\\mathbf{E}_{p,q}\\left\\langle\\phi(x),\\phi(y)\\right\\rangle_{ \\mathcal{H}},\\]The proof is completed by applying \\(\\left\\langle\\phi(x),\\phi(x^{\\prime})\\right\\rangle_{\\mathcal{H}}=k(x,x^{\\prime})\\); the empirical estimate follows straightforwardly. The empirical statistic is an unbiased estimate of \\(\\mathrm{MMD}^{2}\\), although it does not have minimum variance, since we are ignoring the cross-terms \\(k(x_{i},y_{i})\\) of which there are only \\(O(n)\\). The minimum variance estimate is almost identical, though (Serfling, 1980, Section 5.1.4).The biased statistic in (2) may also be easily computed following the above reasoning. Substituting the empirical estimates \\(\\mu[X]:=\\frac{1}{m}\\sum_{i=1}^{m}\\phi(x_{i})\\) and \\(\\mu[Y]:=\\frac{1}{n}\\sum_{i=1}^{n}\\phi(y_{i})\\) of the feature space means based on respective samples \\(X\\) and \\(Y\\), we obtain\\[\\mathrm{MMD}_{b}\\left[\\mathcal{F},X,Y\\right]=\\left[\\frac{1}{m^{2}}\\sum_{i,j=1} ^{m}k(x_{i},x_{j})-\\frac{2}{mn}\\sum_{i,j=1}^{m,n}k(x_{i},y_{j})+\\frac{1}{n^{2} }\\sum_{i,j=1}^{n}k(y_{i},y_{j})\\right]^{\\frac{1}{2}}. \\tag{6}\\]Intuitively we expect the empirical test statistic \\(\\mathrm{MMD}[\\mathcal{F},X,Y]\\), whether biased or unbiased, to be small if \\(p=q\\), and large if the distributions are far apart. It costs \\(O((m+n)^{2})\\) time to compute both statistics.Finally, we note that Harchaoui et al. (2008) recently proposed a modification of the kernel MMD statistic in Lemma 4, by scaling the feature space mean distance using the inverse within-sample covariance operator, thus employing the kernel Fisher discriminant as a statistic for testing homogeneity. This statistic is shown to be related to the \\(\\chi^{2}\\) divergence.### Witness Function of the MMD for RKHSsIt is also instructive to consider the witness \\(f\\) which is chosen by MMD to exhibit the maximum discrepancy between the two distributions. The population \\(f\\) and its empiricalFigure 1: Illustration of the function maximizing the mean discrepancy in the case where a Gaussian is being compared with a Laplace distribution. Both distributions have zero mean and unit variance. The function \\(f\\) that witnesses the MMD has been scaled for plotting purposes, and was computed empirically on the basis of \\(2\\times 10^{4}\\) samples, using a Gaussian kernel with \\(\\sigma=0.5\\).estimate \\(\\hat{f}(x)\\) are respectively\\[\\begin{array}{rclrcl}f(x)&\\propto&\\langle\\phi(x),\\mu[p]-\\mu[q]\\rangle&=&{\\bf E} _{x^{\\prime}\\sim p}\\left[k(x,x^{\\prime})\\right]-{\\bf E}_{x^{\\prime}\\sim q}\\left[ k(x,x^{\\prime})\\right]\\\\ \\hat{f}(x)&\\propto&\\langle\\phi(x),\\mu[X]-\\mu[Y]\\rangle&=&\\frac{1}{m}\\sum_{i=1}^ {m}k(x_{i},x)-\\frac{1}{n}\\sum_{i=1}^{n}k(y_{i},x).\\end{array}\\]This follows from the fact that the unit vector \\(v\\) maximizing \\(\\left\\langle v,x\\right\\rangle_{\\mathcal{H}}\\) in a Hilbert space is \\(v=x/\\left\\|x\\right\\|\\).We illustrate the behavior of MMD in Figure 1 using a one-dimensional example. The data \\(X\\) and \\(Y\\) were generated from distributions \\(p\\) and \\(q\\) with equal means and variances, with \\(p\\) Gaussian and \\(q\\) Laplacian. We chose \\(\\mathcal{F}\\) to be the unit ball in an RKHS using the Gaussian kernel. We observe that the function \\(f\\) that witnesses the MMD -- in other words, the function maximizing the mean discrepancy in (1) -- is smooth, positive where the Laplace density exceeds the Gaussian density (at the center and tails), and negative where the Gaussian density is larger. Moreover, the magnitude of \\(f\\) is a direct reflection of the amount by which one density exceeds the other, insofar as the smoothness constraint permits it.### The MMD in Other Function ClassesThe definition of the maximum mean discrepancy is by no means limited to RKHS. In fact, any function class \\(\\mathcal{F}\\) that comes with uniform convergence guarantees and is sufficiently powerful will enjoy the above properties.**Definition 6**: _Let \\(\\mathcal{F}\\) be a subset of some vector space. The star \\(S[\\mathcal{F}]\\) of a set \\(\\mathcal{F}\\) is_\\[S[\\mathcal{F}]:=\\{\\alpha x|x\\in\\mathcal{F}\\text{ and }\\alpha\\in[0,\\infty)\\}\\]**Theorem 7**: _Denote by \\(\\mathcal{F}\\) the subset of some vector space of functions from \\(\\mathcal{X}\\) to \\(\\mathbb{R}\\) for which \\(S[\\mathcal{F}]\\cap C(\\mathcal{X})\\) is dense in \\(C(\\mathcal{X})\\) with respect to the \\(L_{\\infty}(\\mathcal{X})\\) norm. Then \\(\\mathrm{MMD}\\left[\\mathcal{F},p,q\\right]=0\\) if and only if \\(p=q\\).__Moreover, under the above conditions \\(\\mathrm{MMD}[\\mathcal{F},p,q]\\) is a metric on the space of probability distributions. Whenever the star of \\(\\mathcal{F}\\) is not dense, \\(\\mathrm{MMD}\\) is a pseudo-metric space.3_Footnote 3: According to Dudley (2002, p. 26) a metric \\(d(x,y)\\) satisfies the following four properties: symmetry, triangle inequality, \\(d(x,x)=0\\), and \\(d(x,y)=0\\implies x=y\\). A pseudo-metric only satisfies the first three properties.**Proof** The first part of the proof is almost identical to that of Theorem 3 and is therefore omitted. To see the second part, we only need to prove the triangle inequality. We have\\[\\sup_{f\\in\\mathcal{F}}\\left|E_{p}f-E_{q}f\\right|+\\sup_{g\\in \\mathcal{F}}\\left|E_{q}g-E_{r}g\\right| \\geq\\sup_{f\\in\\mathcal{F}}\\left[\\left|E_{p}f-E_{q}f\\right|+\\left|E _{q}f-E_{r}\\right|\\right]\\] \\[\\geq\\sup_{f\\in\\mathcal{F}}\\left|E_{p}f-E_{r}f\\right|.\\]The first part of the theorem establishes that \\(\\mathrm{MMD}[\\mathcal{F},p,q]\\) is a metric, since only for \\(p=q\\) do we have \\(\\mathrm{MMD}[\\mathcal{F},p,q]=0\\). \\(\\blacksquare\\)Note that any uniform convergence statements in terms of \\(\\mathcal{F}\\) allow us immediately to characterize an estimator of \\(\\mathrm{MMD}(\\mathcal{F},p,q)\\) explicitly. The following result shows how (we will refine this reasoning for the RKHS case in Section 4).**Theorem 8**: _Let \\(\\delta\\in(0,1)\\) be a confidence level and assume that for some \\(\\epsilon(\\delta,m,\\mathcal{F})\\) the following holds for samples \\(\\{x_{1},\\ldots,x_{m}\\}\\) drawn from \\(p\\):_\\[\\Pr\\left\\{\\sup_{f\\in\\mathcal{F}}\\left|\\mathbf{E}_{p}[f]-\\frac{1}{m}\\sum_{i=1}^{ m}f(x_{i})\\right|>\\epsilon(\\delta,m,\\mathcal{F})\\right\\}\\leq\\delta. \\tag{7}\\]_In this case we have that_\\[\\Pr\\left\\{\\left|\\mathrm{MMD}[\\mathcal{F},p,q]-\\mathrm{MMD}_{b}[\\mathcal{F},X,Y] \\right|>2\\epsilon(\\delta/2,m,\\mathcal{F})\\right\\}\\leq\\delta. \\tag{8}\\]**Proof** The proof works simply by using convexity and suprema as follows:\\[\\left|\\mathrm{MMD}[\\mathcal{F},p,q]-\\mathrm{MMD}_{b}[\\mathcal{F},X,Y]\\right|\\] \\[= \\left|\\sup_{f\\in\\mathcal{F}}\\left|\\mathbf{E}_{p}[f]-\\mathbf{E}_{ q}[f]\\right|-\\sup_{f\\in\\mathcal{F}}\\left|\\frac{1}{m}\\sum_{i=1}^{m}f(x_{i})- \\frac{1}{n}\\sum_{i=1}^{n}f(y_{i})\\right|\\right|\\] \\[\\leq \\sup_{f\\in\\mathcal{F}}\\left|\\mathbf{E}_{p}[f]-\\mathbf{E}_{q}[f]- \\frac{1}{m}\\sum_{i=1}^{m}f(x_{i})+\\frac{1}{n}\\sum_{i=1}^{n}f(y_{i})\\right|\\] \\[\\leq \\sup_{f\\in\\mathcal{F}}\\left|\\mathbf{E}_{p}[f]-\\frac{1}{m}\\sum_{i= 1}^{m}f(x_{i})\\right|+\\sup_{f\\in\\mathcal{F}}\\left|\\mathbf{E}_{q}[f]-\\frac{1}{n }\\sum_{i=1}^{n}f(y_{i})\\right|.\\]Bounding each of the two terms via a uniform convergence bound proves the claim.This shows that \\(\\mathrm{MMD}_{b}[\\mathcal{F},X,Y]\\) can be used to estimate \\(\\mathrm{MMD}[\\mathcal{F},p,q]\\) and that the quantity is asymptotically unbiased.**Remark 9** (Reduction to Binary Classification): _Any classifier which maps a set of observations \\(\\{z_{i},l_{i}\\}\\) with \\(z_{i}\\in\\mathcal{X}\\) on some domain \\(\\mathcal{X}\\) and labels \\(l_{i}\\in\\{\\pm 1\\}\\), for which uniform convergence bounds exist on the convergence of the empirical loss to the expected loss, can be used to obtain a similarity measure on distributions -- simply assign \\(l_{i}=1\\) if \\(z_{i}\\in X\\) and \\(l_{i}=-1\\) for \\(z_{i}\\in Y\\) and find a classifier which is able to separate the two sets. In this case maximization of \\(\\mathbf{E}_{p}[f]-\\mathbf{E}_{q}[f]\\) is achieved by ensuring that as many \\(z\\sim p(z)\\) as possible correspond to \\(f(z)=1\\), whereas for as many \\(z\\sim q(z)\\) as possible we have \\(f(z)=-1\\). Consequently neural networks, decision trees, boosted classifiers and other objects for which uniform convergence bounds can be obtained can be used for the purpose of distribution comparison. For instance, Ben-David et al. (2007, Section 4) use the error of a hyperplane classifier to approximate the \\(\\mathcal{A}\\)-distance between distributions of Kifer et al. (2004)._### Examples of Non-RKHS Function ClassesOther function spaces \\(\\mathcal{F}\\) inspired by the statistics literature can also be considered in defining the MMD. Indeed, Lemma 1 defines an MMD with \\(\\mathcal{F}\\) the space of bounded continuous real-valued functions, which is a Banach space with the supremum norm (Dudley, 2002, p. 158). We now describe two further metrics on the space of probability distributions, the Kolmogorov-Smirnov and Earth Mover's distances, and their associated function classes.#### 2.5.1 Kolmogorov-Smirnov StatisticThe Kolmogorov-Smirnov (K-S) test is probably one of the most famous two-sample tests in statistics. It works for random variables \\(x\\in\\mathbb{R}\\) (or any other set for which we can establish a total order). Denote by \\(F_{p}(x)\\) the cumulative distribution function of \\(p\\) and let \\(F_{X}(x)\\) be its empirical counterpart, that is\\[F_{p}(z):=\\Pr\\left\\{x\\leq z\\text{ for }x\\sim p(x)\\right\\}\\text{ and }F_{X}(z):= \\frac{1}{|X|}\\sum_{i=1}^{m}1_{z\\leq x_{i}}.\\]It is clear that \\(F_{p}\\) captures the properties of \\(p\\). The Kolmogorov metric is simply the \\(L_{\\infty}\\) distance \\(\\left\\|F_{X}-F_{Y}\\right\\|_{\\infty}\\) for two sets of observations \\(X\\) and \\(Y\\). Smirnov (1939) showed that for \\(p=q\\) the limiting distribution of the empirical cumulative distribution functions satisfies\\[\\lim_{m,n\\to\\infty}\\Pr\\left\\{\\left[\\tfrac{mn}{m+n}\\right]^{\\frac{1}{2}}\\left\\| F_{X}-F_{Y}\\right\\|_{\\infty}>x\\right\\}=2\\sum_{j=1}^{\\infty}(-1)^{j-1}e^{-2j^{2} x^{2}}\\text{ for }x\\geq 0. \\tag{9}\\]This allows for an efficient characterization of the distribution under the null hypothesis \\(\\mathcal{H}_{0}\\). Efficient numerical approximations to (9) can be found in numerical analysis handbooks (Press et al., 1994). The distribution under the alternative, \\(p\eq q\\), however, is unknown.The Kolmogorov metric is, in fact, a special instance of \\(\\text{MMD}[\\mathcal{F},p,q]\\) for a certain Banach space (Muller, 1997, Theorem 5.2)**Proposition 10**: _Let \\(\\mathcal{F}\\) be the class of functions \\(\\mathcal{X}\\to\\mathbb{R}\\) of bounded variation4 1. Then \\(\\text{MMD}[\\mathcal{F},p,q]=\\left\\|F_{p}-F_{q}\\right\\|_{\\infty}\\)._Footnote 4: A function \\(f\\) defined on \\([a,b]\\) is of bounded variation \\(C\\) if the total variation is bounded by \\(C\\), i.e. the supremum over all sums\\[\\sum_{1\\leq i\\leq n}|f(x_{i})-f(x_{i-1})|,\\] where \\(a\\leq x_{0}\\leq\\ldots\\leq x_{n}\\leq b\\) (Dudley, 2002, p. 184).#### 2.5.2 Earth-Mover DistancesAnother class of distance measures on distributions that may be written as an MMD are the Earth-Mover distances. We assume \\((\\mathcal{X},d)\\) is a separable metric space, and define \\(\\mathcal{P}_{1}(\\mathcal{X})\\) to be the space of probability measures on \\(\\mathcal{X}\\) for which \\(\\int d(x,z)dp(z)<\\infty\\) for all \\(p\\in\\mathcal{P}_{1}(\\mathcal{X})\\) and \\(x\\in\\mathcal{X}\\) (these are the probability measures for which \\(\\mathbf{E}\\left|x\\right|<\\infty\\) when \\(\\mathcal{X}=\\mathbb{R}\\)). We then have the following definition (Dudley, 2002, p. 420).**Definition 11** (Monge-Wasserstein metric): _Let \\(p\\in\\mathcal{P}_{1}(\\mathcal{X})\\) and \\(q\\in\\mathcal{P}_{1}(\\mathcal{X})\\). The Monge-Wasserstein distance is defined as_\\[W(p,q):=\\inf_{\\mu\\in M(p,q)}\\int d(x,y)d\\mu(x,y),\\]_where \\(M(p,q)\\) is the set of joint distributions on \\(\\mathcal{X}\\times\\mathcal{X}\\) with marginals \\(p\\) and \\(q\\)._We may interpret this as the cost (as represented by the metric \\(d(x,y)\\)) of transferring mass distributed according to \\(p\\) to a distribution in accordance with \\(q\\), where \\(\\mu\\) is the movement schedule. In general, a large variety of costs of moving mass from \\(x\\) to \\(y\\) can be used, such as psychooptical similarity measures in image retrieval (Rubner et al., 2000). The following theorem holds (Dudley, 2002, Theorem 11.8.2).**Theorem 12** (Kantorovich-Rubinstein): _Let \\(p\\in\\mathcal{P}_{1}(\\mathfrak{X})\\) and \\(q\\in\\mathcal{P}_{1}(\\mathfrak{X})\\), where \\(\\mathfrak{X}\\) is separable. Then a metric on \\(\\mathcal{P}_{1}(S)\\) is defined as_\\[W(p,q)=\\left\\|p-q\\right\\|_{L}^{*}=\\sup_{\\left\\|f\\right\\|_{L}\\leq 1}\\left| \\int f\\,d(p-q)\\right|,\\]_where_\\[\\left\\|f\\right\\|_{L}:=\\sup_{x\eq y\\,\\in\\,\\mathfrak{X}}\\frac{\\left|f(x)-f(y) \\right|}{d(x,y)}\\]_is the Lipschitz seminorm5 for real valued \\(f\\) on \\(\\mathfrak{X}\\)._Footnote 5: A seminorm satisfies the requirements of a norm besides \\(\\left\\|x\\right\\|=0\\) only for \\(x=0\\) (Dudley, 2002, p. 156).A simple example of this theorem is as follows (Dudley, 2002, Exercise 1, p. 425).**Example 1**: _Let \\(\\mathfrak{X}=\\mathbb{R}\\) with associated \\(d(x,y)=\\left|x-y\\right|\\). Then given \\(f\\) such that \\(\\left\\|f\\right\\|_{L}\\leq 1\\), we use integration by parts to obtain_\\[\\left|\\int f\\,d(p-q)\\right|=\\left|\\int(F_{p}-F_{q})(x)f^{\\prime}(x)dx\\right| \\leq\\int\\left|(F_{p}-F_{q})\\right|(x)dx,\\]_where the maximum is attained for the function \\(g\\) with derivative \\(g^{\\prime}=2\\,1_{F_{p}>F_{q}}-1\\) (and for which \\(\\left\\|g\\right\\|_{L}=1\\)). We recover the \\(L_{1}\\) distance between distribution functions,_\\[W(P,Q)=\\int\\left|(F_{p}-F_{q})\\right|(x)dx.\\]One may further generalize Theorem 12 to the set of all laws \\(\\mathcal{P}(\\mathfrak{X})\\) on arbitrary metric spaces \\(\\mathfrak{X}\\)(Dudley, 2002, Proposition 11.3.2).**Definition 13** (Bounded Lipschitz metric): _Let \\(p\\) and \\(q\\) be laws on a metric space \\(\\mathfrak{X}\\). Then_\\[\\beta(p,q):=\\sup_{\\left\\|f\\right\\|_{BL}\\leq 1}\\left|\\int f\\,d(p-q)\\right|\\]_is a metric on \\(\\mathcal{P}(\\mathfrak{X})\\), where \\(f\\) belongs to the space of bounded Lipschitz functions with norm_\\[\\left\\|f\\right\\|_{BL}:=\\left\\|f\\right\\|_{L}+\\left\\|f\\right\\|_{\\infty}.\\]## 3 Background MaterialWe now present three background results. First, we introduce the terminology used in statistical hypothesis testing. Second, we demonstrate via an example that even for tests which have asymptotically no error, one cannot guarantee performance at any fixed sample size without making assumptions about the distributions. Finally, we briefly review some earlier approaches to the two-sample problem.### Statistical Hypothesis TestingHaving described a metric on probability distributions (the MMD) based on distances between their Hilbert space embeddings, and empirical estimates (biased and unbiased) of this metric, we now address the problem of determining whether the empirical MMD shows a _statistically significant_ difference between distributions. To this end, we briefly describe the framework of statistical hypothesis testing as it applies in the present context, following Casella and Berger (2002, Chapter 8). Given i.i.d. samples \\(X\\sim p\\) of size \\(m\\) and \\(Y\\sim q\\) of size \\(n\\), the statistical test, \\(\\mathcal{T}(X,Y)\\,:\\,\\mathcal{X}^{m}\\times\\mathcal{X}^{n}\\mapsto\\{0,1\\}\\) is used to distinguish between the null hypothesis \\(\\mathcal{H}_{0}\\,:\\,p=q\\) and the alternative hypothesis \\(\\mathcal{H}_{1}\\,:\\,p\eq q\\). This is achieved by comparing the test statistic6\\(\\mathrm{MMD}[\\mathcal{F},X,Y]\\) with a particular threshold: if the threshold is exceeded, then the test rejects the null hypothesis (bearing in mind that a zero population MMD indicates \\(p=q\\)). The acceptance region of the test is thus defined as the set of real numbers below the threshold. Since the test is based on finite samples, it is possible that an incorrect answer will be returned: we define the Type I error as the probability of rejecting \\(p=q\\) based on the observed sample, despite the null hypothesis having generated the data. Conversely, the Type II error is the probability of accepting \\(p=q\\) despite the underlying distributions being different. The _level_\\(\\alpha\\) of a test is an upper bound on the Type I error: this is a design parameter of the test, and is used to set the threshold to which we compare the test statistic (finding the test threshold for a given \\(\\alpha\\) is the topic of Sections 4 and 5). A consistent test achieves a level \\(\\alpha\\), and a Type II error of zero, in the large sample limit. We will see that the tests proposed in this paper are consistent.Footnote 6: This may be biased or unbiased.### A Negative ResultEven if a test is consistent, it is not possible to distinguish distributions with high probability at a given, fixed sample size (i.e., to provide guarantees on the Type II error), without prior assumptions as to the nature of the difference between \\(p\\) and \\(q\\). This is true _regardless_ of the two-sample test used. There are several ways to illustrate this, which each give different insight into the kinds of differences that might be undetectable for a given number of samples. The following example7 is one such illustration.Footnote 7: This is a variation of a construction for independence tests, which was suggested in a private communication by John Langford.**Example 2**: _Assume that we have a distribution \\(p\\) from which we draw \\(m\\) iid observations. Moreover, we construct a distribution \\(q\\) by drawing \\(m^{2}\\) iid observations from p and subsequently defining a discrete distribution over these \\(m^{2}\\) instances with probability \\(m^{-2}\\) each. It is easy to check that if we now draw \\(m\\) observations from \\(q\\), there is at least a \\(\\binom{m^{2}}{m}\\frac{m!}{m^{2m}}>1-e^{-1}>0.63\\) probability that we thereby will have effectively obtained an \\(m\\) sample from \\(p\\). Hence no test will be able to distinguish samples from \\(p\\) and \\(q\\) in this case. We could make the probability of detection arbitrarily small by increasing the size of the sample from which we construct \\(q\\)._### Previous WorkWe next give a brief overview of some earlier approaches to the two sample problem for multivariate data. Since our later experimental comparison is with respect to certain of these methods, we give abbreviated algorithm names in italics where appropriate: these should be used as a key to the tables in Section 8. A generalisation of the Wald-Wolfowitz runs test to the multivariate domain was proposed and analysed by Friedman and Rafsky (1979); Henze and Penrose (1999) _(FR Wolf)_, and involves counting the number of edges in the minimum spanning tree over the aggregated data that connect points in \\(X\\) to points in \\(Y\\). The resulting test relies on the asymptotic normality of the test statistic, and this quantity is not distribution-free under the null hypothesis for finite samples (it depends on \\(p\\) and \\(q\\)). The computational cost of this method using Kruskal's algorithm is \\(O((m+n)^{2}\\log(m+n))\\), although more modern methods improve on the \\(\\log(m+n)\\) term. See Chazelle (2000) for details. Friedman and Rafsky (1979) claim that calculating the matrix of distances, which costs \\(O((m+n)^{2})\\), dominates their computing time; we return to this point in our experiments (Section 8). Two possible generalisations of the Kolmogorov-Smirnov test to the multivariate case were studied in (Bickel, 1969; Friedman and Rafsky, 1979). The approach of Friedman and Rafsky _(FR Smirnov)_ in this case again requires a minimal spanning tree, and has a similar cost to their multivariate runs test.A more recent multivariate test was introduced by Rosenbaum (2005). This entails computing the minimum distance non-bipartite matching over the aggregate data, and using the number of pairs containing a sample from both \\(X\\) and \\(Y\\) as a test statistic. The resulting statistic is distribution-free under the null hypothesis at finite sample sizes, in which respect it is superior to the Friedman-Rafsky test; on the other hand, it costs \\(O((m+n)^{3})\\) to compute. Another distribution-free test _(Hall)_ was proposed by Hall and Tajvidi (2002): for each point from \\(p\\), it requires computing the closest points in the aggregated data, and counting how many of these are from \\(q\\) (the procedure is repeated for each point from \\(q\\) with respect to points from \\(p\\)). As we shall see in our experimental comparisons, the test statistic is costly to compute; Hall and Tajvidi (2002) consider only tens of points in their experiments.Yet another approach is to use some distance (e.g. \\(L_{1}\\) or \\(L_{2}\\)) between Parzen window estimates of the densities as a test statistic (Anderson et al., 1994; Biau and Gyorfi, 2005), based on the asymptotic distribution of this distance given \\(p=q\\). When the \\(L_{2}\\) norm is used, the test statistic is related to those we present here, although it is arrived at from a different perspective. Briefly, the test of Anderson et al. (1994) is obtained in a more restricted setting where the RKHS kernel is an inner product between Parzen windows. Since we are not doing density estimation, however, we need not decrease the kernel width as the sample grows. In fact, decreasing the kernel width reduces the convergence rate of the associated two-sample test, compared with the \\((m+n)^{-1/2}\\) rate for fixed kernels. We provide more detail in Section 7.1. The \\(L_{1}\\) approach of Biau and Gyorfi (2005) _(Biau)_ requires the space to be partitioned into a grid of bins, which becomes difficult or impossible for high dimensional problems. Hence we use this test only for low-dimensional problems in our experiments.## 4 Tests Based on Uniform Convergence BoundsIn this section, we introduce two statistical tests of independence which have exact performance guarantees at finite sample sizes, based on uniform convergence bounds. The first, in Section 4.1, uses the McDiarmid (1989) bound on the biased MMD statistic, and the second, in Section 4.2, uses a Hoeffding (1963) bound for the unbiased statistic.### Bound on the Biased Statistic and TestWe establish two properties of the MMD, from which we derive a hypothesis test. First, we show that regardless of whether or not \\(p=q\\), the empirical MMD converges in probability at rate \\(O((m+n)^{-\\frac{1}{2}})\\) to its population value. This shows the consistency of statistical tests based on the MMD. Second, we give probabilistic bounds for large deviations of the empirical MMD in the case \\(p=q\\). These bounds lead directly to a threshold for our first hypothesis test. We begin our discussion of the convergence of \\(\\mbox{MMD}_{b}[\\mathcal{F},X,Y]\\) to \\(\\mbox{MMD}[\\mathcal{F},p,q]\\).**Theorem 14**: _Let \\(p,q,X,Y\\) be defined as in Problem 1, and assume \\(0\\leq k(x,y)\\leq K\\). Then_\\[\\Pr\\left\\{|\\mbox{MMD}_{b}[\\mathcal{F},X,Y]-\\mbox{MMD}[\\mathcal{F},p,q]|>2\\left( (K/m)^{\\frac{1}{2}}+(K/n)^{\\frac{1}{2}}\\right)+\\epsilon\\right\\}\\leq 2\\exp\\left( \\tfrac{-\\epsilon^{2}mn}{2K(m+n)}\\right).\\]See Appendix A.2 for proof. Our next goal is to refine this result in a way that allows us to define a test threshold under the null hypothesis \\(p=q\\). Under this circumstance, the constants in the exponent are slightly improved.**Theorem 15**: _Under the conditions of Theorem 14 where additionally \\(p=q\\) and \\(m=n\\),_\\[\\mbox{MMD}_{b}[\\mathcal{F},X,Y]\\leq\\underbrace{m^{-\\frac{1}{2}}\\sqrt{2\\mathbf{ E}_{p}\\left[k(x,x)-k(x,x^{\\prime})\\right]}}_{B_{1}(\\mathcal{F},p)}+\\epsilon\\leq \\underbrace{(2K/m)^{1/2}}_{B_{2}(\\mathcal{F},p)}+\\epsilon,\\]_both with probability at least \\(1-\\exp\\left(-\\tfrac{\\epsilon^{2}m}{4K}\\right)\\) (see Appendix A.3 for the proof)._In this theorem, we illustrate two possible bounds \\(B_{1}(\\mathcal{F},p)\\) and \\(B_{2}(\\mathcal{F},p)\\) on the bias in the empirical estimate (6). The first inequality is interesting inasmuch as it provides a link between the bias bound \\(B_{1}(\\mathcal{F},p)\\) and kernel size (for instance, if we were to use a Gaussian kernel with large \\(\\sigma\\), then \\(k(x,x)\\) and \\(k(x,x^{\\prime})\\) would likely be close, and the bias small). In the context of testing, however, we would need to provide an additional bound to show convergence of an empirical estimate of \\(B_{1}(\\mathcal{F},p)\\) to its population equivalent. Thus, in the following test for \\(p=q\\) based on Theorem 15, we use \\(B_{2}(\\mathcal{F},p)\\) to bound the bias.8Footnote 8: Note that we use a tighter bias bound than Gretton et al. (2007a).**Corollary 16**: _A hypothesis test of level \\(\\alpha\\) for the null hypothesis \\(p=q\\), that is, for \\(\\mbox{MMD}[\\mathcal{F},p,q]=0\\), has the acceptance region \\(\\mbox{MMD}_{b}[\\mathcal{F},X,Y]<\\sqrt{2K/m}\\left(1+\\sqrt{2\\log\\alpha^{-1}} \\right).\\)_We emphasise that Theorem 14 guarantees the consistency of the test, and that the Type II error probability decreases to zero at rate \\(O(m^{-\\frac{1}{2}})\\), assuming \\(m=n\\). To put this convergence rate in perspective, consider a test of whether two normal distributions have equalmeans, given they have unknown but equal variance (Casella and Berger, 2002, Exercise 8.41). In this case, the test statistic has a Student-\\(t\\) distribution with \\(n+m-2\\) degrees of freedom, and its error probability converges at the same rate as our test.It is worth noting that bounds may be obtained for the deviation between expectations \\(\\mu[p]\\) and the empirical means \\(\\mu[X]\\) in a completely analogous fashion. The proof requires symmetrization by means of a _ghost sample_, i.e. a second set of observations drawn from the same distribution. While not the key focus of the present paper, such bounds can be used in the design of inference principles based on moment matching (Altun and Smola, 2006; Dudik and Schapire, 2006; Dudik et al., 2004).### Bound on the Unbiased Statistic and TestWhile the previous bounds are of interest since the proof strategy can be used for general function classes with well behaved Rademacher averages, a much easier approach may be used directly on the unbiased statistic \\(\\mathrm{MMD}_{u}^{2}\\) in Lemma 5. We base our test on the following theorem, which is a straightforward application of the large deviation bound on U-statistics of Hoeffding (1963, p. 25).**Theorem 17**: _Assume \\(0\\leq k(x_{i},x_{j})\\leq K\\), from which it follows \\(-2K\\leq h(z_{i},z_{j})\\leq 2K\\). Then_\\[\\Pr\\left\\{\\mathrm{MMD}_{u}^{2}(\\mathcal{F},X,Y)-\\mathrm{MMD}^{2}(\\mathcal{F}, p,q)>t\\right\\}\\leq\\exp\\left(\\frac{-t^{2}m_{2}}{8K^{2}}\\right)\\]_where \\(m_{2}:=\\lfloor m/2\\rfloor\\) (the same bound applies for deviations of \\(-t\\) and below)._A consistent statistical test for \\(p=q\\) using \\(\\mathrm{MMD}_{u}^{2}\\) is then obtained.**Corollary 18**: _A hypothesis test of level \\(\\alpha\\) for the null hypothesis \\(p=q\\) has the acceptance region \\(\\mathrm{MMD}_{u}^{2}<(4K/\\sqrt{m})\\,\\sqrt{\\log(\\alpha^{-1})}\\)._We now compare the thresholds of the two tests. We note first that the threshold for the biased statistic applies to an estimate of \\(\\mathrm{MMD}\\), whereas that for the unbiased statistic is for an estimate of \\(\\mathrm{MMD}^{2}\\). Squaring the former threshold to make the two quantities comparable, the squared threshold in Corollary 16 decreases as \\(m^{-1}\\), whereas the threshold in Corollary 18 decreases as \\(m^{-1/2}\\). Thus for sufficiently large9\\(m\\), the McDiarmid-based threshold will be lower (and the associated test statistic is in any case biased upwards), and its Type II error will be better for a given Type I bound. This is confirmed in our Section 8 experiments. Note, however, that the rate of convergence of the squared, biased \\(\\mathrm{MMD}\\) estimate to its population value remains at \\(1/\\sqrt{m}\\) (bearing in mind we take the square of a biased estimate, where the bias term decays as \\(1/\\sqrt{m}\\)).Footnote 9: In the case of \\(\\alpha=0.05\\), this is \\(m\\geq 12\\).Finally, we note that the bounds we obtained here are rather conservative for a number of reasons: first, they do not take the actual distributions into account. In fact, they are finite sample size, distribution free bounds that hold even in the worst case scenario. The bounds could be tightened using localization, moments of the distribution, etc. Any such improvements could be plugged straight into Theorem 8 for a tighter bound. See e.g. Bousquet et al. (2005) for a detailed discussion of recent uniform convergence bounding methods. Second, in computing _bounds_ rather than trying to characterize the distribution of \\(\\mathrm{MMD}(\\mathcal{F},X,Y)\\) explicitly, we force our test to be conservative by design. In the following we aim for an exact characterization of the asymptotic distribution of \\(\\mathrm{MMD}(\\mathcal{F},X,Y)\\) instead of a bound. While this will not satisfy the uniform convergence requirements, it leads to superior tests in practice.## 5 Test Based on the Asymptotic Distribution of the Unbiased StatisticWe now propose a third test, which is based on the asymptotic distribution of the unbiased estimate of \\(\\mathrm{MMD}^{2}\\) in Lemma 5.**Theorem 19**: _We assume \\(\\mathbf{E}\\left(h^{2}\\right)<\\infty\\). Under \\(\\mathcal{H}_{1}\\), \\(\\mathrm{MMD}^{2}_{u}\\) converges in distribution (see e.g. Grimmet and Stirzaker, 2001, Section 7.2) to a Gaussian according to_\\[m^{\\frac{1}{2}}\\left(\\mathrm{MMD}^{2}_{u}-\\mathrm{MMD}^{2}\\left[\\mathcal{F},p,q \\right]\\right)\\overset{D}{\\rightarrow}\\mathcal{N}\\left(0,\\sigma^{2}_{u} \\right),\\]_where \\(\\sigma^{2}_{u}=4\\left(\\mathbf{E}_{z}\\left[(\\mathbf{E}_{z^{\\prime}}h(z,z^{ \\prime}))^{2}\\right]-\\left[\\mathbf{E}_{z,z^{\\prime}}(h(z,z^{\\prime}))\\right]^ {2}\\right)\\), uniformly at rate \\(1/\\sqrt{m}\\) (Serfling, 1980, Theorem B, p. 193). Under \\(\\mathcal{H}_{0}\\), the U-statistic is degenerate, meaning \\(\\mathbf{E}_{z^{\\prime}}h(z,z^{\\prime})=0\\). In this case, \\(\\mathrm{MMD}^{2}_{u}\\) converges in distribution according to_\\[m\\mathrm{MMD}^{2}_{u}\\overset{D}{\\rightarrow}\\sum_{l=1}^{\\infty}\\lambda_{l} \\left[z_{l}^{2}-2\\right], \\tag{10}\\]_where \\(z_{l}\\sim\\mathcal{N}(0,2)\\) i.i.d., \\(\\lambda_{i}\\) are the solutions to the eigenvalue equation_\\[\\int_{\\mathcal{X}}\\tilde{k}(x,x^{\\prime})\\psi_{i}(x)dp(x)=\\lambda_{i}\\psi_{i}( x^{\\prime}),\\]_and \\(\\tilde{k}(x_{i},x_{j}):=k(x_{i},x_{j})-\\mathbf{E}_{x}k(x_{i},x)-\\mathbf{E}_{x}k (x,x_{j})+\\mathbf{E}_{x,x^{\\prime}}k(x,x^{\\prime})\\) is the centred RKHS kernel._The asymptotic distribution of the test statistic under \\(\\mathcal{H}_{1}\\) is given by Serfling (1980, Section 5.5.1), and the distribution under \\(\\mathcal{H}_{0}\\) follows Serfling (1980, Section 5.5.2) and Anderson et al. (1994, Appendix); see Appendix B.1 for details. We illustrate the MMD density under both the null and alternative hypotheses by approximating it empirically for both \\(p=q\\) and \\(p\eq q\\). Results are plotted in Figure 2.Our goal is to determine whether the empirical test statistic \\(\\mathrm{MMD}^{2}_{u}\\) is so large as to be outside the \\(1-\\alpha\\) quantile of the null distribution in (10) (consistency of the resulting test is guaranteed by the form of the distribution under \\(\\mathcal{H}_{1}\\)). One way to estimate this quantile is using the bootstrap on the aggregated data, following Arcones and Gine (1992). Alternatively, we may approximate the null distribution by fitting Pearson curves to its first four moments (Johnson et al., 1994, Section 18.8). Taking advantage of the degeneracy of the U-statistic, we obtain (see Appendix B.2)\\[\\mathbf{E}\\left(\\left[\\mathrm{MMD}^{2}_{u}\\right]^{2}\\right) =\\frac{2}{m(m-1)}\\mathbf{E}_{z,z^{\\prime}}\\left[h^{2}(z,z^{\\prime })\\right]\\text{ and } \\tag{11}\\] \\[\\mathbf{E}\\left(\\left[\\mathrm{MMD}^{2}_{u}\\right]^{3}\\right) =\\frac{8(m-2)}{m^{2}(m-1)^{2}}\\mathbf{E}_{z,z^{\\prime}}\\left[h(z,z^{\\prime})\\mathbf{E}_{z^{\\prime\\prime}}\\left(h(z,z^{\\prime\\prime})h(z^{ \\prime},z^{\\prime\\prime})\\right)\\right]+O(m^{-4}). \\tag{12}\\]The fourth moment \\(\\mathbf{E}\\left(\\left[\\mathrm{MMD}_{u}^{2}\\right]^{4}\\right)\\) is not computed, since it is both very small, \\(O(m^{-4})\\), and expensive to calculate, \\(O(m^{4})\\). Instead, we replace the kurtosis10 with a lower bound due to Wilkins (1944), \\(\\mathrm{kurt}\\left(\\mathrm{MMD}_{u}^{2}\\right)\\geq\\left(\\mathrm{skew}\\left( \\mathrm{MMD}_{u}^{2}\\right)\\right)^{2}+1\\).Footnote 10: The kurtosis is defined in terms of the fourth and second moments as \\(\\mathrm{kurt}\\left(\\mathrm{MMD}_{u}^{2}\\right)=\\frac{\\mathbf{E}\\left(\\left[ \\mathrm{MMD}_{u}^{2}\\right]^{4}\\right)}{\\left[\\mathbf{E}\\left(\\left[\\mathrm{ MMD}_{u}^{2}\\right]^{2}\\right)\\right]^{2}}-3\\).Note that \\(\\mathrm{MMD}_{u}^{2}\\) may be negative, since it is an unbiased estimator of \\(\\left(\\mathrm{MMD}[\\mathcal{F},p,q]\\right)^{2}\\). However, the only terms missing to ensure nonnegativity are the terms \\(h(z_{i},z_{i})\\), which were removed to remove spurious correlations between observations. Consequently we have the bound\\[\\mathrm{MMD}_{u}^{2}+\\frac{1}{m(m-1)}\\sum_{i=1}^{m}k(x_{i},x_{i})+k(y_{i},y_{i })-2k(x_{i},y_{i})\\geq 0. \\tag{13}\\]## 6 A Linear Time Statistic and TestWhile the above tests are already more efficient than the \\(O(m^{2}\\log m)\\) and \\(O(m^{3})\\) tests described earlier, it is still desirable to obtain \\(O(m)\\) tests which do not sacrifice too much statistical power. Moreover, we would like to obtain tests which have \\(O(1)\\) storage requirements for computing the test statistic in order to apply it to data streams. We now describe how to achieve this by computing the test statistic based on a subsampling of the terms in the sum. The empirical estimate in this case is obtained by drawing pairs from \\(X\\) and \\(Y\\) respectively _without_ replacement.**Lemma 20**: _Recall \\(m_{2}:=\\lfloor m/2\\rfloor\\). The estimator_\\[\\mathrm{MMD}^{2}_{l}[\\mathcal{F},X,Y]:=\\frac{1}{m_{2}}\\sum_{i=1}^{m_{2}}h((x_{2i- 1},y_{2i-1}),(x_{2i},y_{2i}))\\]_can be computed in linear time. Moreover, it is an unbiased estimate of \\(\\mathrm{MMD}^{2}[\\mathcal{F},p,q]\\)._While it is expected (as we will see explicitly later) that \\(\\mathrm{MMD}^{2}_{l}\\) has higher variance than \\(\\mathrm{MMD}^{2}_{u}\\), it is computationally much more appealing. In particular, the statistic can be used in stream computations with need for only \\(O(1)\\) memory, whereas \\(\\mathrm{MMD}^{2}_{u}\\) requires \\(O(m)\\) storage and \\(O(m^{2})\\) time to compute the kernel \\(h\\) on all interacting pairs.Since \\(\\mathrm{MMD}^{2}_{l}\\) is just the average over a set of random variables, Hoeffding's bound and the central limit theorem readily allow us to provide both uniform convergence and asymptotic statements for it with little effort. The first follows directly from Hoeffding (1963, Theorem 2).**Theorem 21**: _Assume \\(0\\leq k(x_{i},x_{j})\\leq K\\). Then_\\[\\Pr\\left\\{\\mathrm{MMD}^{2}_{l}(\\mathcal{F},X,Y)-\\mathrm{MMD}^{2}(\\mathcal{F}, p,q)>t\\right\\}\\leq\\exp\\left(\\frac{-t^{2}m_{2}}{8K^{2}}\\right)\\]_where \\(m_{2}:=\\lfloor m/2\\rfloor\\) (the same bound applies for deviations of \\(-t\\) and below)._Note that the bound of Theorem 17 is identical to that of Theorem 21, which shows the former is rather loose. Next we invoke the central limit theorem.**Corollary 22**: _Assume \\(0<\\mathbf{E}\\left(h^{2}\\right)<\\infty\\). Then \\(\\mathrm{MMD}^{2}_{l}\\) converges in distribution to a Gaussian according to_\\[m^{\\frac{1}{2}}\\left(\\mathrm{MMD}^{2}_{l}-\\mathrm{MMD}^{2}\\left[\\mathcal{F},p,q \\right]\\right)\\overset{D}{\\rightarrow}\\mathcal{N}\\left(0,\\sigma^{2}_{l}\\right),\\]_where \\(\\sigma^{2}_{l}=2\\left[\\mathbf{E}_{z,z^{\\prime}}h^{2}(z,z^{\\prime})-\\left[ \\mathbf{E}_{z,z^{\\prime}}h(z,z^{\\prime})\\right]^{2}\\right]\\), uniformly at rate \\(1/\\sqrt{m}\\)._The factor of \\(2\\) arises since we are averaging over only \\(\\lfloor m/2\\rfloor\\) observations. Note the difference in the variance between Theorem 19 and Corollary 22, namely in the former case we are interested in the average conditional variance \\(\\mathbf{E}_{z}\\mathrm{Var}_{z^{\\prime}}[h(z,z^{\\prime})|z]\\), whereas in the latter case we compute the full variance \\(\\mathrm{Var}_{z,z^{\\prime}}[h(z,z^{\\prime})]\\).We end by noting another potential approach to reducing the computational cost of the MMD, by computing a low rank approximation to the Gram matrix (Fine and Scheinberg, 2001; Williams and Seeger, 2001; Smola and Scholkopf, 2000). An incremental computation of the MMD based on such a low rank approximation would require \\(O(md)\\) storage and \\(O(md)\\) computation (where \\(d\\) is the rank of the approximate Gram matrix which is used to factorize _both_ matrices) rather than \\(O(m)\\) storage and \\(O(m^{2})\\) operations. That said, it remains to be determined what effect this approximation would have on the distribution of the test statistic under \\(\\mathcal{H}_{0}\\), and hence on the test threshold.## 7 Similarity Measures Related to MMDOur main point is to propose a new kernel statistic to test whether two distributions are the same. However, it is reassuring to observe links to other measures of similarity between distributions.### Link with \\(L_{2}\\) Distance between Parzen Window EstimatesIn this section, we demonstrate the connection between our test statistic and the Parzen window-based statistic of Anderson et al. (1994). We show that a two-sample test based on Parzen windows converges more slowly than an RKHS-based test, also following Anderson et al. (1994). Before proceeding, we motivate this discussion with a short overview of the Parzen window estimate and its properties (Silverman, 1986). We assume a distribution \\(p\\) on \\(\\mathbb{R}^{d}\\), which has an associated density function also written \\(p\\) to minimise notation. The Parzen window estimate of this density from an i.i.d. sample \\(X\\) of size \\(m\\) is\\[\\hat{p}(x)=\\frac{1}{m}\\sum_{l=1}^{m}\\kappa\\left(x_{l}-x\\right)\\text{ where } \\kappa\\text{ satisfies }\\int_{\\mathcal{X}}\\kappa\\left(x\\right)dx=1\\text{ and }\\kappa \\left(x\\right)\\geq 0.\\]We may rescale \\(\\kappa\\) according to \\(\\frac{1}{h_{m}^{d}}\\kappa\\left(\\frac{x}{h_{m}}\\right)\\). Consistency of the Parzen window estimate requires\\[\\lim_{m\\to\\infty}h_{m}^{d}=0\\quad\\text{and}\\quad\\lim_{m\\to\\infty}mh_{m}^{d}=\\infty. \\tag{14}\\]We now show that the \\(L_{2}\\) distance between Parzen windows density estimates (Anderson et al., 1994) is a special case of the biased MMD in equation (6). Denote by \\(D_{r}(p,q):=\\left\\|p-q\\right\\|_{r}\\) the \\(L_{r}\\) distance. For \\(r=1\\) the distance \\(D_{r}(p,q)\\) is known as the Levy distance (Feller, 1971), and for \\(r=2\\) we encounter distance measures derived from the Renyi entropy (Gokcay and Principe, 2002).Assume that \\(\\hat{p}\\) and \\(\\hat{q}\\) are given as kernel density estimates with kernel \\(\\kappa(x-x^{\\prime})\\), that is, \\(\\hat{p}(x)=m^{-1}\\sum_{i}\\kappa(x_{i}-x)\\) and \\(\\hat{q}(y)\\) is defined by analogy. In this case\\[D_{2}(\\hat{p},\\hat{q})^{2} =\\int\\left[\\frac{1}{m}\\sum_{i}\\kappa(x_{i}-z)-\\frac{1}{n}\\sum_{i} \\kappa(y_{i}-z)\\right]^{2}dz \\tag{15}\\] \\[=\\frac{1}{m^{2}}\\sum_{i,j=1}^{m}k(x_{i}-x_{j})+\\frac{1}{n^{2}} \\sum_{i,j=1}^{n}k(y_{i}-y_{j})-\\frac{2}{mn}\\sum_{i,j=1}^{m,n}k(x_{i}-y_{j}), \\tag{16}\\]where \\(k(x-y)=\\int\\kappa(x-z)\\kappa(y-z)dz\\). By its definition \\(k(x-y)\\) is a Mercer kernel (Mercer, 1909), as it can be viewed as inner product between \\(\\kappa(x-z)\\) and \\(\\kappa(y-z)\\) on the domain \\(\\mathcal{X}\\).A disadvantage of the Parzen window interpretation is that when the Parzen window estimates are consistent (which requires the kernel size to decrease with increasing sample size), the resulting two-sample test converges more slowly than using fixed kernels. According to Anderson et al. (1994, p. 43), the Type II error of the two-sample test converges as \\(m^{-1/2}h_{m}^{-d/2}\\). Thus, given the schedule for the Parzen window size decrease in (14), the convergence rate will lie in the open interval \\((0,1/2)\\): the upper limit is approached asdecreases more slowly, and the lower limit corresponds to \\(h_{m}\\) decreasing near the upper bound of \\(1/m\\). In other words, by avoiding density estimation, we obtain a better convergence rate (namely \\(m^{-1/2}\\)) than using a Parzen window estimate with _any_ permissible bandwidth decrease schedule. In addition, the Parzen window interpretation cannot explain the excellent performance of MMD based tests in experimental settings where the dimensionality greatly exceeds the sample size (for instance the Gaussian toy example in Figure 4B, for which performance actually improves when the dimensionality increases; and the microarray datasets in Table 1). Finally, our tests are able to employ universal kernels that cannot be written as inner products between Parzen windows, normalized or otherwise: several examples are given by Steinwart (2001, Section 3) and Micchelli et al. (2006, Section 3). We may further generalize to kernels on structured objects such as strings and graphs (Scholkopf et al., 2004): see also our experiments in Section 8.### Set Kernels and Kernels Between Probability MeasuresGartner et al. (2002) propose kernels to deal with sets of observations. These are then used in the context of Multi-Instance Classification (MIC). The problem MIC attempts to solve is to find estimators which are able to infer from the fact that some elements in the set satisfy a certain property, then the set of observations has this property, too. For instance, a dish of mushrooms is poisonous if it contains poisonous mushrooms. Likewise a keyring will open a door if it contains a suitable key. One is only given the ensemble, however, rather than information about which instance of the set satisfies the property.The solution proposed by Gartner et al. (2002) is to map the ensembles \\(X_{i}:=\\{x_{i1},\\ldots,x_{im_{i}}\\}\\), where \\(i\\) is the ensemble index and \\(m_{i}\\) the number of elements in the \\(i\\)th ensemble, jointly into feature space via\\[\\phi(X_{i}):=\\frac{1}{m_{i}}\\sum_{j=1}^{m_{i}}\\phi(x_{ij}), \\tag{17}\\]and use the latter as the basis for a kernel method. This simple approach affords rather good performance. With the benefit of hindsight, it is now understandable why the kernel\\[k(X_{i},X_{j})=\\frac{1}{m_{i}m_{j}}\\sum_{u,v}^{m_{i},m_{j}}k(x_{iu},x_{jv}) \\tag{18}\\]produces useful results: it is simply the kernel between the empirical means in feature space \\(\\langle\\mu(X_{i}),\\mu(X_{j})\\rangle\\)(Hein et al., 2004, Eq. 4). Jebara and Kondor (2003) later extended this setting by smoothing the empirical densities before computing inner products.Note, however, that property testing for distributions is probably not optimal when using the mean \\(\\mu[p]\\) (or \\(\\mu[X]\\) respectively): we are only interested in determining whether _some_ instances in the domain have the desired property, rather than making a statement regarding the distribution of those instances. Taking this into account leads to an improved algorithm (Andrews et al., 2003).### Kernel Measures of IndependenceWe next demonstrate the application of MMD in determining whether two random variables \\(x\\) and \\(y\\) are independent. In other words, assume that pairs of random variables \\((x_{i},y_{i})\\)are jointly drawn from some distribution \\(p:=\\Pr_{x,y}\\). We wish to determine whether this distribution factorizes, i.e. whether \\(q:=\\Pr_{x}\\Pr_{y}\\) is the same as \\(p\\). One application of such an independence measure is in independent component analysis (Comon, 1994), where the goal is to find a linear mapping of the observations \\(x_{i}\\) to obtain mutually independent outputs. Kernel methods were employed to solve this problem by Bach and Jordan (2002); Gretton et al. (2005a,b). In the following we re-derive one of the above kernel independence measures using mean operators instead.We begin by defining\\[\\mu[\\Pr_{xy}] :=\\mathbf{E}_{x,y}\\left[v((x,y),\\cdot)\\right]\\] \\[\\text{and }\\mu[\\Pr_{x}\\mathop{\\times}\\Pr_{y}] :=\\mathbf{E}_{x}\\mathbf{E}_{y}\\left[v((x,y),\\cdot)\\right].\\]Here we assumed that \\(\\mathcal{V}\\) is an RKHS over \\(\\mathcal{X}\\times\\mathcal{Y}\\) with kernel \\(v((x,y),(x^{\\prime},y^{\\prime}))\\). If \\(x\\) and \\(y\\)_are_ dependent, the equality \\(\\mu[\\Pr_{xy}]=\\mu[\\Pr_{x}\\mathop{\\times}\\Pr_{y}]\\) will not hold. Hence we may use \\(\\Delta:=\\|\\mu[\\Pr_{xy}]-\\mu[\\Pr_{x}\\mathop{\\times}\\Pr_{y}]\\|\\) as a measure of dependence.Now assume that \\(v((x,y),(x^{\\prime},y^{\\prime}))=k(x,x^{\\prime})l(y,y^{\\prime})\\), i.e. that the RKHS \\(\\mathcal{V}\\) is a direct product \\(\\mathcal{H}\\otimes\\mathcal{G}\\) of the RKHSs on \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). In this case it is easy to see that\\[\\Delta^{2} = \\left\\|\\mathbf{E}_{xy}\\left[k(x,\\cdot)l(y,\\cdot)\\right]-\\mathbf{E} _{x}\\left[k(x,\\cdot)\\right]\\mathbf{E}_{y}\\left[l(y,\\cdot)\\right]\\right\\|^{2}\\] \\[= \\mathbf{E}_{xy}\\mathbf{E}_{x^{\\prime}y^{\\prime}}\\left[k(x,x^{ \\prime})l(y,y^{\\prime})\\right]-2\\mathbf{E}_{x}\\mathbf{E}_{y}\\mathbf{E}_{x^{ \\prime}y^{\\prime}}\\left[k(x,x^{\\prime})l(y,y^{\\prime})\\right]\\] \\[+\\mathbf{E}_{x}\\mathbf{E}_{y}\\mathbf{E}_{x^{\\prime}}\\mathbf{E}_{ y^{\\prime}}\\left[k(x,x^{\\prime})l(y,y^{\\prime})\\right]\\]The latter, however, is exactly what Gretton et al. (2005a) show to be the Hilbert-Schmidt norm of the cross-covariance operator between RKHSs: this is zero if and only if \\(x\\) and \\(y\\) are independent, for universal kernels. We have the following theorem:**Theorem 23**: _Denote by \\(C_{xy}\\) the covariance operator between random variables \\(x\\) and \\(y\\), drawn jointly from \\(\\Pr_{xy}\\), where the functions on \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) are the reproducing kernel Hilbert spaces \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\) respectively. Then the Hilbert-Schmidt norm \\(\\left\\|C_{xy}\\right\\|_{\\mathrm{HS}}\\) equals \\(\\Delta\\)._Empirical estimates of this quantity are as follows:**Theorem 24**: _Denote by \\(K\\) and \\(L\\) the kernel matrices on \\(X\\) and \\(Y\\) respectively, and by \\(H=I-\\mathbf{1}/m\\) the projection matrix onto the subspace orthogonal to the vector with all entries set to \\(1\\). Then \\(m^{-2}\\operatorname{tr}HKHL\\) is an estimate of \\(\\Delta^{2}\\) with bias \\(O(m^{-1})\\). With high probability the deviation from \\(\\Delta^{2}\\) is \\(O(m^{-\\frac{1}{2}})\\)._Gretton et al. (2005a) provide explicit constants. In certain circumstances, including in the case of RKHSs with Gaussian kernels, the empirical \\(\\Delta^{2}\\) may also be interpreted in terms of a smoothed difference between the joint empirical characteristic function (ECF) and the product of the marginal ECFs (Feuerverger, 1993; Kankainen, 1995). This interpretation does not hold in all cases, however, e.g. for kernels on strings, graphs, and other structured spaces. An illustration of the witness function \\(f\\in\\mathcal{F}\\) from Definition 2 is provided in Figure 3. This is a smooth function which has large magnitude where the joint density is most different from the product of the marginals.We remark that a hypothesis test based on the above kernel statistic is more complicated than for the two-sample problem, since the product of the marginal distributions is in effect simulated by permuting the variables of the original sample. Further details are provided by Gretton et al. (2008).Figure 3: Illustration of the function maximizing the mean discrepancy when MMD is used as a measure of independence. A sample from dependent random variables \\(x\\) and \\(y\\) is shown in black, and the associated function \\(f\\) that witnesses the MMD is plotted as a contour. The latter was computed empirically on the basis of 200 samples, using a Gaussian kernel with \\(\\sigma=0.2\\).### Kernel Statistics Using a Distribution over Witness FunctionsShawe-Taylor and Dolia (2007) define a distance between distributions as follows: let \\(\\mathcal{H}\\) be a set of functions on \\(\\mathcal{X}\\) and \\(r\\) be a probability distribution over \\(\\mathcal{F}\\). Then the distance between two distributions \\(p\\) and \\(q\\) is given by\\[D(p,q):=\\mathbf{E}_{f\\sim r(f)}\\left|\\mathbf{E}_{x\\sim p}[f(x)]-\\mathbf{E}_{x \\sim q}[f(x)]\\right|. \\tag{19}\\]That is, we compute the average distance between \\(p\\) and \\(q\\) with respect to a distribution of test functions.**Lemma 25**: _Let \\(\\mathcal{H}\\) be a reproducing kernel Hilbert space, \\(f\\in\\mathcal{H}\\), and assume \\(r(f)=r(\\|f\\|_{\\mathcal{H}})\\) with finite \\(\\mathbf{E}_{f\\sim r}[\\|f\\|_{\\mathcal{H}}]\\). Then \\(D(p,q)=C\\left\\|\\mu[p]-\\mu[q]\\right\\|_{\\mathcal{H}}\\) for some constant \\(C\\) which depends only on \\(\\mathcal{H}\\) and \\(r\\)._**Proof** By definition \\(\\mathbf{E}_{p}[f(x)]=\\left\\langle\\mu[p],f\\right\\rangle_{\\mathcal{H}}\\). Using linearity of the inner product, Equation (19) equals\\[\\int\\left|\\left\\langle\\mu[p]-\\mu[q],f\\right\\rangle_{\\mathcal{H}} \\right|\\mathrm{d}r(f)\\] \\[= \\left\\|\\mu[p]-\\mu[q]\\right\\|_{\\mathcal{H}}\\int\\left|\\left\\langle \\frac{\\mu[p]-\\mu[q]}{\\|\\mu[p]-\\mu[q]\\|_{\\mathcal{H}}},f\\right\\rangle_{\\mathcal{ H}}\\right|\\mathrm{d}r(f),\\]where the integral is independent of \\(p,q\\). To see this, note that for any \\(p,q\\), \\(\\frac{\\mu[p]-\\mu[q]}{\\|\\mu[p]-\\mu[q]\\|_{\\mathcal{H}}}\\) is a unit vector which can turned into, say, the first canonical basis vector by a rotation which leaves the integral invariant, bearing in mind that \\(r\\) is rotation invariant. \\(\\blacksquare\\)### Outlier DetectionAn application related to the two sample problem is that of outlier detection: this is the question of whether a novel point is generated from the same distribution as a particular i.i.d. sample. In a way, this is a special case of a two sample test, where the second sample contains only one observation. Several methods essentially rely on the distance between a novel point to the sample mean in feature space to detect outliers.For instance, Davy et al. (2002) use a related method to deal with nonstationary time series. Likewise Shawe-Taylor and Cristianini (2004, p. 117) discuss how to detect novel observations by using the following reasoning: the probability of being an outlier is bounded both as a function of the spread of the points in feature space and the uncertainty in the empirical feature space mean (as bounded using symmetrisation and McDiarmid's tail bound).Instead of using the sample mean and variance, Tax and Duin (1999) estimate the center and radius of a minimal enclosing sphere for the data, the advantage being that such bounds can potentially lead to more reliable tests for _single_ observations. Scholkopf et al. (2001) show that the minimal enclosing sphere problem is equivalent to novelty detection by means of finding a hyperplane separating the data from the origin, at least in the case of radial basis function kernels.## 8 ExperimentsWe conducted distribution comparisons using our MMD-based tests on datasets from three real-world domains: database applications, bioinformatics, and neurobiology. We investigated both uniform convergence approaches (MMD\\({}_{b}\\) with the Corollary 16 threshold, and MMD\\({}_{u}^{2}\\) H with the Corollary 18 threshold); the asymptotic approaches with bootstrap (MMD\\({}_{u}^{2}\\) B) and moment matching to Pearson curves (MMD\\({}_{u}^{2}\\) M), both described in Section 5; and the asymptotic approach using the linear time statistic (MMD\\({}_{l}^{2}\\)) from Section 6. We also compared against several alternatives from the literature (where applicable): the multivariate t-test, the Friedman-Rafsky Kolmogorov-Smirnov generalisation _(Smir)_, the Friedman-Rafsky Wald-Wolfowitz generalisation _(Wolf)_, the Biau-Gyorfi test _(Biau)_, and the Hall-Tajvidi test _(Hall)_. See Section 3.3 for details regarding these tests. Note that we do not apply the Biau-Gyorfi test to high-dimensional problems (since the required space partitioning is no longer possible), and that MMD is the only method applicable to structured data such as graphs.An important issue in the practical application of the MMD-based tests is the selection of the kernel parameters. We illustrate this with a Gaussian RBF kernel, where we must choose the kernel width \\(\\sigma\\) (we use this kernel for univariate and multivariate data, but not for graphs). The empirical MMD is zero both for kernel size \\(\\sigma=0\\) (where the aggregate Gram matrix over \\(X\\) and \\(Y\\) is a unit matrix), and also approaches zero as \\(\\sigma\\rightarrow\\infty\\) (where the aggregate Gram matrix becomes uniformly constant). We set \\(\\sigma\\) to be the median distance between points in the aggregate sample, as a compromise between these two extremes: this remains a heuristic, similar to those described in Takeuchi et al. (2006); Scholkopf (1997), and the optimum choice of kernel size is an ongoing area of research.### Toy Example: Two GaussiansIn our first experiment, we investigated the scaling performance of the various tests as a function of the dimensionality \\(d\\) of the space \\(\\mathcal{X}\\subset\\mathbb{R}^{d}\\), when both \\(p\\) and \\(q\\) were Gaussian. We considered values of \\(d\\) up to 2500: the performance of the MMD-based tests cannot therefore be explained in the context of density estimation (as in Section 7.1), since the associated density estimates are necessarily meaningless here. The levels for all tests were set at \\(\\alpha=0.05\\), \\(m=250\\) samples were used, and results were averaged over 100 repetitions. In the first case, the distributions had different means and unit variance. The percentage of times the null hypothesis was correctly rejected over a set of Euclidean distances between the distribution means (20 values logarithmically spaced from 0.05 to 50), was computed as a function of the dimensionality of the normal distributions. In case of the t-test, a ridge was added to the covariance estimate, to avoid singularity (the ratio of largest to smallest eigenvalue was ensured to be at most 2). In the second case, samples were drawn from distributions \\(\\mathcal{N}(0,\\mathbf{I})\\) and \\(\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})\\) with different variance. The percentage of null rejections was averaged over 20 \\(\\sigma\\) values logarithmically spaced from \\(10^{0.01}\\) to 10. The t-test was not compared in this case, since its output would have been irrelevant. Results are plotted in Figure 4.In the case of Gaussians with differing means, we observe the t-test performs best in low dimensions, however its performance is severely weakened when the number of samples exceeds the number of dimensions. The performance of \\(MMD_{u}^{2}\\) M is comparable to the t-testfor low sample sizes, and outperforms all other methods for larger sample sizes. The worst performance is obtained for \\(MMD_{u}^{2}\\) H, though \\(MMD_{b}\\) also does relatively poorly: this is unsurprising given that these tests derive from distribution-free large deviation bounds, whereas the sample size is relatively small. Remarkably, \\(MMD_{l}^{2}\\) performs quite well compared with classical tests in high dimensions.In the case of Gaussians of differing variance, the _Hall_ test performs best, followed closely by \\(MMD_{u}^{2}\\). _FR Wolf_ and (to a much greater extent) _FR Smirnov_ both have difficulties in high dimensions, failing completely once the dimensionality becomes too great. The linear test \\(MMD_{l}^{2}\\) again performs surprisingly well, almost matching the \\(MMD_{u}^{2}\\) performance in the highest dimensionality. Both \\(MMD_{u}^{2}\\)H and \\(MMD_{b}\\) perform poorly, the former failing completely: this is one of several illustrations we will encounter of the much greater tightness of the Corollary 16 threshold over that in Corollary 18.### Data IntegrationIn our next application of MMD, we performed distribution testing for data integration: the objective being to aggregate two datasets into a single sample, with the understanding that both original samples were generated from the same distribution. Clearly, it is important to check this last condition before proceeding, or an analysis could detect patterns in the new dataset that are caused by combining the two different source distributions, and not by real-world phenomena. We chose several real-world settings to perform this task: we compared microarray data from normal and tumor tissues (Health status), microarray data from different subtypes of cancer (Subtype), and local field potential (LFP) electrode recordings from the Macaque primary visual cortex (V1) with and without spike events (Neural Data I and II, as described in more detail by Rasch et al., 2008). In all cases, the two data sets have different statistical properties, but the detection of these differences is made difficult by theFigure 4: Type II performance of the various tests when separating two Gaussians, with test level \\(\\alpha=0.05\\). **A** Gaussians have same variance and different means. **B** Gaussians have same mean and different variances.high data dimensionality (indeed, for the microarray data, density estimation is impossible given the sample size and data dimensionality, and no successful test can rely on accurate density estimates as an intermediate step).We applied our tests to these datasets in the following fashion. Given two datasets A and B, we either chose one sample from A and the other from B _(attributes = different)_; or both samples from either A or B _(attributes = same)_. We then repeated this process up to 1200 times. Results are reported in Table 1. Our asymptotic tests perform better than all competitors besides _Wolf_: in the latter case, we have greater Type II error for one neural dataset, lower Type II error on the Health Status data (which has very high dimension and low sample size), and identical (error-free) performance on the remaining examples. We note that the Type I error of the bootstrap test on the Subtype dataset is far from its design value of 0.05, indicating that the Pearson curves provide a better threshold estimate for these low sample sizes. For the remaining datasets, the Type I errors of the Pearson and Bootstrap approximations are close. Thus, for larger datasets, the bootstrap is to be preferred, since it costs \\(O(m^{2})\\), compared with a cost of \\(O(m^{3})\\) for Pearson (due to the cost of computing (12)). Finally, the uniform convergence-based tests are too conservative, with \\(\\text{MMD}_{b}\\) finding differences in distribution only for the data with largest sample size, and \\(\\text{MMD}_{u}^{2}\\) H never finding differences.### Computational CostWe next investigate the tradeoff between computational cost and performance of the various tests, with particular attention to how the quadratic time MMD tests from Sections 4 and 5 compare with the linear time MMD-based asymptotic test from Section 6. We consider two 1-D datasets (CNUM and FOREST) and two higher-dimensional datasets (FOREST10D and NEUROII). Results are plotted in Figure 5. If cost is not a factor, then the \\(\\text{MMD}_{u}^{2}\\) B shows best overall performance as a function of sample size, with a Type II error dropping to zero as fast or faster than competing approaches in three of four cases, and narrowly\\begin{table}\\begin{tabular}{|l|l|r|r|r|r|r|r|r|} \\hline Dataset & Attr. & \\(\\text{MMD}_{b}\\) & \\(\\text{MMD}_{u}^{2}\\) H & \\(\\text{MMD}_{u}^{2}\\) B & \\(\\text{MMD}_{u}^{2}\\) M & t-test & Wolf & Smir & Hall \\\\ \\hline \\hline Neural Data I & Same & 100.0 & 100.0 & 96.5 & 96.5 & 100.0 & 97.0 & 95.0 & 96.0 \\\\ \\hline  & Different & 38.0 & 100.0 & **0.0** & **0.0** & 42.0 & **0.0** & 10.0 & 49.0 \\\\ \\hline \\hline Neural Data II & Same & 100.0 & 100.0 & 94.6 & 95.2 & 100.0 & 95.0 & 94.5 & 96.0 \\\\ \\hline  & Different & 99.7 & 100.0 & 3.3 & 3.4 & 100.0 & **0.8** & 31.8 & 5.9 \\\\ \\hline \\hline Health status & Same & 100.0 & 100.0 & 95.5 & 94.4 & 100.0 & 94.7 & 96.1 & 95.6 \\\\ \\hline  & Different & 100.0 & 100.0 & 1.0 & **0.8** & 100.0 & 2.8 & 44.0 & 35.7 \\\\ \\hline \\hline Subtype & Same & 100.0 & 100.0 & 99.1 & 96.4 & 100.0 & 94.6 & 97.3 & 96.5 \\\\ \\hline  & Different & 100.0 & 100.0 & **0.0** & **0.0** & 100.0 & **0.0** & 28.4 & 0.2 \\\\ \\hline \\end{tabular}\\end{table}Table 1: Distribution testing for data integration on multivariate data. Numbers indicate the percentage of repetitions for which the null hypothesis (p=q) was accepted, given \\(\\alpha=0.05\\). Sample size (dimension; repetitions of experiment): Neural I 4000 (63; 100) ; Neural II 1000 (100; 1200); Health Status 25 (12,600; 1000); Subtype 25 (2,118; 1000).trailing _FR Wolf_ in the fourth (FOREST10D). That said, for datasets CNUM, FOREST, and FOREST10D, the linear time MMD achieves results comparable to \\(\\text{MMD}_{u}^{2}\\) B at a far smaller computational cost, albeit by looking at a great deal more data. In the CNUM case, however, the linear test is not able to achieve zero error even for the largest data set size. For the NEUROII data, attaining zero Type II error has about the same cost for both approaches. The difference in cost of \\(\\text{MMD}_{u}^{2}\\) B and \\(\\text{MMD}_{b}\\) is due to the bootstrapping required for the former, which produces a constant offset in cost between the two (here 150 resamplings were used).The \\(t\\)-test also performs well in three of the four problems, and in fact represents the best cost-performance tradeoff in these three datasets (i.e. while it requires much more data than \\(\\text{MMD}_{u}^{2}\\) B for a given level of performance, it costs far less to compute). The \\(t\\)-test assumes that only the difference in means is important in distinguishing the distributions, and it requires an accurate estimate of the within-sample covariance; the test fails completely on the NEUROII data. We emphasise that the Kolmogorov-Smirnov results in 1-D were obtained using the classical statistic, and not the Friedman-Rafsky statistic, hence the low computational cost. The cost of both Friedman-Rafsky statistics is therefore given by the _FR Wolf_ cost in this case. The latter scales similarly with sample size to the quadratic time MMD tests, confirming Friedman and Rafsky's observation that obtaining the pairwise distances between sample points is the dominant cost of their tests. We also remark on the unusual behaviour of the Type II error of the _FR Wolf_ test in the FOREST dataset, which worsens for increasing sample size.We conclude that the approach to be recommended when testing homogeneity will depend on the data available: for small amounts of data, the best results are obtained using every observation to maximum effect, and employing the quadratic time \\(\\text{MMD}_{u}^{2}\\) B test. When large volumes of data are available, a better option is to look at each point only once, which can yield greater accuracy for a given computational cost. It may also be worth doing a t-test first in this case, and only running more sophisticated non-parametric tests if the t-test accepts the null hypothesis, to verify the distributions are identical in more than just mean.### Attribute MatchingOur final series of experiments addresses automatic attribute matching. Given two databases, we want to detect corresponding attributes in the schemas of these databases, based on their data-content (as a simple example, two databases might have respective fields Wage and Salary, which are assumed to be observed via a subsampling of a particular population, and we wish to automatically determine that both Wage and Salary denote to the same underlying attribute). We use a two-sample test on pairs of attributes from two databases to find corresponding pairs.11 This procedure is also called _table matching_ for tables from different databases. We performed attribute matching as follows: first, the dataset D was split into two halves A and B. Each of the \\(n\\) attributes in A (and B, resp.) was then represented by its instances in A (resp. B). We then tested all pairs of attributes from A and Figure 5: Linear vs quadratic MMD. First column is performance, second is runtime. The dashed grey horizontal line indicates zero Type II error (required due log y-axis)from B against each other, to find the optimal assignment of attributes \\(A_{1},\\ldots,A_{n}\\) from A to attributes \\(B_{1},\\ldots,B_{n}\\) from \\(B\\). We assumed that A and B contain the same number of attributes.As a naive approach, one could assume that any possible pair of attributes might correspond, and thus that every attribute of \\(A\\) needs to be tested against all the attributes of \\(B\\) to find the optimal match. We report results for this naive approach, aggregated over all pairs of possible attribute matches, in Table 2. We used three datasets: the census income dataset from the UCI KDD archive (CNUM), the protein homology dataset from the 2004 KDD Cup (BIO) (Caruana and Joachims, 2004), and the forest dataset from the UCI ML archive (Blake and Merz, 1998). For the final dataset, we performed univariate matching of attributes (FOREST) and multivariate matching of tables (FOREST10D) from two different databases, where each table represents one type of forest. Both our asymptotic MMD\\({}_{u}^{2}\\)-based tests perform as well as or better than the alternatives, notably for CNUM, where the advantage of MMD\\({}_{u}^{2}\\) is large. Unlike in Table 1, the next best alternatives are not consistently the same across all data: e.g. in BIO they are _Wolf_ or _Hall_, whereas in FOREST they are _Smir_, _Biau_, or the t-test. Thus, MMD\\({}_{u}^{2}\\) appears to perform more consistently across the multiple datasets. The Friedman-Rafsky tests do not always return a Type I error close to the design parameter: for instance, _Wolf_ has a Type I error of 9.7% on the BIO dataset (on these data, MMD\\({}_{u}^{2}\\) has the joint best Type II error without compromising the designed Type I performance). Finally, MMD\\({}_{b}\\) performs much better than in Table 1, although surprisingly it fails to reliably detect differences in FOREST10D. The results of MMD\\({}_{u}^{2}\\) H are also improved, although it remains among the worst performing methods.A more principled approach to attribute matching is also possible. Assume that \\(\\phi(A)=(\\phi_{1}(A_{1}),\\phi_{2}(A_{2}),...,\\phi_{n}(A_{n}))\\): in other words, the kernel decomposes into kernels on the individual attributes of A (and also decomposes this way on the attributes of B). In this case, \\(MMD^{2}\\) can be written \\(\\sum_{i=1}^{n}\\|\\mu_{i}(A_{i})-\\mu_{i}(B_{i})\\|^{2}\\), where we sum over the MMD terms on each of the attributes. Our goal of optimally assigning attributes from \\(B\\) to attributes of \\(A\\) via MMD is equivalent to finding the optimal permutation \\(\\pi\\) of attributes of \\(B\\) that minimizes \\(\\sum_{i=1}^{n}\\|\\mu_{i}(A_{i})-\\mu_{i}(B_{\\pi(i)})\\|^{2}.\\) If we define \\(C_{ij}=\\|\\mu_{i}(A_{i})-\\mu_{i}(B_{j})\\|^{2}\\), then this is the same as minimizing the sum over \\(C_{i,\\pi(i)}\\). This is the linear assignment problem, which costs \\(O(n^{3})\\) time using the Hungarian method (Kuhn, 1955).While this may appear to be a crude heuristic, it nonetheless defines a semi-metric on the sample spaces \\(X\\) and \\(Y\\) and the corresponding distributions \\(p\\) and \\(q\\). This follows from the fact that matching distances are proper metrics if the matching cost functions are metrics. We formalize this as follows:**Theorem 26**: _Let \\(p,q\\) be distributions on \\(\\mathbb{R}^{d}\\) and denote by \\(p_{i},q_{i}\\) the marginal distributions on the \\(i\\)-th variable. Moreover, denote by \\(\\Pi\\) the symmetric group on \\(\\{1,\\ldots,d\\}\\). The following distance, obtained by optimal coordinate matching, is a semi-metric._\\[\\Delta[\\mathcal{F},p,q]:=\\min_{\\pi\\in\\Pi}\\sum_{i=1}^{d}\\mathrm{MMD}[\\mathcal{F },p_{i},q_{\\pi(i)}]\\]**Proof** Clearly \\(\\Delta[\\mathcal{F},p,q]\\) is nonnegative, since all of its summands are. Next we show the triangle inequality. Denote by \\(r\\) a third distribution on \\(\\mathbb{R}^{d}\\) and let \\(\\pi_{p,q},\\pi_{q,r}\\) and \\(\\pi_{p,r}\\) be the distance minimizing permutations between \\(p,q\\) and \\(r\\) respectively. It then follows that\\[\\Delta[\\mathcal{F},p,q]+\\Delta[\\mathcal{F},q,r] =\\sum_{i=1}^{d}\\text{MMD}[\\mathcal{F},p_{i},q_{\\pi_{p,q}(i)}]+\\sum _{i=1}^{d}\\text{MMD}[\\mathcal{F},q_{i},r_{\\pi_{q,r}(i)}]\\] \\[\\geq\\sum_{i=1}^{d}\\text{MMD}[\\mathcal{F},p_{i},r_{[\\pi_{p,q}\\circ \\pi_{q,r}](i)}]\\geq\\Delta[\\mathcal{F},p,r].\\]Here the first inequality follows from the triangle inequality on MMD, that is\\[\\text{MMD}[\\mathcal{F},p_{i},q_{\\pi_{p,q}(i)}]+\\text{MMD}[\\mathcal{F},q_{\\pi_{ p,q}(i)},r_{[\\pi_{p,q}\\circ\\pi_{q,r}](i)}]\\geq\\text{MMD}[\\mathcal{F},p_{i},r_{[ \\pi_{p,q}\\circ\\pi_{q,r}](i)}].\\]The second inequality is a result of minimization over \\(\\pi\\).We tested this 'Hungarian approach' to attribute matching via \\(\\text{MMD}_{u}^{2}\\) B on three univariate datasets (BIO, CNUM, FOREST) and for table matching on a fourth (FOREST10D). To study \\(\\text{MMD}_{u}^{2}\\) B on structured data, we obtained two datasets of protein graphs (PROTEINS and ENZYMES) and used the graph kernel for proteins from Borgwardt et al. (2005) for table matching via the Hungarian method (the other tests were not applicable to this graph data). The challenge here is to match tables representing one functional class of proteins (or enzymes) from dataset A to the corresponding tables (functional classes) in B. Results are shown in Table 3. Besides on the BIO and CNUM datasets, \\(\\text{MMD}_{u}^{2}\\) B made no errors.## 9 ConclusionWe have established three simple multivariate tests for comparing two distributions \\(p\\) and \\(q\\), based on samples of size \\(m\\) and \\(n\\) from these respective distributions. Our test statistic is the\\begin{table}\\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r} \\hline Dataset & Attr. & \\(\\text{MMD}_{b}\\) & \\(\\text{MMD}_{u}^{2}\\) H & \\(\\text{MMD}_{u}^{2}\\) B & \\(\\text{MMD}_{u}^{2}\\) M & t-test & Wolf & Smir & Hall & Biau \\\\ \\hline \\hline BIO & Same & 100.0 & 100.0 & 93.8 & 94.8 & 95.2 & 90.3 & 95.8 & 95.3 & 99.3 \\\\ \\cline{2-11}  & Different & 20.0 & 52.6 & **17.2** & 17.6 & 36.2 & **17.2** & 18.6 & 17.9 & 42.1 \\\\ \\hline FOREST & Same & 100.0 & 100.0 & 96.4 & 96.0 & 97.4 & 94.6 & 99.8 & 95.5 & 100.0 \\\\ \\cline{2-11}  & Different & 3.9 & 11.0 & **0.0** & **0.0** & 0.2 & 3.8 & **0.0** & 50.1 & **0.0** \\\\ \\hline \\hline CNUM & Same & 100.0 & 100.0 & 94.5 & 93.8 & 94.0 & 98.4 & 97.5 & 91.2 & 98.5 \\\\ \\cline{2-11}  & Different & 14.9 & 52.7 & 2.7 & **2.5** & 19.17 & 22.5 & 11.6 & 79.1 & 50.5 \\\\ \\hline \\hline FOREST10D & Same & 100.0 & 100.0 & 94.0 & 94.0 & 100.0 & 93.5 & 96.5 & 97.0 & 100.0 \\\\ \\hline  & Different & 86.6 & 100.0 & **0.0** & **0.0** & **0.0** & **0.0** & 1.0 & 72.0 & 100.0 \\\\ \\hline \\end{tabular}\\end{table}Table 2: Naive attribute matching on univariate (BIO, FOREST, CNUM) and multivariate data (FOREST10D). Numbers indicate the percentage of accepted null hypothesis (p=q) pooled over attributes. \\(\\alpha=0.05\\). Sample size (dimension; attributes; repetitions of experiment): BIO 377 (1; 6; 100); FOREST 538 (1; 10; 100); CNUM 386 (1; 13; 100); FOREST10D 1000 (10; 2; 100).maximum mean discrepancy (MMD), defined as the maximum deviation in the expectation of a function evaluated on each of the random variables, taken over a sufficiently rich function class: in our case, a universal reproducing kernel Hilbert space (RKHS). Equivalently, the statistic can be written as the norm of the difference between distribution feature means in the RKHS. We do not require density estimates as an intermediate step. Two of our tests provide Type I error bounds that are exact and distribution-free for finite sample sizes. We also give a third test based on quantiles of the asymptotic distribution of the associated test statistic. All three tests can be computed in \\(O((m+n)^{2})\\) time, however when sufficient data are available, a linear time statistic can be used, which employs more data to get better results at smaller computational cost. In addition, a number of metrics on distributions (Kolmogorov-Smirnov, Earth Mover's, \\(L_{2}\\) distance between Parzen window density estimates), as well as certain kernel similarity measures on distributions, are included within our framework.While our result establishes that statistical tests based on the MMD are consistent for universal kernels on compact domains, we draw attention to the recent introduction of _characteristic kernels_ by Fukumizu et al. (2008), these being kernels for which the mean map is injective. Fukumizu et al. establish that Gaussian and Laplace kernels are characteristic on \\(\\mathbb{R}^{d}\\), and thus the MMD is a consistent test for this domain. Sriperumbudur et al. (2008) further explore the properties of characteristic kernels, providing a simple condition to determine whether convolution kernels are characteristic, and describing characteristic kernels which are not universal on compact domains. We also note (following Section 7.2) that the MMD for RKHSs is associated with a particular kernel between probability distributions. Hein et al. (2004) describe several further such kernels, which induce corresponding distances between feature space distribution mappings: these may in turn lead to new and powerful two-sample tests.Two recent studies have shown that additional divergence measures between distributions can be obtained empirically through optimization in a reproducing kernel Hilbert space. Harchaoui et al. (2008) build on the work of Gretton et al. (2007a), considering a homogeneity statistic arising from the kernel Fisher discriminant, rather than the difference of RKHS means; and Nguyen et al. (2008) obtain a KL divergence estimate by approximat\\begin{table}\\begin{tabular}{|l|l|c|c|c|c|} \\hline Dataset & Data type & No. attributes & Sample size & Repetitions & \\% correct matches \\\\ \\hline \\hline BIO & univariate & 6 & 377 & 100 & 90.0 \\\\ \\hline CNUM & univariate & 13 & 386 & 100 & 99.8 \\\\ \\hline FOREST & univariate & 10 & 538 & 100 & 100.0 \\\\ \\hline FOREST10D & multivariate & 2 & 1000 & 100 & 100.0 \\\\ \\hline ENZYME & structured & 6 & 50 & 50 & 100.0 \\\\ \\hline PROTEINS & structured & 2 & 200 & 50 & 100.0 \\\\ \\hline \\end{tabular}\\end{table}Table 3: Hungarian Method for attribute matching via \\(\\text{MMD}_{u}^{2}\\) B on univariate (BIO, CNUM, FOREST), multivariate (FOREST10D), and structured data (ENZYMES, PROTEINS) (\\(\\alpha=0.05\\); \u2018% correct matches\u2019 is the percentage of the correct attribute matches detected over all repetitions).ing the ratio of densities (or its log) with a function in an RKHS. By design, both these kernel-based statistics prioritise different features of \\(p\\) and \\(q\\) when measuring the divergence between them, and the resulting effects on distinguishability of distributions are therefore of interest.Finally, we have seen in Section 2 that several classical metrics on probability distributions can be written as maximum mean discrepancies with function classes that are not Hilbert spaces, but rather Banach, metric, or semi-metric spaces. It would be of particular interest to establish under what conditions one could write these discrepancies in terms of norms of differences of mean elements. In particular, Der and Lee (2007) consider Banach spaces endowed with a semi-inner product, for which a General Riesz Representation exists for elements in the dual.## Appendix A Large Deviation Bounds for Tests with Finite Sample Guarantees### Preliminary Definitions and TheoremsWe need the following theorem, due to McDiarmid (1989).**Theorem 27** (McDiarmid's inequality): _Let \\(f\\,:\\,\\mathcal{X}^{m}\\to\\mathbb{R}\\) be a function such that for all \\(i\\in\\{1,\\ldots,m\\}\\), there exist \\(c_{i}<\\infty\\) for which_\\[\\sup_{X\\in\\mathcal{X}^{m},\\tilde{x}\\in\\mathcal{X}}|f(x_{1},\\ldots x_{m})-f(x_ {1},\\ldots x_{i-1},\\tilde{x},x_{i+1},\\ldots,x_{m})|\\leq c_{i}.\\]_Then for all probability measures \\(p\\) and every \\(\\epsilon>0\\),_\\[p_{\\mathsf{k}^{m}}\\left(f(\\mathbf{x})-\\mathbf{E}_{\\mathsf{k}^{m}}(f(\\mathbf{x} ))>t\\right)<\\exp\\left(-\\frac{2\\epsilon^{2}}{\\sum_{i=1}^{m}c_{i}^{2}}\\right).\\]We also define the Rademacher average of the function class \\(\\mathcal{F}\\) with respect to the \\(m\\)-sample \\(X\\).**Definition 28** (Rademacher average of \\(\\mathcal{F}\\) on \\(X\\)): _Let \\(\\mathcal{F}\\) be the unit ball in a universal RKHS on the compact domain \\(\\mathcal{X}\\), with kernel bounded according to \\(0\\leq k(x,y)\\leq K\\). Let \\(X\\) be an i.i.d. sample of size \\(m\\) drawn according to a probability measure \\(p\\) on \\(\\mathcal{X}\\), and let \\(\\sigma_{i}\\) be i.i.d and take values in \\(\\{-1,1\\}\\) with equal probability. We define the Rademacher average_\\[R_{m}(\\mathcal{F},X) := \\mathbf{E}_{\\sigma}\\sup_{f\\in\\mathcal{F}}\\left|\\frac{1}{m}\\sum_{i =1}^{m}\\sigma_{i}f(x_{i})\\right|\\] \\[\\leq \\left(K/m\\right)^{1/2},\\]_where the upper bound is due to Bartlett and Mendelson (2002, Lemma 22). Similarly, we define_\\[R_{m}(\\mathcal{F},p):=\\mathbf{E}_{p,\\sigma}\\sup_{f\\in\\mathcal{F}}\\left|\\frac{ 1}{m}\\sum_{i=1}^{m}\\sigma_{i}f(x_{i})\\right|.\\]### Bound when \\(p\\) and \\(q\\) May DifferWe want to show that the absolute difference between \\(\\text{MMD}(\\mathcal{F},p,q)\\) and \\(\\text{MMD}_{b}(\\mathcal{F},X,Y)\\) is close to its expected value, independent of the distributions \\(p\\) and \\(q\\). To this end, we prove three intermediate results, which we then combine. The first result we need is an upper bound on the absolute difference between \\(\\text{MMD}(\\mathcal{F},p,q)\\) and \\(\\text{MMD}_{b}(\\mathcal{F},X,Y)\\). We have\\[\\left|\\text{MMD}(\\mathcal{F},p,q)-\\text{MMD}_{b}(\\mathcal{F},X,Y)\\right| \\tag{20}\\] \\[= \\left|\\sup_{f\\in\\mathcal{F}}\\left(\\mathbf{E}_{p}(f)-\\mathbf{E}_{ q}(f)\\right)-\\sup_{f\\in\\mathcal{F}}\\left(\\frac{1}{m}\\sum_{i=1}^{m}f(x_{i})-\\frac{1 }{n}\\sum_{i=1}^{n}f(y_{i})\\right)\\right|\\] \\[\\leq \\underbrace{\\sup_{f\\in\\mathcal{F}}\\left|\\mathbf{E}_{p}(f)- \\mathbf{E}_{q}(f)-\\frac{1}{m}\\sum_{i=1}^{m}f(x_{i})+\\frac{1}{n}\\sum_{i=1}^{n }f(y_{i})\\right|}_{\\Delta(p,q,X,Y)}.\\]Second, we provide an upper bound on the difference between \\(\\Delta(p,q,X,Y)\\) and its expectation. Changing either of \\(x_{i}\\) or \\(y_{i}\\) in \\(\\Delta(p,q,X,Y)\\) results in changes in magnitude of at most \\(2K^{1/2}/m\\) or \\(2K^{1/2}/n\\), respectively. We can then apply McDiarmid's theorem, given a denominator in the exponent of\\[m\\left(2K^{1/2}/m\\right)^{2}+n\\left(2K^{1/2}/n\\right)^{2}=4K\\left(\\frac{1}{m}+ \\frac{1}{n}\\right)=4K\\frac{m+n}{mn},\\]to obtain\\[\\Pr\\left(\\Delta(p,q,X,Y)-\\mathbf{E}_{X,Y}\\left[\\Delta(p,q,X,Y)\\right]>\\epsilon \\right)\\leq\\exp\\left(-\\frac{\\epsilon^{2}mn}{2K(m+n)}\\right). \\tag{21}\\]For our final result, we exploit symmetrisation, following e.g. van der Vaart and Wellner (1996, p. 108), to upper bound the expectation of \\(\\Delta(p,q,X,Y)\\). Denoting by \\(X^{\\prime}\\) an i.i.dsample of size \\(m\\) drawn independently of \\(X\\) (and likewise for \\(Y^{\\prime}\\)), we have\\[\\mathbf{E}_{X,Y}\\left[\\Delta(p,q,X,Y)\\right] \\tag{22}\\] \\[= \\mathbf{E}_{X,Y}\\sup_{f\\in\\mathcal{F}}\\left|\\mathbf{E}_{p}(f)- \\frac{1}{m}\\sum_{i=1}^{m}f(x_{i})-\\mathbf{E}_{q}(f)+\\frac{1}{n}\\sum_{i=1}^{n}f (y_{j})\\right|\\] \\[= \\mathbf{E}_{X,Y}\\sup_{f\\in\\mathcal{F}}\\left|\\mathbf{E}_{X^{\\prime }}\\left(\\frac{1}{m}\\sum_{i=1}^{m}f(x^{\\prime}_{i})\\right)-\\frac{1}{m}\\sum_{i=1 }^{m}f(x_{i})-\\mathbf{E}_{Y^{\\prime}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}f(y^{ \\prime}_{j})\\right)+\\frac{1}{n}\\sum_{i=1}^{n}f(y_{j})\\right|\\] \\[\\underset{(a)}{\\leq} \\mathbf{E}_{X,Y,X^{\\prime},Y^{\\prime}}\\sup_{f\\in\\mathcal{F}}\\left| \\frac{1}{m}\\sum_{i=1}^{m}f(x^{\\prime}_{i})-\\frac{1}{m}\\sum_{i=1}^{m}f(x_{i})- \\frac{1}{n}\\sum_{i=1}^{n}f(y^{\\prime}_{j})+\\frac{1}{n}\\sum_{i=1}^{n}f(y_{j})\\right|\\] \\[= \\mathbf{E}_{X,Y,X^{\\prime},Y^{\\prime},\\sigma,\\sigma^{\\prime}}\\sup _{f\\in\\mathcal{F}}\\left|\\frac{1}{m}\\sum_{i=1}^{m}\\sigma_{i}\\left(f(x^{\\prime}_ {i})-f(x_{i})\\right)+\\frac{1}{n}\\sum_{i=1}^{n}\\sigma^{\\prime}_{i}\\left(f(y^{ \\prime}_{j})-f(y_{j})\\right)\\right|\\] \\[\\underset{(b)}{\\leq} \\mathbf{E}_{X,X^{\\prime}\\sigma}\\sup_{f\\in\\mathcal{F}}\\left|\\frac{1 }{m}\\sum_{i=1}^{m}\\sigma_{i}\\left(f(x^{\\prime}_{i})-f(x_{i})\\right)\\right|+ \\mathbf{E}_{Y,Y^{\\prime}\\sigma}\\sup_{f\\in\\mathcal{F}}\\left|\\frac{1}{n}\\sum_{i =1}^{n}\\sigma_{i}\\left(f(y^{\\prime}_{j})-f(y_{j})\\right)\\right|\\] \\[\\underset{(c)}{\\leq} 2\\left[R_{m}(\\mathcal{F},p)+R_{n}(\\mathcal{F},q)\\right].\\] \\[\\underset{(d)}{\\leq} 2\\left[(K/m)^{1/2}+(K/n)^{1/2}\\right],\\]where (a) uses Jensen's inequality, (b) uses the triangle inequality, (c) substitutes Definition 28 (the Rademacher average), and (d) bounds the Rademacher averages, also via Definition 28.Having established our preliminary results, we proceed to the proof of Theorem 14.**Proof** [Theorem 14] Combining equations (21) and (22), gives\\[\\Pr\\left(\\Delta(p,q,X,Y)-2\\left[(K/m)^{1/2}+(K/n)^{1/2}\\right]>\\epsilon\\right) \\leq\\exp\\left(-\\frac{\\epsilon^{2}mn}{2K(m+n)}\\right).\\]Substituting equation (20) yields the result.### Bound when \\(p=q\\) and \\(m=n\\)In this section, we derive the Theorem 15 result, namely the large deviation bound on the MMD when \\(p=q\\) and \\(m=n\\). Note also that we consider only positive deviations of \\(\\mathrm{MMD}_{b}(\\mathcal{F},X,Y)\\) from \\(\\mathrm{MMD}(\\mathcal{F},p,q)\\), since negative deviations are irrelevant to our hypothesis test. The proof follows the same three steps as in the previous section. The first step in (20) becomes\\[\\mathrm{MMD}_{b}(\\mathcal{F},X,Y)-\\mathrm{MMD}(\\mathcal{F},p,q) = \\mathrm{MMD}_{b}(\\mathcal{F},X,X^{\\prime})-0 \\tag{23}\\] \\[= \\sup_{f\\in\\mathcal{F}}\\left(\\frac{1}{m}\\sum_{i=1}^{m}\\left(f(x_{i })-f(x^{\\prime}_{i})\\right)\\right).\\]The McDiarmid bound on the difference between (23) and its expectation is now a function of \\(2m\\) observations in (23), and has a denominator in the exponent of \\(2m\\left(2K^{1/2}/m\\right)^{2}=8K/m\\). We use a different strategy in obtaining an upper bound on the expected (23), however: this is now\\[\\mathbf{E}_{X,X^{\\prime}}\\left[\\sup_{f\\in\\mathcal{F}}\\frac{1}{m} \\sum_{i=1}^{m}\\left(f(x_{i})-f(x^{\\prime}_{i})\\right)\\right] \\tag{24}\\] \\[= \\frac{1}{m}\\mathbf{E}_{X,X^{\\prime}}\\left\\|\\sum_{i=1}^{m}\\left( \\phi(x_{i})-\\phi(x^{\\prime}_{i})\\right)\\right\\|\\] \\[= \\frac{1}{m}\\mathbf{E}_{X,X^{\\prime}}\\left[\\sum_{i=1}^{m}\\sum_{j=1 }^{m}\\left(k(x_{i},x_{j})+k(x^{\\prime}_{i},x^{\\prime}_{j})-k(x_{i},x^{\\prime} _{j})-k(x^{\\prime}_{i},x_{j})\\right)\\right]^{\\frac{1}{2}}\\] \\[\\leq \\frac{1}{m}\\left[2m\\mathbf{E}_{x}k(x,x)+2m(m-1)\\mathbf{E}_{x,x^{ \\prime}}k(x,x^{\\prime})-2m^{2}\\mathbf{E}_{x,x^{\\prime}}k(x,x^{\\prime})\\right] ^{\\frac{1}{2}}\\] \\[= \\left[\\frac{2}{m}\\mathbf{E}_{x,x^{\\prime}}\\left(k(x,x)-k(x,x^{ \\prime})\\right)\\right]^{\\frac{1}{2}}\\] \\[\\leq \\left(2K/m\\right)^{1/2}. \\tag{25}\\]We remark that both (24) and (25) bound the amount by which our biased estimate of the population MMD exceeds zero under \\(\\mathcal{H}_{0}\\). Combining the three results, we find that under \\(\\mathcal{H}_{0}\\),\\[p_{X}\\left(\\mathrm{MMD}_{b}(\\mathcal{F},X,X^{\\prime})-\\left[ \\frac{2}{m}\\mathbf{E}_{x,x^{\\prime}}\\left(k(x,x)-k(x,x^{\\prime})\\right)\\right] ^{\\frac{1}{2}}>\\epsilon\\right) < \\exp\\left(\\frac{-\\epsilon^{2}m}{4K}\\right)\\quad\\text{and}\\] \\[p_{X}\\left(\\mathrm{MMD}_{b}(\\mathcal{F},X,X^{\\prime})-\\left(2K/m \\right)^{1/2}>\\epsilon\\right) < \\exp\\left(\\frac{-\\epsilon^{2}m}{4K}\\right).\\]## Appendix B Proofs for Asymptotic TestsWe derive results needed in the asymptotic test of Section 5. Appendix B.1 describes the distribution of the empirical MMD under \\(\\mathcal{H}_{0}\\) (both distributions identical). Appendix B.2 contains derivations of the second and third moments of the empirical MMD, also under \\(\\mathcal{H}_{0}\\).### Convergence of the Empirical MMD under \\(\\mathcal{H}_{0}\\)We describe the distribution of the unbiased estimator \\(\\mathrm{MMD}_{u}^{2}[\\mathcal{F},X,Y]\\) under the null hypothesis. In this circumstance, we denote it by \\(\\mathrm{MMD}_{u}^{2}[\\mathcal{F},X,X^{\\prime}]\\), to emphasise that the second sample \\(X^{\\prime}\\) is drawn independently from the same distribution as \\(X\\). We thus obtain the U-statistic\\[\\mathrm{MMD}^{2}_{u}[\\mathcal{F},X,X^{\\prime}] = \\frac{1}{m(m-1)}\\sum_{i\eq j}k(x_{i},x_{j})+k(x^{\\prime}_{i},x^{ \\prime}_{j})-k(x_{i},x^{\\prime}_{j})-k(x_{j},x^{\\prime}_{i}) \\tag{26}\\] \\[= \\frac{1}{m(m-1)}\\sum_{i\eq j}h(z_{i},z_{j}), \\tag{27}\\]where \\(z_{i}=(x_{i},x^{\\prime}_{i})\\). Under the null hypothesis, this U-statistic is degenerate, meaning\\[\\mathbf{E}_{z_{j}}h(z_{i},z_{j}) = \\mathbf{E}_{x_{j}}k(x_{i},x_{j})+\\mathbf{E}_{x^{\\prime}_{j}}k(x^{ \\prime}_{i},x^{\\prime}_{j})-\\mathbf{E}_{x^{\\prime}_{j}}k(x_{i},x^{\\prime}_{j} )-\\mathbf{E}_{x_{j}}k(x_{j},x^{\\prime}_{i})\\] \\[= 0.\\]The following theorem from Serfling (1980, Section 5.5.2) then applies.**Theorem 29**: _Assume \\(\\mathrm{MMD}^{2}_{u}[\\mathcal{F},X,X^{\\prime}]\\) is as defined in (27), with \\(\\mathbf{E}_{z^{\\prime}}h(z,z^{\\prime})=0\\), and furthermore assume \\(0<\\mathbf{E}_{z,z^{\\prime}}h^{2}(z,z^{\\prime})<\\infty\\). Then \\(\\mathrm{MMD}^{2}_{u}[\\mathcal{F},X,X^{\\prime}]\\) converges in distribution according to_\\[m\\mathrm{MMD}^{2}_{u}[\\mathcal{F},X,X^{\\prime}]\\stackrel{{ D}}{{ \\rightarrow}}\\sum_{l=1}^{\\infty}\\gamma_{l}\\left(\\chi^{2}_{1l}-1\\right),\\]_where \\(\\chi^{2}_{1l}\\) are independent chi squared random variables of degree one, and \\(\\gamma_{l}\\) are the solutions to the eigenvalue equation_\\[\\gamma_{l}\\psi_{l}(u)=\\int h(u,v)\\psi_{l}(v)d\\Pr_{v}.\\]While this result is adequate for our purposes (since we do not explicitly use the quantities \\(\\gamma_{l}\\) in our subsequent reasoning), it does not make clear the dependence of the null distribution on the kernel choice. For this reason, we provide an alternative expression based on the reasoning of Anderson et al. (1994, Appendix), bearing in mind the following changes:* we do not need to deal with the bias terms \\(S_{1j}\\) seen by Anderson et al. (1994, Appendix) that vanish for large sample sizes, since our statistic is unbiased (although these bias terms drop faster than the variance);* we require greater generality, since we deal with distributions on compact metric spaces, and not densities on \\(\\mathbb{R}^{d}\\); correspondingly, our kernels are not necessarily inner products in \\(L_{2}\\) between probability density functions (although this is a special case).Our first step is to express the kernel \\(h(z_{i},z_{j})\\) of the U-statistic in terms of an RKHS kernel \\(\\tilde{k}(x_{i},x_{j})\\) between feature space mappings from which the mean has been subtracted,\\[\\tilde{k}(x_{i},x_{j}) := \\langle\\phi(x_{i})-\\mu[p],\\phi(x_{j})-\\mu[p]\\rangle\\] \\[= k(x_{i},x_{j})-\\mathbf{E}_{x}k(x_{i},x)-\\mathbf{E}_{x}k(x,x_{j})+ \\mathbf{E}_{x,x^{\\prime}}k(x,x^{\\prime}).\\]The centering terms cancel (the distance between the two points is unaffected by an identical global shift in both the points), meaning\\[h(z_{i},z_{j})=\\tilde{k}(x_{i},x_{j})+\\tilde{k}(y_{i},y_{j})-\\tilde{k}(x_{i},y _{j})-\\tilde{k}(x_{j},y_{i}).\\]Next, we write the kernel \\(\\tilde{k}(x_{i},x_{j})\\) in terms of eigenfunctions \\(\\psi_{i}(x)\\) with respect to the probability measure \\(\\Pr_{x}\\),\\[\\tilde{k}(x,x^{\\prime})=\\sum_{l=1}^{\\infty}\\lambda_{l}\\psi_{l}(x)\\psi_{l}(x^{ \\prime}),\\]where\\[\\int_{\\mathfrak{X}}\\tilde{k}(x,x^{\\prime})\\psi_{i}(x)d\\Pr_{x}(x)=\\lambda_{i} \\psi_{i}(x^{\\prime})\\]and\\[\\int_{\\mathfrak{X}}\\psi_{i}(x)\\psi_{j}(x)d\\Pr_{x}(x)=\\delta_{ij}. \\tag{28}\\]Since\\[\\mathbf{E}_{x}\\tilde{k}(x,v) = \\mathbf{E}_{x}k(x,v)-\\mathbf{E}_{x,x^{\\prime}}k(x,x^{\\prime})- \\mathbf{E}_{x}k(x,v)+\\mathbf{E}_{x,x^{\\prime}}k(x,x^{\\prime})\\] \\[= 0,\\]then when \\(\\lambda_{i}\eq 0\\), we have that\\[\\lambda_{i}\\mathbf{E}_{x^{\\prime}}\\psi_{i}(x^{\\prime}) = \\int_{\\mathfrak{X}}\\mathbf{E}_{x^{\\prime}}\\tilde{k}(x,x^{\\prime} )\\psi_{i}(x)d\\Pr_{x}(x)\\] \\[= 0,\\]and hence\\[\\mathbf{E}_{x}\\psi_{i}(x)=0. \\tag{29}\\]We now use these results to transform the expression in (26). First,\\[\\frac{1}{m}\\sum_{i\eq j}\\tilde{k}(x_{i},x_{j}) = \\frac{1}{m}\\sum_{i\eq j}\\sum_{l=1}^{\\infty}\\lambda_{l}\\psi_{l}(x _{i})\\psi_{l}(x_{j})\\] \\[= \\frac{1}{m}\\sum_{l=1}^{\\infty}\\lambda_{l}\\left(\\left(\\sum_{i}\\psi _{l}(x_{i})\\right)^{2}-\\sum_{i}\\psi_{l}^{2}(x_{i})\\right)\\] \\[\\underset{D}{\\rightarrow} \\sum_{l=1}^{\\infty}\\lambda_{l}(y_{l}^{2}-1),\\]where \\(y_{l}\\sim\\mathcal{N}(0,1)\\) are i.i.d., and the final relation denotes convergence in distribution, using (28) and (29), following Serfling (1980, Section 5.5.2). Likewise\\[\\frac{1}{m}\\sum_{i\eq j}\\tilde{k}(x_{i}^{\\prime},x_{j}^{\\prime})\\ \\underset{D}{\\rightarrow}\\ \\sum_{l=1}^{\\infty}\\lambda_{l}(z_{l}^{2}-1),\\]where \\(z_{l}\\sim\\mathcal{N}(0,1)\\), and\\[\\frac{1}{m(m-1)}\\sum_{i\eq j}\\left(\\tilde{k}(x_{i},y_{j})+\\tilde{k}(x_{j},y_{ i})\\right)\\ \\underset{D}{\\rightarrow}\\ 2\\sum_{l=1}^{\\infty}\\lambda_{l}y_{l}z_{l}.\\]Combining these results, we get\\[m\\mathrm{MMD}_{u}^{2}(\\mathcal{F},X,X^{\\prime}) \\underset{D}{\\rightarrow} \\sum_{l=1}^{\\infty}\\lambda_{l}\\left(y_{l}^{2}+z_{l}^{2}-2-2y_{l}z _{l}\\right)\\] \\[= \\sum_{l=1}^{\\infty}\\lambda_{l}\\left[(y_{l}-z_{l})^{2}-2\\right].\\]Note that \\(y_{l}-z_{l}\\), being the difference of two independent Gaussian variables, has a normal distribution with mean zero and variance \\(2\\). This is therefore a quadratic form in a Gaussian random variable minus an offset \\(2\\sum_{l=1}^{\\infty}\\lambda_{l}\\).### Moments of the Empirical MMD Under \\(\\mathcal{H}_{0}\\)In this section, we compute the moments of the U-statistic in Section 5, under the null hypothesis conditions\\[\\mathbf{E}_{z,z^{\\prime}}h(z,z^{\\prime})=0, \\tag{30}\\]and, importantly,\\[\\mathbf{E}_{z^{\\prime}}h(z,z^{\\prime})=0. \\tag{31}\\]Note that the latter implies the former.**Variance/2nd moment:** This was derived by Hoeffding (1948, p. 299), and is also described by Serfling (1980, Lemma A p. 183). Applying these results,\\[\\mathbf{E}\\left(\\left[\\mathrm{MMD}_{u}^{2}\\right]^{2}\\right)\\] \\[=\\left(\\frac{2}{n(n-1)}\\right)^{2}\\left[\\frac{n(n-1)}{2}(n-2)(2) \\mathbf{E}_{z}\\left[(\\mathbf{E}_{z^{\\prime}}h(z,z^{\\prime}))^{2}\\right]+\\frac{ n(n-1)}{2}\\mathbf{E}_{z,z^{\\prime}}\\left[h^{2}(z,z^{\\prime})\\right]\\right]\\] \\[=\\frac{2(n-2)}{n(n-1)}\\mathbf{E}_{z}\\left[(\\mathbf{E}_{z^{\\prime }}h(z,z^{\\prime}))^{2}\\right]+\\frac{2}{n(n-1)}\\mathbf{E}_{z,z^{\\prime}}\\left[h ^{2}(z,z^{\\prime})\\right]\\] \\[=\\frac{2}{n(n-1)}\\mathbf{E}_{z,z^{\\prime}}\\left[h^{2}(z,z^{\\prime })\\right],\\]where the first term in the penultimate line is zero due to (31). Note that variance and 2nd moment are the same under the zero mean assumption.**3rd moment:** We consider the terms that appear in the expansion of \\(\\mathbf{E}\\left(\\left[\\mathrm{MMD}_{u}^{2}\\right]^{3}\\right)\\). These are all of the form\\[\\left(\\frac{2}{n(n-1)}\\right)^{3}\\mathbf{E}(h_{ab}h_{cd}h_{ef}),\\]where we shorten \\(h_{ab}=h(z_{a},z_{b})\\), and we know \\(z_{a}\\) and \\(z_{b}\\) are always independent. Most of the terms vanish due to (30) and (31). The first terms that remain take the form\\[\\left(\\frac{2}{n(n-1)}\\right)^{3}\\mathbf{E}(h_{ab}h_{bc}h_{ca}),\\]and there are\\[\\frac{n(n-1)}{2}(n-2)(2)\\]of them, which gives us the expression\\[\\left(\\frac{2}{n(n-1)}\\right)^{3}\\frac{n(n-1)}{2}(n-2)(2)\\mathbf{E} _{z,z^{\\prime}}\\left[h(z,z^{\\prime})\\mathbf{E}_{z^{\\prime\\prime}}\\left(h(z,z^{ \\prime\\prime})h(z^{\\prime},z^{\\prime\\prime})\\right)\\right]\\] \\[=\\frac{8(n-2)}{n^{2}(n-1)^{2}}\\mathbf{E}_{z,z^{\\prime}}\\left[h(z,z ^{\\prime})\\mathbf{E}_{z^{\\prime\\prime}}\\left(h(z,z^{\\prime\\prime})h(z^{\\prime}, z^{\\prime\\prime})\\right)\\right]. \\tag{32}\\]Note the scaling \\(\\frac{8(n-2)}{n^{2}(n-1)^{2}}\\sim\\frac{1}{n^{3}}\\). The remaining non-zero terms, for which \\(a=c=e\\) and \\(b=d=f\\), take the form\\[\\left(\\frac{2}{n(n-1)}\\right)^{3}\\mathbf{E}_{z,z^{\\prime}}\\left[h^{3}(z,z^{ \\prime})\\right],\\]and there are \\(\\frac{n(n-1)}{2}\\) of them, which gives\\[\\left(\\frac{2}{n(n-1)}\\right)^{2}\\mathbf{E}_{z,z^{\\prime}}\\left[h^{3}(z,z^{ \\prime})\\right]. \\tag{33}\\]However \\(\\left(\\frac{2}{n(n-1)}\\right)^{2}\\sim n^{-4}\\) so this term is negligible compared with (32). Thus, a reasonable approximation to the third moment is\\[\\mathbf{E}\\left(\\left[\\mathrm{MMD}_{u}^{2}\\right]^{3}\\right)\\approx\\frac{8(n-2 )}{n^{2}(n-1)^{2}}\\mathbf{E}_{z,z^{\\prime}}\\left[h(z,z^{\\prime})\\mathbf{E}_{z^ {\\prime\\prime}}\\left(h(z,z^{\\prime\\prime})h(z^{\\prime},z^{\\prime\\prime}) \\right)\\right].\\]Acknowledgements:We thank Philipp Berens, Olivier Bousquet, John Langford, Omri Guttman, Matthias Hein, Novi Quadrianto, Le Song, and Vishy Vishwanathan for constructive discussions; Patrick Warnat (DKFZ, Heidelberg), for providing the microarray datasets; and Nikos Logothetis, for providing the neural datasets. National ICT Australia is funded through the Australian Government's _Backing Australia's Ability_ initiative, in part through the Australian Research Council. This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778, and by the Austrian Science Fund (FWF), project # S9102-N04.## References* Altun and Smola [2006] Y. Altun and A.J. Smola. Unifying divergence minimization and statistical inference via convex duality. In H.U. Simon and G. Lugosi, editors, _Proc. Annual Conf. Computational Learning Theory_, LNCS, pages 139-153. Springer, 2006.* Anderson et al. [1994] N. Anderson, P. Hall, and D. Titterington. Two-sample test statistics for measuring discrepancies between two multivariate probability density functions using kernel-based density estimates. _Journal of Multivariate Analysis_, 50:41-54, 1994.* Andrews et al. [2003] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support vector machines for multiple-instance learning. In S. Becker, S. Thrun, and K. Obermayer, editors, _Advances in Neural Information Processing Systems 15_. MIT Press, 2003.* Arcones and Gine (1992) M. Arcones and E. Gine. On the bootstrap of \\(u\\) and \\(v\\) statistics. _The Annals of Statistics_, 20(2):655-674, 1992.* Bach and Jordan (2002) F. R. Bach and M. I. Jordan. Kernel independent component analysis. _J. Mach. Learn. Res._, 3:1-48, 2002.* Bartlett and Mendelson (2002) P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. _J. Mach. Learn. Res._, 3:463-482, 2002.* Ben-David et al. (2007) S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. Analysis of representations for domain adaptation. In _NIPS 19_, pages 137-144. MIT Press, 2007.* Biau and Gyorfi (2005) G. Biau and L. Gyorfi. On the asymptotic properties of a nonparametric \\(l_{1}\\)-test statistic of homogeneity. _IEEE Transactions on Information Theory_, 51(11):3965-3973, 2005.* Bickel (1969) P. Bickel. A distribution free version of the Smirnov two sample test in the p-variate case. _The Annals of Mathematical Statistics_, 40(1):1-23, 1969.* Blake and Merz (1998) C. L. Blake and C. J. Merz. UCI repository of machine learning databases, 1998. URL [http://www.ics.uci.edu/~mlearn/MLRepository.html](http://www.ics.uci.edu/~mlearn/MLRepository.html).* Borgwardt et al. (2005) K. M. Borgwardt, C. S. Ong, S. Schonauer, S. V. N. Vishwanathan, A. J. Smola, and H. P. Kriegel. Protein function prediction via graph kernels. _Bioinformatics_, 21(Suppl 1):i47-i56, Jun 2005.* Borgwardt et al. (2006) K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Scholkopf, and A. J. Smola. Integrating structured biological data by kernel maximum mean discrepancy. _Bioinformatics (ISMB)_, 22(14):e49-e57, 2006.* Bousquet et al. (2005) O. Bousquet, S. Boucheron, and G. Lugosi. Theory of classification: a survey of recent advances. _ESAIM: Probab. Stat._, 9:323- 375, 2005.* Caruana and Joachims (2004) R. Caruana and T. Joachims. Kdd cup. [http://kodiak.cs.cornell.edu/kddcup/index.html](http://kodiak.cs.cornell.edu/kddcup/index.html), 2004.* Casella and Berger (2002) G. Casella and R. Berger. _Statistical Inference_. Duxbury, Pacific Grove, CA, 2nd edition, 2002.* Chazelle (2000) B. Chazelle. A minimum spanning tree algorithm with inverse-ackermann type complexity. _Journal of the ACM_, 47, 2000.* Comon (1994) P. Comon. Independent component analysis, a new concept? _Signal Processing_, 36:287-314, 1994.* Davy et al. (2002) M. Davy, A. Gretton, A. Doucet, and P. J. W. Rayner. Optimized support vector machines for nonstationary signal classification. _IEEE Signal Processing Letters_, 9(12):442-445, 2002.* Der and Lee (2007) R. Der and D. Lee. Large-margin classification in banach spaces. In _AISTATS 11_, 2007.* Davy et al. (2008)* Dudik and Schapire (2006) M. Dudik and R. E. Schapire. Maximum entropy distribution estimation with generalized regularization. In Gabor Lugosi and Hans U. Simon, editors, _Proc. Annual Conf. Computational Learning Theory_. Springer Verlag, June 2006.* Dudik et al. (2004) M. Dudik, S. Phillips, and R.E. Schapire. Performance guarantees for regularized maximum entropy density estimation. In _Proc. Annual Conf. Computational Learning Theory_. Springer Verlag, 2004.* Dudley (2002) R. M. Dudley. _Real analysis and probability_. Cambridge University Press, Cambridge, UK, 2002.* Feller (1971) W. Feller. _An Introduction to Probability Theory and its Applications_. John Wiley and Sons, New York, 2 edition, 1971.* Feuerverger (1993) Andrey Feuerverger. A consistent test for bivariate dependence. _International Statistical Review_, 61(3):419-433, 1993.* Fine and Scheinberg (2001) S. Fine and K. Scheinberg. Efficient SVM training using low-rank kernel representations. _Journal of Machine Learning Research_, 2:243-264, Dec 2001.* Fortet and Mourier (1953) R. Fortet and E. Mourier. Convergence de la reparation empirique vers la reparation theorique. _Ann. Scient. Ecole Norm. Sup._, 70:266-285, 1953.* Friedman and Rafsky (1979) J. Friedman and L. Rafsky. Multivariate generalizations of the Wald-Wolfowitz and Smirnov two-sample tests. _The Annals of Statistics_, 7(4):697-717, 1979.* Fukumizu et al. (2008) K. Fukumizu, A. Gretton, X. Sun, and B. Scholkopf. Kernel measures of conditional dependence. In _Advances in Neural Information Processing Systems 20_, 2008.* Gartner et al. (2002) T. Gartner, P. A. Flach, A. Kowalczyk, and A. J. Smola. Multi-instance kernels. In _Proc. Intl. Conf. Machine Learning_, 2002.* Gokcay and Principe (2002) E. Gokcay and J.C. Principe. Information theoretic clustering. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 24(2):158-171, 2002.* Gretton et al. (2005a) A. Gretton, O. Bousquet, A.J. Smola, and B. Scholkopf. Measuring statistical dependence with Hilbert-Schmidt norms. In S. Jain, H. U. Simon, and E. Tomita, editors, _Proceedings Algorithmic Learning Theory_, pages 63-77, Berlin, Germany, 2005a. Springer-Verlag.* Gretton et al. (2005b) A. Gretton, R. Herbrich, A. Smola, O. Bousquet, and B. Scholkopf. Kernel methods for measuring independence. _J. Mach. Learn. Res._, 6:2075-2129, 2005b.* Gretton et al. (2007a) A. Gretton, K. Borgwardt, M. Rasch, B. Scholkopf, and A. Smola. A kernel method for the two-sample-problem. In _Advances in Neural Information Processing Systems 19_, pages 513-520, Cambridge, MA, 2007a. MIT Press.* Gretton et al. (2007b) A. Gretton, K. Borgwardt, M. Rasch, B. Scholkopf, and A. Smola. A kernel approach to comparing distributions. _Proceedings of the 22nd Conference on Artificial Intelligence (AAAI-07)_, pages 1637-1641, 2007b.* Gretton et al. (2008) A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Scholkopf, and A. Smola. A kernel statistical test of independence. In _Neural Information Processing Systems_, 2008.* Grimmet and Stirzaker (2001) G. R. Grimmet and D. R. Stirzaker. _Probability and Random Processes_. Oxford University Press, Oxford, third edition, 2001.* Hall and Tajvidi (2002) P. Hall and N. Tajvidi. Permutation tests for equality of distributions in high-dimensional settings. _Biometrika_, 89(2):359-374, 2002.* Harchaoui et al. (2008) Z. Harchaoui, F. Bach, and E. Moulines. Testing for homogeneity with kernel fisher discriminant analysis. In _NIPS 20_. MIT Press, 2008.* Hein et al. (2004) M. Hein, T.N. Lal, and O. Bousquet. Hilbertian metrics on probability measures and their application in svm's. In _Proceedings of the 26th DAGM Symposium_, pages 270-277, Berlin, 2004. Springer.* Henze and Penrose (1999) N. Henze and M. Penrose. On the multivariate runs test. _The Annals of Statistics_, 27(1):290-298, 1999.* Hoeffding (1963) W. Hoeffding. Probability inequalities for sums of bounded random variables. _Journal of the American Statistical Association_, 58:13-30, 1963.* Hoeffding (1948) Wassily Hoeffding. A class of statistics with asymptotically normal distribution. _The Annals of Mathematical Statistics_, 19(3):293-325, 1948.* Jebara and Kondor (2003) T. Jebara and I. Kondor. Bhattacharyya and expected likelihood kernels. In B. Scholkopf and M. Warmuth, editors, _Proceedings of the Sixteenth Annual Conference on Computational Learning Theory_, number 2777 in Lecture Notes in Computer Science, pages 57-71, Heidelberg, Germany, 2003. Springer-Verlag.* Johnson et al. (1994) N. L. Johnson, S. Kotz, and N. Balakrishnan. _Continuous Univariate Distributions. Volume 1 (Second Edition)_. John Wiley and Sons, 1994.* Kankainen (1995) A. Kankainen. _Consistent Testing of Total Independence Based on the Empirical Characteristic Function_. PhD thesis, University of Jyvaskyla, 1995.* Kifer et al. (2004) D. Kifer, S. Ben-David, and J. Gehrke. Detecting change in data streams. In _Very Large Databases (VLDB)_, 2004.* Kuhn (1955) H.W. Kuhn. The Hungarian method for the assignment problem. _Naval Research Logistics Quarterly_, 2:83-97, 1955.* McDiarmid (1989) C. McDiarmid. On the method of bounded differences. In _Survey in Combinatorics_, pages 148-188. Cambridge University Press, 1989.* Mercer (1909) J. Mercer. Functions of positive and negative type and their connection with the theory of integral equations. _Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci._, A 209:415-446, 1909.* Micchelli et al. (2006) C. Micchelli, Y. Xu, and H. Zhang. Universal kernels. _Journal of Machine Learning Research_, 7:2651-2667, 2006.* Micchelli et al. (2008)* Muller (1997) A. Muller. Integral probability metrics and their generating classes of functions. _Adv. Appl. Prob._, 29:429-443, 1997.* Nguyen et al. (2008) XuanLong Nguyen, Martin Wainwright, and Michael Jordan. Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization. In _NIPS 20_. MIT Press, 2008.* Press et al. (1994) W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. _Numerical Recipes in C. The Art of Scientific Computation_. Cambridge University Press, Cambridge, UK, 1994.* Rasch et al. (2008) M. Rasch, A. Gretton, Y. Murayama, W. Maass, and N. Logothetis. Inferring spike trains from local field potentials. _Journal of Neurophysiology_, 99:1461-1476, 2008.* Rosenbaum (2005) P. Rosenbaum. An exact distribution-free test comparing two multivariate distributions based on adjacency. _Journal of the Royal Statistical Society B_, 67(4):515-530, 2005.* Rubner et al. (2000) Y. Rubner, C. Tomasi, and L.J. Guibas. The earth mover's distance as a metric for image retrieval. _Int. J. Comput. Vision_, 40(2):99-121, 2000. doi: [http://dx.doi.org/10.1023/A:1026543900054](http://dx.doi.org/10.1023/A:1026543900054).* Scholkopf (1997) B. Scholkopf. _Support Vector Learning_. R. Oldenbourg Verlag, Munich, 1997. Download: [http://www.kernel-machines.org](http://www.kernel-machines.org).* Scholkopf and Smola (2002) B. Scholkopf and A. Smola. _Learning with Kernels_. MIT Press, Cambridge, MA, 2002.* Scholkopf et al. (2001) B. Scholkopf, J. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the support of a high-dimensional distribution. _Neural Comput._, 13(7):1443-1471, 2001.* Scholkopf et al. (2004) B. Scholkopf, K. Tsuda, and J.-P. Vert. _Kernel Methods in Computational Biology_. MIT Press, Cambridge, MA, 2004.* Serfling (1980) R. Serfling. _Approximation Theorems of Mathematical Statistics_. Wiley, New York, 1980.* Shawe-Taylor and Cristianini (2004) J. Shawe-Taylor and N. Cristianini. _Kernel Methods for Pattern Analysis_. Cambridge University Press, Cambridge, UK, 2004.* Shawe-Taylor and Dolia (2007) J. Shawe-Taylor and A. Dolia. A framework for probability density estimation. In M. Meila and X. Shen, editors, _Proceedings of International Workshop on Artificial Intelligence and Statistics_, 2007.* Silverman (1986) B. W. Silverman. _Density estimation for statistical and data analysis_. Monographs on statistics and applied probability. Chapman and Hall, London, 1986.* Smirnov (1939) N.V. Smirnov. On the estimation of the discrepancy between empirical curves of distribution for two independent samples. _Bulletin Mathematics_, 2:3-26, 1939. University of Moscow.* Smola and Scholkopf (2000) A. J. Smola and B. Scholkopf. Sparse greedy matrix approximation for machine learning. In P. Langley, editor, _Proc. Intl. Conf. Machine Learning_, pages 911-918, San Francisco, 2000. Morgan Kaufmann Publishers.* Smola et al. (2007) A.J. Smola, A. Gretton, L. Song, and B. Scholkopf. A hilbert space embedding for distributions. In E. Takimoto, editor, _Algorithmic Learning Theory_, Lecture Notes on Computer Science. Springer, 2007.* Song et al. (2008) L. Song, X. Zhang, A. Smola, A. Gretton, and B. Scholkopf. Tailoring density estimation via reproducing kernel moment matching. In _ICML_, 2008. to appear.* Sriperumbudur et al. (2008) B. Sriperumbudur, A. Gretton, K. Fukumizu, G. Lanckriet, and B. Scholkopf. Injective hilbert space embeddings of probability measures. In _COLT_, 2008. to appear.* Steinwart (2001) I. Steinwart. On the influence of the kernel on the consistency of support vector machines. _J. Mach. Learn. Res._, 2:67-93, 2001.* Takeuchi et al. (2006) I. Takeuchi, Q.V. Le, T. Sears, and A.J. Smola. Nonparametric quantile estimation. _J. Mach. Learn. Res._, 2006.* Tax and Duin (1999) D. M. J. Tax and R. P. W. Duin. Data domain description by support vectors. In M. Verleysen, editor, _Proceedings ESANN_, pages 251-256, Brussels, 1999. D Facto.* van der Vaart and Wellner (1996) A. W. van der Vaart and J. A. Wellner. _Weak Convergence and Empirical Processes_. Springer, 1996.* Wilkins (1944) J. E. Wilkins. A note on skewness and kurtosis. _The Annals of Mathematical Statistics_, 15(3):333-335, 1944.* Williams and Seeger (2001) Christopher K. I. Williams and Matthias Seeger. Using the Nystrom method to speed up kernel machines. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, _Advances in Neural Information Processing Systems 13_, pages 682-688, Cambridge, MA, 2001. MIT Press."}


